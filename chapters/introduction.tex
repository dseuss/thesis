 % -*- root: ../thesis.tex -*-

\chapter{Introduction}%
\label{chap:introduction}


Physics as an inherently empirical science relies on experimental data to single out theories that are compatible with our observations.
It is therefore a fundamental problem to transform data into answers to questions posed by the physicist.
A large fraction of experimental problems can be phrased in terms of parametric estimation:
Given a mathematical model that relates the parameters of interest to observable outcomes, estimate the parameters that fit the data \quotes{best}.
In other words, parameter estimation is an inverse problem for a fixed model of the system.

Two crucial aspects of estimation problems in general are constraints and complexity.
The former refers to the fact that many models are specified in terms of continuous parameters, but not all parameter values correspond to valid models.
One typical example for constraints are mathematical facts, e.g.\ the standard deviation of any random variable has to be non-negative.
Others include assumptions on the particular model such as the constraint that a valid density matrix of a quantum system is positive semi-definite.
Note that not all constraints need to be hard constraints as the examples stated above.
Soft constraints can be used to promote desired properties of the estimate or to penalize values due to prior knowledge.

In general, the notion of complexity refers to the amount of resources required to solve inference problems as the size of the underlying models grows.
On the one hand, we use \quotes{complexity} to refer to the amount of experimental resources required, which is often phrased in terms of sample complexity, i.e.\ the number of measurements.
But also other measures such as the time required to perform a given experiment are possible if the necessary information can be quantified.
On the other hand, we are interested in the amount of computational resources required to extract the desired information from the available data.
Theses are often quantified in terms of runtime of the inference computation.
Note that these two notions of complexity can be strongly related:
If an experiment -- such as the ones performed at the LHC -- truly deserves the ubiquitous \quotes{Big Data} label, then an enormous amount of computational resources is necessary to perform even simple computations on the whole dataset.
Additionally, inference problems can also be inherently hard to solve.\\



In this work, we are interested in the interaction of constraints and complexity.
The main motivation stems from recent progress in quantum technologies in general and quantum computing in particular:
Many established techniques for characterization -- i.e.\ inferring a complete description of a system from experimental data -- work well for small systems with only a few qubits, which were prevalent in the past.
However, many of these approaches do not scale to larger systems as they require exponentially large amounts of resources.
With the recent announcements of quantum devices with up to 72-qubits~\cite{Sciencenews}, it becomes clear that new techniques for characterization need to be developed.
One way to go forward is to exploit physical constraints or to impose additional assumptions on the model to reduce the amount of resources necessary or to improve estimates.
To better understand the interplay of constraints and complexity, we investigate three inference problems with applications to quantum estimation in this work.

In \cref{chap:error}, we examine uncertainty quantification in quantum state estimation -- the problem of estimating the density matrix $\rho$ of a quantum system from measurements.
Since all outcomes of quantum measurements are inherently random, one should not only report the final estimate, but also answer the question whether the result is statistically reliable or simply arose due to chance.
For this purpose, we can use the notion of \quotes{error bars} or \quotes{error regions} from statistical inference.
To investigate the effects of constraints on the computational complexity of the problem, we investigate optimal error regions that are easy to compute in an unconstraint model.
However, we show that taking into account the physical constraints on $\rho$, i.e.\ positive semi-definiteness, renders the problem of computing optimal error regions intractable.
We also show that there are settings, where exactly those physical constraints drastically improve the power of the regions, and therefore, are necessary for optimal error regions.
In conclusion, we show that exploiting the physical constraints on $\rho$ is essential to obtain optimal error regions, but doing so in an optimal way poses an intractable computational problem.

The second main result of this thesis is concerned with characterizing linear optical circuits, which have been proposed as one possible architecture for quantum computing.
By measuring the output of such a device for different inputs, the problem is to reconstruct the \emph{transfer matrix} of the device.
Here, the main challenge is that the standard measurement devices in optics -- single photon detectors and photo diodes -- are insensitive to the phases of the output.
Therefore, we need to deliberately use interference between different modes of the device to gain information on the complex phases.
This raises the question of how to choose the inputs in an optimal way, in order to reduce the total number of measurements required and how to reconstruct the transfer matrix in a computationally efficient way.
In \cref{chap:phaselift}, we propose a characterization method that is asymptotically optimal w.r.t.\ the sample complexity, comes with a rigorous proof of convergence, and is robust to noise.
\todo{Hmmm.... 2}
The suggested method adapts ideas from low-rank matrix recovery and leverages an exact mathematical constraint on the signal to be recovered to derive a reconstruction algorithm that is efficient w.r.t.\ both sample and computational complexity.

\Cref{chap:tensors} is dedicated to the problem of efficiently reconstructing low-rank tensors from linear measurements.
We consider tensors $X \in \Reals^{d^N}$, where $d$ is the local dimension and $N$ the order of the tensor.
Without any additional structure, reconstructing $X$ from measurements of the form  $\braket{A, X}$ for measurement tensors $A$ requires at least $d^N$ such overlaps as each component of $X$ is independent from the other.
Therefore, the sample complexity scales exponentially in $N$ and the problem becomes infeasible already for moderate values of $N$.
Furthermore, any reconstruction algorithm of an arbitrary tensor requires an exponentially long runtime as simply outputting the result takes this amount of time.
However, many practically relevant tensors have additional structure that can be used to render their description and reconstruction efficient.
Here, we consider low-MPS rank tensors, which are a generalization of low-rank matrices and constitute a variational class of tenors that have an efficient description in terms of the matrix product state (MPS) representation.
More precisely, the number of parameters required to express a tensor of fixed MPS rank in said representation scales linearly in $N$.
The question we are trying to answer is whether such low-MPS rank tensors can be recovered from $m$ linear measurements such that $m$ scales polynomially in the intrinsic complexity, i.e.
 polynomially in $N$, and is independent of the dimension of the full space.
Additionally, we want the reconstruction algorithm to be efficient.
For this purpose, it is necessary to consider measurement tensors $A$ that also have an efficient representation, e.g.\ in the MPS tensor format.
In this work, we provide a partial answer to this question
We derive a condition on the measurement tensors that is sufficient for recovery via an alternating-minimization algorithm.
Numerically, show that random Gaussian product tensors are a viable candidate for measurement tensors fulfilling this condition.
The role of the low-MPS rank constraint in this problem is in stark contrast to \cref{chap:error}, where imposing constraints on the parameter to be inferred rendered the problem computationally intractable.
Here, the problem of tensor recovery only becomes tractable with the additional low-rank constraint.
