 % -*- root: ../thesis.tex -*-

\chapter{Tensors}%
\label{chap:tensors}


\todo{The $A\ind{l}$ notation is used for two different things here.}

The theoretical and numerical study of many-body quantum systems is severely hindered by the \emph{curse of dimensionality}, which in this context refers to the exponential growth of the Hilbert space of quantum states w.r.t.\ the number of constituents.
However, many realistic systems do not exhibit this pathological complexity and can be described efficiently in terms of \emph{tensor networks}~\cite{}.
An especially simple but important special case of tensor networks is known under names such as \emph{finitely correlated states}~\cite{Fannes_1992_Finitely} or \emph{matrix-product states} (MPS)~\cite{Garcia_2006_Matrix,Verstraete_2008_Matrix,Orus_2014_Practical}.
\todo{Check this!}
Examples for quantum states that can be efficiently described as MPS include W-, GHZ-, and cluster states.
More generally, low energy states of one-dimensional gapped Hamiltonians with local interactions have an efficient MPS description and vice versa~\cite{}.
Here, \quotes{efficient description} refers to the fact that the number of parameters required to describe such a state scales polynomially in the number of subsystem.

The question we are attempting to answer in this chapter is whether such MPS can be efficiently reconstructed from few linear measurements\footnote{%
  Note that since we assume a linear measurement model, the results of this section do not apply to the problem of estimating pure quantum states.
  We use the term \quotes{matrix product state} to not only refer to quantum states, but to general tensors in the MPS format.
}.
More precisely, we provide evidence that the number of measurements required to recover a MPS is related to its intrinsic complexity -- and hence, scales polynomially in the number of constituents -- although its Hilbert space is exponentially large.
\todo{Two fold: compressed sensing and scalable MMLE}
This work is a natural extension of \emph{low-rank matrix recovery}, which formed the foundation of the results in \cref{chap:phaselift}, to higher-order tensors.\\
\todo{tensors -> higher order, rank}

This chapter is structured as follows:
In \cref{sec:tensors.mps}, we introduce the MPS tensor format.
\Cref{sec:tensors.als} reports on work in progress, which is concerned with recovering MPS from few linear measurements using an \emph{Alternating Least Squares} (ALS) algorithm.
Finally, in \cref{sec:tensors.mpnum} we present the software library \mpnum dealing with tensors in MPS representation.
\mpnum was developed as part of this work to facilitate numerical computations in a user friendly and reusable manner.


\subsection*{Relevant publications}
\begin{itemize}
  \item Ž.\ Stojanac, D.\ Suess, M.\ Kliesch: \textit{On the distribution of a product of N Gaussian random variables}, Proceedings Volume 10394, Wavelets and Sparsity XVII; 1039419 (2017)
  \item Ž.\ Stojanac, D.\ Suess, M.\ Kliesch, \textit{On products of Gaussian random variables}, arXiv:1711.10516
  \item D.\ Suess, M.\ Holzaepfel, \textit{mpnum: A matrix product representation library for Python}, Journal of Open Source Software, 2(20), 465 (2017)
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Matrix Product States}%
\label{sec:tensors.mps}

Tensors are a generalization of vectors and matrices.
Although there is a coordinate-free definition for tensors in terms of multi-linear functionals~\cite{Browder_2012_Mathematical}, we are going to identify a tensor with its coordinate representation here.
\todo{Also no distinguishing co/contravariant indices -> euclidean}
A complex tensor of order $N$ is an element $X \in \Complex^{d_1 \times \cdots \times d_N}$ and the $d_i$ are called \emph{local dimensions}.
Hence, a vector is a tensor of order 1 and a matrix is a tensor of order 2.
For the sake of simplicity, we assume that $d_1 = \cdots = d_N$ throughout this chapter.



\subsection{Graphical Notation}
\label{sub:mps.graphical_notation}

Since formulas with higher-order tensors may become incomprehensible due to the large amount of indices necessary, we introduce a common graphical notation~\cite{Orus_2014_Practical,Bridgeman_2017_HandWaving}.
A tensor $X \in \Complex^{d^N}$ is represented by a geometric shape with legs attached, one for each index.
For example, consider the case $N = 3$, then
\[
  X_{i,j,k}
  =
  \begin{tikzpicture}[baseline=-.5ex]
    \tensor{{S=3, tensor_width=2, len_vertical_legs=0.5}}
    \node at (X) {$X$};
    \node [anchor=north] at (X_S1e) {\small $i$};
    \node [anchor=north] at (X_S2e) {\small $j$};
    \node [anchor=north] at (X_S3e) {\small $k$};
  \end{tikzpicture}
  .
\]
A variable written next to a leg fixes the corresponding index to the given value, while an anonymous tensor leg represents an unmatched index.
In the following, we will make use of the following notation for an unmatched index inspired by Python's and Matlab's syntax:
Given above $X \in \Complex^{d^3}$, we define the slice $X_{:,j,:} \in \Complex^{d^2}$ by
\[
  \label{eq:mps.slicing}
  X_{:,j,:}
  =
  \begin{tikzpicture}[baseline=-.5ex]
    \tensor{{S=3, tensor_width=2, len_vertical_legs=0.5}}
    \node at (X) {$X$};
    \node [anchor=north] at (X_S2e) {\small $j$};
  \end{tikzpicture}
  .
\]
For the components, we have the slightly cumbersome equality $\left(X_{:,j,:}\right)_{i,k} = X_{i,j,k}$.
The advantage of this graphical notation becomes clear once we express tensors composed of other tensors by operations such as index contraction or tensor products:
\begin{description}[font=$\bullet$\ \scshape\bfseries]
  \item[Contractions] are indicated by joining two legs, e.g.\ for matrices $A, B \in \Complex^{d \times d}$ their product $AB$ is written as
    \[
      \begin{tikzpicture}[baseline=-.5ex]
        \tensor{{W=1, E=1, tensor_name='AB'}}
        \node at (AB) {$AB$};
      \end{tikzpicture}
      =
      \begin{tikzpicture}[baseline=-.5ex]
        \tensor{{W=1, E=1, tensor_name='A'}}
        \node at (A) {$A$};
        \tensor{{W=1, E=1, tensor_name='B', x=1.25}}
        \node at (B) {$B$};
      \end{tikzpicture}
      :=
      \sum_k
      \left(
      \begin{tikzpicture}[baseline=-.5ex]
        \tensor{{W=1, E=1, tensor_name='A'}}
        \node at (A) {$A$};
        \node[anchor=west] at (A_E1e) {\small $k$};
        \tensor{{W=1, E=1, tensor_name='B', x=2.5}}
        \node at (B) {$B$};
        \node[anchor=east] at (B_W1e) {\small $k$};
      \end{tikzpicture}
      \right)
    \]

  \item[Tensor products] correspond to drawing two tensors side by side, e.g.\ for $x, y \in \Complex^d$ their tensor product $x \otimes y$ is written as
    \[
      \begin{tikzpicture}[baseline=-.5ex]
        \tensor{{S=2, tensor_width=1.5, tensor_name='xy'}}
        \node at (xy) {$x \otimes y$};
      \end{tikzpicture}
      =
      \begin{tikzpicture}[baseline=-.5ex]
        \tensor{{S=1, tensor_name='x'}}
        \node at (x) {$x$};
        \tensor{{S=1, tensor_name='y', x=1.25}}
        \node at (y) {$y$};
      \end{tikzpicture}
    \]

  \item[Grouping] indices -- that is the canonical identification $\Complex^{d_1} \otimes \Complex^{d_2} \cong \Complex^{d_1 d_2}$ -- is indicated by merging two or more legs together.
    \[
      \tikz[baseline=-.5ex]{\mpa{2}{{W=3, virtual=2, E=1}}}
      \cong
      \begin{tikzpicture}[baseline=-.5ex]
        \tensor{{W=1, E=1, len_horizontal_legs=0, tensor_name='A'}}
        \tensor{{W=1, E=1, len_horizontal_legs=0, tensor_name='B', x=1.25}}
        \draw[line width=2pt] (A_W1) -- +(-0.25,0);
        \draw[line width=1.5pt] (A_E1) -- (B_W1);
        \draw (B_E1) -- +(0.25,0);
      \end{tikzpicture}
      \label{eq:tensors.intro.grouping}
    \]
    This operation is often used in numerical implementations of tensor network algorithms as it reduces most tensor operations to standard matrix  operations.
    For example, the twofold contraction on the left hand side of \cref{eq:tensors.intro.grouping} is converted to a matrix multiplication on the right hand side.
    In the following, we often implicitly perform grouping on neighbouring tensor legs.
\end{description}



\subsection{MPS tensor representation}
\label{sub:mps.tensor_representation}

The tensor representation we are interested in has been established in quantum physics independently under the names \emph{finitely correlated states}~\cite{Fannes_1992_Finitely} and \emph{matrix-product states}~\cite{Klumper_1991_Equivalence,Kluemper_1992_Groundstate}.
Subsequently, it has been rediscovered in other contexts:
It is known under the guise of the \emph{density matrix renormalization group}~\cite{White_1992_Density,Schollwoeck_2011_DensityMatrix} in condensed matter physics.
Furthermore, the MPS representation is also known as a special case of the \emph{Hierarchical Tucker} tensor format~\cite{Hackbusch_2012_Tensor,Grasedyck_2010_Hierarchical} and under name \emph{tensor-train}~\cite{Oseledets_2011_TensorTrain} in the applied math community.
However, similar structures have been known for much longer in the form of \emph{deterministic finite automata}~\cite{Hopcroft_2014_Introduction} and \emph{hidden Markov models}~\cite{Cappe_2006_Inference}.

\todo{More motivation?}

To introduce the MPS representation, consider a tensor $X \in \Complex^{d^N}$.
By splitting off the first index, grouping the remaining $N-1$ indices, and performing a SVD on the resulting matrix, we can factor $X$ into three parts
\[
  \begin{tikzpicture}[baseline=-.5ex]
    \tensor{{S=4, tensor_width=3}}
    \node (X) {$X$};
    \draw[dashed] (-.6,-.8) -- (-.6, .8);
  \end{tikzpicture}
  =
  \begin{tikzpicture}[baseline=-.5ex]
    \tensor{{S=1, E=1, x=-3.3, tensor_name='L'}}
    \node at (L) {$M\ind{1}$};
    \tensor{{W=1, E=1, x=-2.1, tensor_name='lambda', tensor_style='tensornode, rounded corners=0.375cm, fill=red!30'}}
    \node at (lambda) {$\Lambda\ind{1}$};
    \tensor{{W=1, S=3, tensor_width=2.5, tensor_name='R'}}
    \node (R) {$\tilde R_{N-1}$};
  \end{tikzpicture}
\]
where $M\ind{1}$ denotes the left-singular vectors, $\Lambda\ind{1}$ the diagonal matrix composed of the singular values $\lambda\ind{i}$, and $\tilde R_{N-1}$ the left-singular vectors after the index-grouping has been reversed.
For convenience, we contract the singular values with the remainder $\tilde R_{N-1}$ and obtain
\[
  \begin{tikzpicture}[baseline=-.5ex]
    \tensor{{S=4, tensor_width=3}}
    \node (X) {$X$};
  \end{tikzpicture}
  =
  \begin{tikzpicture}[baseline=-.5ex]
    \tensor{{S=1, E=1, x=-2.1, tensor_name='L'}}
    \node at (L) {$M\ind{1}$};
    \tensor{{W=1, S=3, tensor_width=2.5, tensor_name='R'}}
    \node (R) {$R_{N-1}$};
  \end{tikzpicture}
\]
This procedure can now be iterated by performing the same steps on $R_{N-1}$:
\begin{align}
  \begin{tikzpicture}[baseline=-.5ex]
    \tensor{{S=4, tensor_width=3}}
    \node (X) {$X$};
    \draw[dashed] (-.6,-.8) -- (-.6, .8);
  \end{tikzpicture}
  &=
  \begin{tikzpicture}[baseline=-.5ex]
    \tensor{{S=1, E=1, x=-2.1, tensor_name='L'}}
    \node at (L) {$M\ind{1}$};
    \tensor{{W=1, S=3, tensor_width=2.5, tensor_name='R'}}
    \node (R) {$R_{N-1}$};
    \draw[dashed] (-.3,-.8) -- (-.3, .8);
  \end{tikzpicture}\\
  &=
  \begin{tikzpicture}[baseline=-.5ex]
    \mpa{{1,2}}{{S=1, E=1, x=-3.1, tensor_name='L'}}
    \node at (L_1) {$M\ind{1}$};
    \node at (L_2) {$M\ind{2}$};
    \tensor{{W=1, S=2, tensor_width=2}}
    \node (R) {$R_{N-2}$};
    \draw[dashed] (0,-.8) -- (0, .8);
  \end{tikzpicture}\\
  &=
  \begin{tikzpicture}[baseline=-.5ex]
    \mpa{4}{{S=1, tensor_name='L'}}
    \node at (L_1) {$M\ind{1}$};
    \node at (L_2) {$M\ind{2}$};
    \node at (L_3) {$M\ind{3}$};
    \node at (L_4) {$M\ind{4}$};
  \end{tikzpicture}
  \label{eq:mps.matrix_product}
\end{align}
This yields a representation of $X$ in terms of the \emph{local tensors} $M\ind{l}$.
The above algorithm for computing the local tensors is referred to as TT-SVD in \cite{Oseledets_2011_TensorTrain}.
In the following, we identify the index order as follows:
\[
  M\ind{l}_{i,j,k} =
  \begin{tikzpicture}[baseline=-.5ex]
    \tensor{{W=1, S=1, E=1}}
    \node at (X) {$M\ind{l}$};
    \node [anchor=east] at (X_W1e) {\small $i$};
    \node [anchor=west] at (X_S1e) {\small $j$};
    \node [anchor=north] at (X_W1e) {\small $k$};
  \end{tikzpicture}
\]
The horizontal, contracted legs corresponding to the indices $i$ and $k$, are often referred to as \emph{bond} or \emph{virtual legs} and the vertical, unmatched ones corresponding to $j$ are referred to as \emph{physical} legs.
By fixing each physical leg to some value $k_l$ in \cref{eq:mps.matrix_product}, we see that each component of $X$ is given by product of $N$ matrices
\[
  \label{eq:mps.local_matrices}
  M\ind{l}_{k_l} := M\ind{l}_{:,k_l,:},
\]
hence the name \quotes{matrix-product representation}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
  \centering
  \begin{tikzpicture}
    \tensor{{S=4, tensor_name='psi', tensor_width=3, tensor_height=1,
             len_vertical_legs=.5, x=-2.5}}
    \node at (psi) {$X_{ijkl}$};

    \node at (0,0) {\Huge $=$};

    \begin{luacode*}
      local indices = {'i', 'j', 'k', 'l'}
      -- local bond_indices = {'a', 'b', 'c'}

      mptikz.draw_mpa(4, {S=1, tensor_name='A', tensor_width=1, tensor_height=1,
                          len_vertical_legs=.5, len_horizontal_legs=.5, x=1.5})

      for i = 1, 4 do
        t('\\node[anchor=west] at (psi_S%i) {$%s$};', i, indices[i])
        t('\\node[anchor=west] at (A_%i_S1) {$%s$};', i, indices[i])
          t('\\node at (A_%i) {$M\\ind{%i}$};', i, i)
      end

      -- for i =1, 3 do
      --  t('\\node[anchor=north] at (A_%i_E1e) {$%s$};', i, bond_indices[i])
      -- end
    \end{luacode*}

    \draw[pointerline] (psi) -- +(2,-1) node[pointernode] {global tensor};
    \draw[pointerline] (A_2) -- +(1,1) node[pointernode] {local tensor};
    \draw[pointerline] (A_3_E1e) -- +(0.5,1) node[pointernode] {virtual leg};
    \draw[pointerline] (A_3_S1) -- +(-0.5,-0.5) node[pointernode,anchor=east] {physical leg};
  \end{tikzpicture}
  \caption{%
    \label{fig:mps.mps}
    A 4th order tensor in MPS representation with open boundary condition as described in \cref{eq:mps.open_boundary}.
  }
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

So far this construction is exact and completely general, i.e.\ every tensor can be represented in MPS form, but not efficient:
Generically, the SVD in the $i$-th step yields $r_i = \min{d^i, d^{N - i}}$ non-zero singular values $\lambda\ind{i}$, and therefore, the local tensors $M\ind{i}$ have shape $(d, r_{i-1}, r_i)$.
However, suppose that all but $r$ of the singular values in each cut vanish, then we can truncate the local tensor $M\ind{i}$ to shape $(d, r, r)$.
In this case, the number of parameters scales as $\Order(r^2 N d)$ and $X$ can be represented efficiently by matrix products according to \cref{eq:mps.matrix_product}.
Although this assumption may seem overly strict on a first glance, many tensors in our applications of interest are well approximated by such an MPS with small $r$.
The following definition sums up this section so far.

\begin{definition}%
  \label{def:mps.mps}
  A \emph{matrix-product state representation} of a tensor $X \in \Complex^{d^N}$ is defined in terms of a tuple of local tensors $(M\ind{l})_{l \in [N]}$ with $M\ind{l} \in \Complex^{r_{l - 1} \times d \times r_l}$ where $r_0 = r_N$.
  Then, the components of $X$ are given by
  \[
    \label{eq:mps.periodic_boundary}
    X_{i_1,\ldots,i_N} = \tr \left( M\ind{1}_{:,i_1,:}\ \cdots\  M\ind{N}_{:,i_N,:} \right).
  \]
  We say that the MPS representation has \emph{open boundary conditions} if $r_0 = r_N = 1$, otherwise the representation has \emph{periodic boundary conditions}.
\end{definition}

For the sake of simplicity, we restrict our attention to the open boundary case in the following.
In this case, we have
\[
  \label{eq:mps.open_boundary}
  X_{i_1,\ldots,i_N} = M\ind{1}_{i_1}\ \cdots\  M\ind{N}_{i_N},
\]
where we used the shorthand notation from \cref{eq:mps.local_matrices}
As already noted above, the crucial parameter balancing the efficiency of the MPS representation on one hand and its expressive power on the other hand is the size of the virtual legs $r_l$.
This warrants the following definition.

\begin{definition}%
  \label{def:mps.rank}
  Let $(M\ind{l})_{l \in [N]}$ denote an MPS representation with $M\ind{l} \in \Complex^{r_{l - 1} \times d \times r_l}$.
  We call $\max_l r_l$ the (MPS-)\emph{rank} or \emph{bond dimension} of the MPS.
  Furthermore, for any tensor $X$, we define its \emph{MPS-rank} to be the smallest rank of any MPS representation of $X$.
\end{definition}

Note that by~\cite[Thm.\ 2.2]{Oseledets_2011_TensorTrain}, the definition of MPS-rank in \cref{def:mps.rank} agrees with the usual definition using matrications of $X$, and hence, is well defined.
\todo{Fix this}
In contrast to the matrix case, there are multiple inequivalent definitions rank for tensors~\cite{???}.
% Each notion of rank is induced by a tensor representation analogous to the MPS case.
% The choice which format to use in practice is mainly determined by two factors:
% On the one hand, the natural structure of the problem determines, which tensor format yields the most efficient
\\


With the factorization of the tensor $X$ in \cref{def:mps.mps}, we have introduced new virtual degrees of freedom such that the partial trace over them equals $X$.
Clearly, the following gauge transformation leaves \cref{eq:mps.open_boundary} invariant
\begin{align}
  \tilde M\ind{1}_i = M\ind{1}_i R\ind{1},
  \tilde M\ind{N}_i = R\ind{N - 1} M\ind{N}_i, \\
  \tilde M\ind{l}_i = L\ind{l - 1} M\ind{l}_i R\ind{l} \quad (1 < l < N)
\end{align}
provided the (generally non-square) matrices $L\ind{l}$ and $R\ind{l}$ satisfy $L\ind{l} R\ind{l} = \1$ since then
\begin{equation}
  \begin{tikzpicture}[baseline=-.5ex]
    \mpa{4}{{S=1, tensor_name='A', len_horizontal_legs=0.1}}
    \node at (A_1) {$M\ind{1}$};
    \node at (A_2) {$M\ind{2}$};
    \node at (A_3) {$M\ind{3}$};
    \node at (A_4) {$M\ind{4}$};
  \end{tikzpicture}
  =
  \begin{tikzpicture}[baseline=-.5ex]
    \mpa{{1,4,7,10}}{{S=1,tensor_name='B', len_horizontal_legs=0.1}}
    \node at (B_1) {$M\ind{1}$};
    \node at (B_4) {$M\ind{2}$};
    \node at (B_7) {$M\ind{3}$};
    \node at (B_10) {$M\ind{4}$};
    \mpa{{2,5,8}}{{tensor_name='R', E=1, len_horizontal_legs=0.1, tensor_style='tensornode, fill=red!30'}}
    \node at (R_2) {$R\ind{1}$};
    \node at (R_5) {$R\ind{2}$};
    \node at (R_8) {$R\ind{3}$};
    \mpa{{3,6,9}}{{tensor_name='L', len_horizontal_legs=0.1, tensor_style='tensornode, fill=red!30'}}
    \node at (L_3) {$L\ind{1}$};
    \node at (L_6) {$L\ind{2}$};
    \node at (L_9) {$L\ind{3}$};
  \end{tikzpicture}
\end{equation}
As shown in~\cite[Thm. 2]{Garcia_2006_Matrix}, these local transformations on the virtual degrees of freedom are the only possible gauge transformations of the MPS representation.
A number of canonical forms exist that partially fix the gauge~\cite{Garcia_2006_Matrix,Schollwoeck_2011_DensityMatrix,Bridgeman_2017_HandWaving}.
The following definition summarizes the corresponding gauge conditions

\begin{definition}%
  \label{def:mps.canonical}
  We say that a local tensor $M\ind{l}$ is \emph{left-normalized} if it satisfies
  \[
    \sum_k \adj{M\ind{l}_k} M\ind{l}_k = \1 \quad\quad\quad\quad
    \begin{tikzpicture}[baseline=-.5ex]
      \tensor{{W=1,S=1,E=1,tensor_name='A',y=0.5}}
      \tensor{{W=1,N=1,E=1,tensor_name='B',y=-0.5}}
      \draw[tensorleg] (A_W1e) -- (B_W1e);
    \end{tikzpicture}
    =
    \begin{tikzpicture}[baseline=-.5ex]
      \coordinate (A) at (0,0.5);
      \coordinate (B) at (0,-0.5);
      \draw[tensorleg] (A) -- (B);
      \draw[tensorleg] (A) -- +(.5,0);
      \draw[tensorleg] (B) -- +(.5,0);
    \end{tikzpicture}
  \]
  A MPS with all but the rightmost local tensors in left-normalized form is called \emph{left-canonical}.
  Similarly, a \emph{right-normalized} local tensor fulfills
  \[
    \sum_k M\ind{l}_k \adj{M\ind{l}_k} = \1 \quad\quad\quad\quad
    \begin{tikzpicture}[baseline=-.5ex]
      \tensor{{W=1,S=1,E=1,tensor_name='A',y=0.5}}
      \tensor{{W=1,N=1,E=1,tensor_name='B',y=-0.5}}
      \draw[tensorleg] (A_E1e) -- (B_E1e);
    \end{tikzpicture}
    =
    \begin{tikzpicture}[baseline=-.5ex]
      \coordinate (A) at (0,0.5);
      \coordinate (B) at (0,-0.5);
      \draw[tensorleg] (A) -- (B);
      \draw[tensorleg] (A) -- +(-.5,0);
      \draw[tensorleg] (B) -- +(-.5,0);
    \end{tikzpicture}
  \]
  and a \emph{right-canonical} MPS has all local tensors in right-normalized form except for the first.
\end{definition}

The exceptions for the local tensors at the beginning and end of the chain are necessary to accommodate tensors with Frobenius norm different from one.
In practice, we often deal with a mixed left-right-canonical form, e.g.\ in variational algorithms updating one local tensor at a time such as DMRG or the alternating least squares algorithm presented in \cref{sec:tensors.als}.
For these purposes, efficient algorithms exist that transform an MPS to a canonical form~\cite{Schollwoeck_2011_DensityMatrix,Orus_2014_Practical}.
If we neglect transformations that change the bond dimensions, the remaining gauge group is unitary, and hence, computations on canonical MPS are far more stable numerically.\\


We have already emphasized the ability of the MPS representation to represent certain tensors efficiently, i.e.\ with a number of parameters scaling polynomially in the order of the tensor.
One crucial advantage of this tensor format is that it also facilitates efficient arithmetical operations for tensors~\cite{Schollwoeck_2011_DensityMatrix,Orus_2014_Practical}.
In other words, operations such as sums and contractions of MPS are also MPS and the corresponding local tensors can be computed efficiently from the local tensors of the inputs.
For example, consider the scalar product of two tensors $A, B \in \Complex^{d^N}$:
\[
  \braket{A, B} =
  \begin{tikzpicture}[baseline=-.5ex]
    \mpa{3}{{S=1, y=.5, tensor_name='B'}}
    \mpa{3}{{N=1, y=-.5, tensor_name='A'}}
    \begin{luacode*}
      for i = 1,3 do
        t('\\node at (A_%i) {$A\\ind{%i}$};', i, i)
        t('\\node at (B_%i) {$B\\ind{%i}$};', i, i)
      end
    \end{luacode*}
  \begin{scope}[on background layer]
    \fill[tensornode,fill=black, opacity=0.1] ($ (A_1) - (0.5,0.5) $) rectangle ($ (B_1) + (0.5,0.5) $);
    \fill[tensornode,fill=black, opacity=0.1] ($ (A_2) - (0.5,0.5) $) rectangle ($ (B_2) + (0.5,0.5) $);
    \fill[tensornode,fill=black, opacity=0.1] ($ (A_3) - (0.5,0.5) $) rectangle ($ (B_3) + (0.5,0.5) $);
  \end{scope}
  \end{tikzpicture}
  \label{eq:mps.inner}
\]
By contracting the physical legs of the local tensors first as indicated by the gray boxes\footnote{%
  Note that this strategy is not optimal in many cases~\cite{Schollwoeck_2011_DensityMatrix} but suffices as an example here.
},
we obtain an MPS representation of a tensor of 0th order, i.e.\ a scalar, which can be contracted afterwards to compute its value.

As most arithmetic operations increase the virtual dimension substantially, we need methods to approximate the result by another MPS with lower virtual dimension while keeping the approximation error small.
Although computing the \emph{best} rank $r$ approximation of an MPS is computationally infeasible in general~\cite{Hillar_2013_Most}, there are efficient algorithms that produce good results in practice:
One of them is \emph{SVD compression}, which successively performs an SVD followed by a truncation of negligible singular values on the local tensors similar to the higher-order SVD introduced in \cref{eq:mps.matrix_product}~\cite{Schollwoeck_2011_DensityMatrix}.
If we truncate all but the $r$ largest singular values in each step, we obtain a quasi-optimal rank $r$ approximation of $X \in \Complex^{d^N}$, i.e.\
\[
  \norm{X - X_\mathrm{SVD}} \le \sqrt{N - 1} \norm{X - X_\mathrm{best}}.
  \label{eq:mps.quasi_optimal}
\]
Here, $\norm{\cdot}$ denotes the Frobenius norm of tensors, $X_\mathrm{SVD}$ the rank $r$ SVD compression, and $X_\mathrm{best}$ the best rank $r$ approximation of $X$.



\todo{Mention CANDECOMP/PARAFAC}
% \Cref{def:mps.rank} generalizes the notion of \quotes{matrix rank} to tensors of higher orders based on the MPS tensor format.
% Similar to the approach given in this section, the matrix rank can be defined in terms of a factorizing representation:
% The rank of a $d \times d$ matrix $X$ is the smallest integer $r$ such that $X$ can be written as a product of two $d \times r$ matrices $U$ and $V$:
% \[
%   A = U \adj{V}
%   \label{eq:mps.matrix_rank}
% \]
% Alternatively, the matrix rank can be defined in terms of sums of rank-1 matrices, i.e.\ as the smallest number of summands $r$ such that $X$ can be written as
% \[
%   A = \sum_{l=1}^r \ket{u\ind{l}} \bra{v\ind{l}}
%   \label{eq:mps.matrix_cannon_rank}
% \]
% with $u\ind{l}, v\ind{l} \in \Complex^d$.
% Of course, Eqs.~\eqref{eq:mps.matrix_rank} and \eqref{eq:mps.matrix_cannon_rank} are equivalent by the simple identification $u\ind{l} = U_{:,l}$ and $v\ind{l} = V_{:,l}$.

% The natural generalization of the matrix rank induced by \eqref{eq:mps.matrix_cannon_rank} to tensors of higher order is known as the \emph{canonical tensor rank}~\cite{Kolda_2009_Tensor}.
% For $X \in \Complex^{d^N}$, it defined as the smallest number of rank-1 tensors that add up to $X$:
% \[
%   X = \sum_{l=1}^r a\ind{l}_1 \otimes \cdots a\ind{l}_N, \quad a\ind{l}_i \in \Complex^d.
%   \label{eq:mps.canonical_decomposition}
% \]
% In other words, the canonical tensor rank is induced by the tensor decomposition~\eqref{eq:mps.canonical_decomposition}, which is known as \emph{CANDECOMP/PARAFAC}~\cite{Kolda_2009_Tensor}.
% In contrast to the matrix case, these two notions of tensor rank are not equivalent~\cite{}.

% Although it captures the natural structure of many problems~\cite{} and only requires $\Order(r N d)$ parameters, it has shortcomings in practice:
% First and foremost, computing the canonical tensor rank is $\NP$-complete~\cite{Hastad_1990_Tensor}.
% Furthermore, the best rank-$r$ approximation of a given tensor may not exist and it is also hard to compute if it exists~\cite{Hastad_1990_Tensor,Hillar_2013_Most,Lim}.
% In general, the CANDECOMP/PARAFAC decomposition is hard to deal with computationally.
% This is the reason why the MPS tensor format is becoming more popular.


\subsection{Applications of the MPS format}%
\label{sub:mps.applications}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
  \centering
  \begin{tikzpicture}
    \fill[fill=yellow!30,line width=0] (0,0) ellipse (3cm and 1.5cm) node[anchor=south] {\textbf{Many-body Hilbert space}};
    \coordinate (mpsstates) at (1.7, -1);
    \fill[fill=blue!30,line width=0] (mpsstates) ellipse (0.2cm and 0.1cm);
    \draw[<-] (mpsstates) -- (3,-0.5) node[anchor=west] {MPS with small bond dimension};
  \end{tikzpicture}
  \caption{\label{fig:mps.cartoon}%
    The manifold of quantum states with an efficient MPS description occupies only a tiny corner in the full Hilbert space.
  }
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The main application of the MPS representation in quantum information and condensed matter physics is the efficient description of certain many-body states.
As the corresponding Hilbert space grows exponentially fast in the number of constituents $N$, the full description of any state in terms of its coefficients w.r.t.\ a fixed basis is only feasible for small $N$.
Fortunately, not all quantum states of a many-body system are equally relevant in practice.
Many systems of interest are well described by a Hamiltonian with local interactions, e.g.\ nearest neighbour interactions, which reflects in the structure of correlations in their low energy spectrum.
More specifically, low energy eigenstates of gapped Hamiltonians with local interactions obey the \emph{area-law} for entanglement entropy~\cite{Hastings_2006_Solving,Verstraete_2006_Matrix,Eisert_2010_Colloquium}:
For those states, the entanglement entropy of a subsystem asymptotically only depends on its boundary size and not on its volume.
Although those stats occupy only a small corner of the enormous many-body Hilbert space as depicted in \cref{fig:mps.cartoon}, they are exceptionally important in practice.
%   - time evolution stays there -> see Orus 3.4

In one dimensional systems, any many-body state $\ket{psi}$ with an area law\footnote{%
  In one dimension, the boundary area of any connected region is constant and independent of the size of the region, and therefore, the area law implies constant entanglement entropy.
}
also has an efficient representation.
If we expand $\ket{\psi}$ in a product basis with coefficient tensor $C$,
\[
  \ket\psi = \sum_{i_1,\ldots,i_N} C_{i_1,\ldots,i_N} \, \ket{i_1} \otimes \cdots \otimes \ket{i_N},
  \label{eq:mps.state_parametrization}
\]
then $C$ can be efficiently approximated by an MPS with bond dimension scaling polynomially in $N$ and the inverse approximation error~\cite{Hastings_2006_Solving,Verstraete_2006_Matrix,Eisert_2010_Colloquium,Arad_2013_Area,Arad_2016_Rigorous}.
States of the form~\eqref{eq:mps.state_parametrization} are appropriately referred to as matrix product states.
% explains DMRG, TEBD

For higher dimensional regular latices, one can generalize the MPS representation.
The resulting efficient tensor network representation is referred to as \emph{projected entangled pair states} (PEPS).
However, a statement analogous to the one above does not hold:
For dimensions $D > 1$, not every state satisfying an area law can be efficiently represented as a PEPS.\\


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
  \centering
  \begin{tikzpicture}
    \tensor{{N=3, S=3, tensor_name='rho', tensor_width=3, tensor_height=1,
             len_vertical_legs=.5, x=-2.5}}
    \node at (rho) {$A$};

    \node at (0,0) {\Huge $=$};

    \begin{luacode*}
      mptikz.draw_mpa(3, {N=1, S=1, tensor_name='A', tensor_width=1, tensor_height=1,
                          len_vertical_legs=.5, len_horizontal_legs=.5, x=1.5})

      for i = 1, 3 do
        t('\\node[anchor=west] at (rho_N%i) {$i_%i$};', i, i)
        t('\\node[anchor=west] at (A_%i_N1) {$i_%i$};', i, i)
        t('\\node[anchor=west] at (rho_S%i) {$j_%i$};', i, i)
        t('\\node[anchor=west] at (A_%i_S1) {$j_%i$};', i, i)
        t('\\node at (A_%i) {$M\\ind{%i}$};', i, i)
      end
    \end{luacode*}

    \draw[pointerline] (A_1_N1e) -- +(0.2,0.5) node[pointernode] {row leg};
    \draw[pointerline] (A_1_S1e) -- +(0.2,-0.5) node[pointernode] {column leg};
    \draw[pointerline] (A_2_E1e) -- +(0.3,1.5) node[pointernode] {virtual leg};
  \end{tikzpicture}
  \caption{%
    \label{fig:mps.mpo}
    An MPO with open boundary condition as described.
  }
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


So far, we were only concerned with multi-body pure states.
The MPS tensor format from \cref{sub:tensors.mps} can also be adapted for the description of mixed sates.
As an example consider a MPS\footnote{%
  From now on we do not distinguish between the state and its corresponding coordinate representation w.r.t.\ a fixed product basis.
}
$\ket\psi$.
The corresponding projector can be written as
\[
  \ket{\psi}\bra{\psi} =
  \begin{tikzpicture}[baseline=-.5ex]
    \mpa{4}{{N=1, y=.3, tensor_width=0.5, tensor_height=0.5, len_horizontal_legs=0.1}}
    \mpa{4}{{S=1, y=-.3, tensor_width=0.5, tensor_height=0.5, len_horizontal_legs=0.1}}
  \end{tikzpicture} =:
  \begin{tikzpicture}[baseline=-.5ex]
    \mpa{4}{{N=1, S=1, tensor_width=0.5, tensor_height=0.5, len_horizontal_legs=0.1}}
  \end{tikzpicture}
  \label{eq:mps.pure_state_projector}
\]
Here, the right hand side is a \emph{matrix product operator} (MPO).
Its local tensors $A\ind{l}$ are defined in terms of the local tensors $B\ind{l}$ of $\ket\psi$ as follows
\[
  \begin{tikzpicture}[baseline=-.5ex]
    \tensor{{N=1, S=1, W=1, E=1, tensor_name='A'}}
    \node at (A) {$A\ind{l}$};
  \end{tikzpicture}
  =
  \begin{tikzpicture}[baseline=-.5ex]
    \tensor{{N=1, W=1, E=1, tensor_name='Bu', y=0.5}}
    \node at (Bu) {$\cc{B\ind{l}}$};
    \tensor{{S=1, W=1, E=1, tensor_name='Bu', y=-0.5}}
    \node at (Bu) {${B\ind{l}}$};
  \end{tikzpicture}
  \label{eq:mps.mpdo_local_tensors}
\]
with implicit grouping of the virtual indices of $B\ind{l}$.
This is an example of a matrix-product density operator (MPDO), which were introduced in \cite{Verstraete_2004_Matrix,Zwolak_2004_MixedState}.
In general, any operator acting on the many-body Hilbert space can be expressed as an MPO with local tensors $A\ind{l} \in \Complex^{d \times d \times r \times r}$ as depicted in \cref{fig:mps.mpo}.
This representation is efficient if the operator acts \quotes{locally}, e.g.\ for a Hamiltonian with nearest neighbor interactions.

The explicit parametrization~\eqref{eq:mps.mpdo_local_tensors} of the local tensors makes it easy to see that the corresponding MPO is positive semidefinite (psd), and hence, represents a valid physical state.
A crucial question for numerical computations with density operators expressed as MPO is if there is an efficient algorithm that decides whether the
MPO is psd or not.
Unfortunately, this problem is $\NP$-hard, even if we restrict to dimensions $d=2$ and translational invariant states~\cite{Kliesch_2014_Matrix}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
  \centering
  \begin{tikzpicture}
    \tensor{{S=6, tensor_name='psi', tensor_width=4, tensor_height=1,
             len_vertical_legs=.5, x=-3}}
             \node at (psi) {$\ket{\psi}$};

    \node at (0,0) {\Huge $=$};

    \begin{luacode*}
      mptikz.draw_mpa(3, {S=2, tensor_name='A', tensor_width=1.5, tensor_height=1,
                          len_vertical_legs=.5, len_horizontal_legs=.25, x=2})

      for i = 1, 3 do
        t('\\node[anchor=west] at (psi_S%i) {$i_%i$};', 2 * i - 1, i)
        t('\\node[anchor=west] at (A_%i_S1) {$i_%i$};', i, i)
        t('\\node[anchor=west] at (psi_S%i) {$j_%i$};', 2 * i, i)
        t('\\node[anchor=west] at (A_%i_S2) {$j_%i$};', i, i)
      end
    \end{luacode*}

    \draw[pointerline] (A_1_S1e) -- +(-0.2,-0.5) node[pointernode,anchor=east] {physical leg};
    \draw[pointerline] (A_1_S2e) -- +(0.2,-0.5) node[pointernode] {auxilliary leg };
  \end{tikzpicture}
  \caption{%
    \label{fig:mps.pmps}
    An PMPS with open boundary condition as described.
  }
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
One way to cope with this problem is to use an inherently psd parameterization of the mixed state.
Denote by $\mathcal{H}_1$ the Hilbert space of the system and by $\rho$ the corresponding mixed state.
By introducing a second \emph{auxiliary} Hilbert space $\mathcal{H}_2$, we can write~\cite{Nielsen_2010_Quantum}
\[
  \rho = \tr_{\mathcal{H}_2} \ket{\psi}\bra{\psi},
  \label{eq:mps.purification}
\]
where $\ket{\psi} \in \mathcal{H}_1 \otimes \mathcal{H}_2$.
Note that we can take $\mathcal{H}_2$ to be of the same dimension as $\mathcal{H}_1$.

Similarly, a \emph{local purification matrix product state} (PMPS)~\cite{Cuevas_2013_Purifications} is a matrix product state with two legs per site as depicted in \cref{fig:mps.pmps} such that
\[
  \underbrace{%
    \begin{tikzpicture}[baseline=-.5ex]
      \mpa{4}{{N=1, S=1, tensor_width=0.5, tensor_height=0.5, len_horizontal_legs=0.1}}
    \end{tikzpicture}
  }_{\rho}
  =
  \begin{tikzpicture}[baseline=-.5ex]
    \mpa{4}{{N=2, y=.3, tensor_width=0.5, tensor_height=0.5, len_horizontal_legs=0.1, tensor_name='up'}}
    \mpa{4}{{S=2, y=-.3, tensor_width=0.5, tensor_height=0.5, len_horizontal_legs=0.1, tensor_name='down'}}
    \draw[tensorleg] (up_1_N2e) to[out=75,in=-75] (down_1_S2e);
    \draw[tensorleg] (up_2_N2e) to[out=75,in=-75] (down_2_S2e);
    \draw[tensorleg] (up_3_N2e) to[out=75,in=-75] (down_3_S2e);
    \draw[tensorleg] (up_4_N2e) to[out=75,in=-75] (down_4_S2e);
  \end{tikzpicture}
  \label{eq:mps.pmps}
\]
This is an inherently positive semidefinite parametrization of $\rho$ in matrix product form.
However, this advantage comes at a price:
The PMPS representation of a state can be arbitrarily more costly than its MPDO representation~\cite{Cuevas_2013_Purifications}.
More precisely, there a families of states $(\rho_N)_{N \in \Naturals}$  with $\rho_N \in \Complex^{d^N}$ such that $\rho_N$ has constant MPO-rank, but the PMPS-rank of $rho_N$ scales as $\Order(N)$.
This and the hardness result from~\cite{Kliesch_2014_Matrix} shows that no efficient algorithm for computing an exact local purification from an MPDO exists in general.
However, the questions how generic this hardness is and how hard approximative versions of this problem are remain open?\\



Besides the original applications in physics, the MPS tensor format has recently also found applications in the field of machine learning:
Deploying the latest neural networks on mobile devices such as smart phones or IOT devices is challenging due to the high dimensional hidden layers, e.g. a single weight matrix of a fully connected layer can have millions of parameters.
By approximating this weight matrix by an MPS with small bond dimension, the authors in~\cite{Novikov_2015_Tensorizing} were able to compress state-of-art image detection networks by a factor of seven with only minor performance penalties.
Other examples of applications of tensor decompositions in deep learning include regularization~\cite{Tai_2015_Convolutional} and deep representation learning~\cite{Yang_2016_Deep}.

Another use of the MPS tensor format in machine learning was pioneered in~\cite{Stoudenmire_2016_Supervised}.
They consider a support vector machine classifier, which for the binary (yes/no) outcome is given by
\[
  f(X) = \braket{W, X},
\]
where $X$ is the input vector and $W$ is the model vector.
The goal is to determine $W$ in such a way that $f(X) > 0$ if $X$ is in the \quotes{yes} class and $f(X) < 0$ otherwise.
By using a MPS representation for $W$ and $X$, they are able to efficiently learn such a model even in high-dimensional settings.
A similar can also be applied to learning polynomial classifiers~\cite{Chen_2017_Parallelized} as well as for image compression and classification~\cite{Bengua}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Python Library mpnum}%
\label{sec:tensors.mpnum}

\input{notebooks/Introduction}
\todo{Examples for thermal state computation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Scalable maximum likelihood estimation for quantum states}%
\label{sec:tensors.mle}

\todo{Leave this out?}
% QSE -> see chapter ..., number of measurements without further assumptions ~ dimension of hilber space ^2 -> exponential for many body states
% slight reduction using low-rank matrix recovery, but still exponential
% here: assuming efficient MPO description -> # of measurements -> linear in N
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Efficient low-rank tensor reconstruction}%
\label{sec:tensors.als}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
  \centering
  \input{tikz/tensors_local_measurements.tex}
  \caption{%
    The local measurements used for the reconstruction of MPS, MPO, and unitary channels in~\cite{Cramer_2010_Efficient,Baumgratz_2013_Scalable,Baumgratz_2013_Scalablea,Lanyon_2017_Efficient,Holzaepfel_2014_Scalable}.
    These consist of informationally complete measurements on blocks of $R$ consecutive qudits, e.g.\ all Pauli product measurements on $R$ qudits.
    }%
  \label{fig:als.quantum_measurements}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

With the ability to approximate structured tensors from many applications efficiently in MPS form, one crucial question remains:
How can we \emph{efficiently} recover this representation in practice from measurable quantities?
Here, \quotes{efficiency} refers to two different aspects.
On the one hand, we want to bound the number of measurements required to perform reconstruction, i.e.\ the \emph{sample complexity}.
On the other hand, it refers to the \quotes{computational complexity} as outlined in \cref{sec:error.complexity}
Both aspects are especially critical for tensor reconstruction as na\"ive approaches generally suffer from the curse of dimensionality.

In the context of quantum estimation, existing work addresses this question for the reconstruction of MPS~\cite{Cramer_2010_Efficient}, MPDO~\cite{Baumgratz_2013_Scalable,Baumgratz_2013_Scalablea,Lanyon_2017_Efficient}, and unitary quantum processes~\cite{Holzaepfel_2014_Scalable} from local measurements.
More precisely, they consider measurements that act non-trivially only on blocks of $R$ adjacent qudits as depicted in \cref{fig:als.quantum_measurements}.
This results in a sample complexity scaling as $\Order(N \times d^R)$ compared to the $m = \Order(d^N)$ for the na\"ive approach.
Numerical experiments in~\cite{Cramer_2010_Efficient,Baumgratz_2013_Scalable,Baumgratz_2013_Scalablea} demonstrate successful recovery of W-states as well as ground and thermal states of nearest-neighbor Hamiltonians for small values of $R$ independent of $N$.
Naturally, these are all examples of MPS or MPO with small bond dimension.
The drastically reduced sampling complexity makes the approaches efficient and viable for large-scale quantum experiments.
However, only the numerically inferior algorithm\footnote{%
  T.\ Baumgratz, private communications.
}
from~\cite{Baumgratz_2013_Scalable} comes with a proof of convergence.
Similar rigorous recovery guarantees have only been proven for comparable, but inefficient versions of the algorithms in~\cite{Cramer_2010_Efficient,Baumgratz_2013_Scalablea}.\\



The question that motivated the work presented in this chapter is whether we can identify other measurement schemes and algorithms that \emph{provably} allow for efficiently reconstructing low-rank tensors.
Recall that we already encountered a similar question in \cref{cha:phaselift} for the problem of \emph{low-rank matrix recovery}.
The latter asks under which conditions we can recover a low-rank matrix $X \in \Complex^{d \times d}$ from $m$ linear measurements of the form
\[
  y\ind{l} = \braket{A\ind{l}, X}, \quad l=1, \ldots, m.
  \label{eq:als.matrix_measurements}
\]
Here, $A\ind{l} \in \Complex^{d \times d}$ denote the measurement matrices and
\[
  \braket{A, X} = \tr \adj{A} X
\]
the Frobenius inner product.
For a general matrix $X$, the number of measurements $m$ needs to scale as $d^2$ by the pigeonhole principle.
However, by exploiting the low-rank structure of $X$, we can reduce $m$.
For example, \cref{thm:???} shows that we can recover any positive semi-definite rank-1 matrix $X$ from only $m = \Order(d)$ measurements provided the $A\ind{l}$ are sampled from an appropriate distribution of random matrices.
Furthermore, said theorem also provides an efficient reconstruction algorithm, namely the semi-definite program~\eqref{eq:???}.

More generally, we can recover any $X \in \Complex^{d \times d}$ with $\rank X = r$ from $m = \Order(d r)$ randomly chosen measurements~\cite{Candes_2011_Tight,Kueng_2014_Low}.
Intuitively, this sample complexity is asymptotically optimal since we need at least $2 d r$ complex parameters to specify the left- and right-singular vectors of $X$, see~\cite{Eldar_2012_Uniqueness,Li_2017_Optimal} for rigorous lower bounds.\\




Here, we consider the generalization of low-rank matrix recovery to higher order tensors\footnote{%
  In contrast to the rest of this chapter, we consider real tensors in order to simplify notation.
}
$X \in \Reals^{d^N}$ of low MPS-rank.
For this purpose, we study an \emph{Alternating Least Squares} (ALS) algorithm inspired by DMRG methods in quantum physics.
The observable quantities are -- analogous to \cref{eq:als.marix_measurements} -- given by overlaps with measurement tensors $A\ind{l} \in \Reals$
\[
  y\ind{l} = \braket{A\ind{l}, X}
  \label{eq:als.tensor_measurements}
\]
with the Frobenius inner product of tensors defined by \cref{eq:mps.inner}.

The field of low-rank tensor recovery has attracted increasing attention in recent years.
Especially the problem of tensor completion, i.e.\ inferring missing values of a low-rank tensor, has been thoroughly studied due to its broad applicability in computer vision, neuroscience, remote sensing, and context-aware recommender systems~\cite{Li_2010_Tensor,Zhu_2016_ContextAware,Wang_2014_LowRank}.
In contrast to our work, most analytical results in this area are concerned with different notions of tensor rank such as the Tucker rank~\cite{Kressner_2013_LowRank,Zhang_2016_Cross} or the canonical tensor rank~\cite{Krishnamurthy_2013_LowRank,Potechin_2017_Exact}.
\todo{Why Tucker/canonical bad?, Why TT good}

The best analytical recovery guarantees for low-MPS rank tensor completion require -- to the best of the authors knowledge -- a number of measurements scaling exponentially in the order of the tensor~\cite{Phien_2016_Efficient}.
Although their result gives a square-root advantage compared to na\"ive reconstruction, it is still infeasible for high-order tensors.
However, numerical investigations using an alternating least squares algorithm similar to the one used in this work demonstrate successful recovery with sub-exponential sample complexity~\cite{Grasedyck_2015_Variants,Wang_2016_Tensor}.

% low-rank approximation
% Riemannian optimization -> Lubich
% ALtmin local convergence for Optimization-> Rohwedder, Uschmajew; Holtz, Rohwedder\tabularnewline

Stronger analytical results exist for a different measurement ensemble:
The authors in~\cite{Rauhut_2014_Tensor}, consider fully Gaussian measurements, i.e.\ measurement tensors with independent components sampled from a normal distribution.
In this approach, a number of such measurements scaling polynomially in the crucial parameters $d$, $N$, and $r$ is sufficient to recover any rank $r$ tensor.
Although their result is highly sample efficient, it is still infeasible for high-order tensors due to the exponentially scaling memory requirement for the measurement tensors.

In conclusion, the existing rigorous work on recovering low MPS-rank tensors is currently situated at two different ends of a spectrum:
On the on hand, there is tensor completion, which is highly relevant for practical applications and efficiently implementable, but without any recovery guarantees for a polynomially scaling number of measurements.
On the other hand, the authors in~\cite{Rauhut_2014_Tensor} prove reconstruction with near-optimal sample complexity, but their full Gaussian measurements are computationally very demanding.

This situation can be understood better by comparing to the history of low-rank matrix recovery and compressive sensing -- the related problem of reconstructing sparse vectors from few measurements.
The first rigorous on compressed sensing from Gaussian measurements were due to Donoho~\cite{For_most} in 2006.
In the same year, Candes and Tao -- a Fields medalist -- proved guarantees for orthonormal basis vector measurements~\cite{Stable_signal}, which can be considered as the analogue of matrix completion for vectors.
The same authors also proved the first matrix recovery guarantees for Gaussian sampling matrices~\cite{} and for matrix completion~\cite{}.
Here, we are going to study a measurement ensemble for tensors that combines the advantages of both approaches.
We take the measurement tensors to be of the form
\[
  A\ind{l} = a\ind{l}_1 \otimes \cdots a\ind{l}_N
  \label{eq:als.rank1_measurements}
\]
with local tensors randomly chosen from a standard multivariate normal distribution $a\ind{l}_k \sim \Normal(0, \1_d)$.
In contrast to the fully Gaussian model from~\cite{}, the measurement tensors~\eqref{eq:als.rank1_measurements} can be represented efficiently as MPS of unit rank.
And, unlike the tensor completion problem, history does not dictate that its solution requires at least one Fields medalist.



\subsection{The alternating least squares algorithm}%

In this section, we are going to report on work in progress for the problem of reconstructing low-rank tensors from few measurements.
To recapitulate, the goal is to recover a tensor $X \in \Reals^{d^N}$ with MPS-rank\footnote{%
  From now on, we simply say \quotes{rank} instead of \quotes{MPS-rank}.
} $r$ from $M$ linear measurements of the form
\[
  b\ind{l} = \braket{A\ind{l}, X}, \ldots l=1,\ldots,M,
\]
where $A\ind{l}$ is a product of Gaussian vectors as defined in \cref{eq:als_rank1_measurements}.
For this purpose, we want an efficient reconstruction algorithm and $m$ to scale polynomially in the parameters $d$, $N$, and $r$.

The idea to recover $X$ is simply to find the tensor $Y$ of desired rank that minimizes the empirical $\ell_2$ error
\[
  \sum_l \left( y\ind{l} - \braket{A\ind{l}, Y} \right)^2.
  \label{eq:als.empirical_l2}
\]
However, this problem is generally hard to solve directly since the space of MPS of given rank is non-convex.
Nevertheless, in the case of $M=\Order(???)$ fully Gaussian measurements, a projected gradient descent on~\eqref{eq:als.empirical_l2} is able to recover $X$.
However, the same proof techniques fail in the case of structured measurements such as the ones defined in \cref{eq:als.rank1_measurements} as we show in \cref{???}.

Instead of updating each local tensor at the same time as in the gradient descent, we iteratively optimize the empirical error over a single local tensor at a time.
The resulting algorithm is presented below:

\begin{algorithm}[H]
  \caption{\label{alg:als}Alternating Least Squares (ALS) for $\ell_2$ minimization}
  \SetKwInOut{Input}{Input}
  \SetKwInOut{Output}{Output}

  \Input{%
    Measurement tensors $A\ind{l}$, measurement outcomes $y\ind{l}$, target rank $r$, and number of epochs $H$ \newline
    Divide $(A\ind{l}, y\ind{l})_l$ into $HN + 1$ subsets of size $m$, which are denoted by ${\left(A\ind{0;l}, y\ind{0;l}\right)}_l$ and ${\left(A\ind{h,n;l}, y\ind{h,n;l}\right)}_l$ ($h \in [H], n \in [N]$)
  }
  \BlankLine

  \tcc{Initialize $Y$ as MPS with given rank}
  $Y \gets \mathtt{compress}\left(\sum_{l=1}^m y\ind{0;l} A\ind{0;l}, r\right)$ \label{line:als.initialization}

  \For{$h \gets 1$ \KwTo $H$}{
    \tcc{right-normalize all local tensors}
    $\mathtt{right\-canonicalize}(Y)$ \label{line:als.right_canonical}

    \For{$n \gets 1$ \KwTo $N$}{
      \For{$l \gets 1$ \KwTo $m$} {
        \tcc{contract $A\ind{h,n;l}$ with all but n-th local tensors of $X$}
        $B\ind{l} \gets \mathtt{contract}(A\ind{h,n;l}, X_{[N] \backslash n})$ \label{line:als.partial_contraction}
      }
      $\hat Z \gets \argmin_Z \sum_l \left( y\ind{h,n;l} - B\ind{l}Z   \right)^2$ \label{line:als.min}

      \tcc{update the n-th local tensor inplace}
      $Y_n \gets \mathtt{left\_normalize}(\hat Z)$
    }
  }

  \Output{Y}
\end{algorithm}

In this version of the alternating least squares scheme, we start updating the left-most tensor and then move through the chain all the way to the right.
When we reach the last tensor of the MPS, we start again on the left after bringing the MPS to right-canonical form.
Hence, all tensors to the left and right of the currently updated tensor are left- and right-normalized, respectively.
This process is repeated $H$ number of times.
In practice, we make the algorithm slightly faster by skipping the right-canonicalization step in line~\ref{line:als.right_canonical} and perform a sweep left-right-left instead.

The crucial step in this algorithm is the local minimization in line~\ref{line:als.min}.
It amounts to keeping all but the $n$-th local tensor of $Y$ fixes and minimizing the empirical error over $Y_n$.
The minimizer $\hat Z$ can be computed efficiently from the linear least squares problem
\[
  \hat Z
  = \argmin_Z \sum_l \left( y\ind{h,n;l} - B\ind{l}Z   \right)^2
  = \norm{y\ind{h,n} - B Z}_{\ell_2}^2.
  \label{eq:als.llsq}
\]
Here, the rows of the matrix\footnote{%
  More precisely, we have $B \in \Reals^{m \times (rd)}$ for $n=1$ or $n=N$.
}
$B \in \Reals^{m \times (dr^2)}$ are defined in line~\ref{line:als.partial_contraction} in terms of partial contractions of $A\ind{h,n;l}$ and $Y$ leaving out the $n$-th local tensor of $Y$:
\[
  B\ind{l} =
  \begin{tikzpicture}[baseline=-.5ex]
    \mpa{{1,2,4,5,6}}{{S=1, y=.5, tensor_height=0.5, tensor_width=0.5, tensor_name='Y'}}
    \mpa{6}{{N=1, virtual=0, y=-.5, tensor_height=0.5, tensor_width=0.5, tensor_name='A'}}

    \draw[pointerline] (Y_6) -- +(1,.125) node[pointernode] {$Y$};
    \draw[pointerline] (A_6) -- +(1,-.125) node[pointernode] {$A\ind{h,n;l}$};
  \end{tikzpicture}
  \label{eq:als.minimization_matrix}
\]
Therefore, the solution $\hat Z$ of \cref{eq:als.llsq} is a $d r^2$ dimensional vector, which can be reshaped to the correct form.
If we replace the exact local minimization in \cref{eq:als.llsq} by a finite gradient descent step, we obtain a standard nonlinear block Gauss–Seidel iteration~\cite{Schechter}.

Finally, note that the sample splitting at the beginning is necessary for the analysis of the algorithm below as it requires independent updates at each step.
Therefore, we use a fresh batch of measurement tensors and measurement values in each step, which results in a total number of $M = (HN + 1)m$ measurements.
Provided that both $H$ and $m$ scale polynomially in the system's parameters, then so does $M$.
However, numerical experiments below show that this resampling is unnecessary in practice.\\



Alternating algorithms such as ALS updating only a few local tensors at a time are a very common approximation technique for circumventing intractabilities when dealing with MPS.
Well known examples include variational compression and DMRG~\cite{Schollwoeck} --  an iterative algorithm for approximating the smallest eigenvalue of a hermitian MPO.
Local convergence of these alternating algorithms has been proved for a large class of problems in~\cite{Rohwedder}.
However, these result only show that the ALS algorithm converges to a local minimum, which unless the initialization is chosen well differs from the target tensor $X$.
Furthermore, it remains to be shown that the minimizer of the empirical $\ell_2$ error~\eqref{eq:als.empirical_l2} with given rank is equal to $X$, i.e.\ that the given measurements suffice to identify $X$.


\subsection{Analysis of the ALS}%
\label{sub:ana}

We now turn to the problem of proving convergence of \cref{alg:als}.
For this purpose, we generalize the methods from~\cite{Zhong} to the tensor case.
To simplify notation, we are only going to treat the case of a product signal tensor, i.e.\ $X$ is assumed to have rank one.
The ideas presented here can easily be generalized to higher-rank tenors.
We also assume w.l.o.g.\ that $X$ is normalized in Frobenius norm, i.e.\ $\norm{X}_2 = 1$.
Then, $X$ can be written as a tensor product of $N$ normalized vectors $x_i \in \Reals^d$ and we have
\[
  b\ind{l} = \braket{A\ind{l}, X} = \prod_{i=1}^N \braket{a\ind{l}_i, x_i}.
  \label{eq:ana.b_as_product}
\]

The main theorem of this section provides necessary conditions on a general product measurement~\eqref{eq:als.rank1_measurements} for successfully recovering the rank-1 tensor $X$.
\begin{theorem}%
  \label{thm:ana.conditions}
  Let $X \in \Reals^{d^N}$ be a normalized tensor of unit rank, $H$ be the number of epochs in the ALS \cref{alg:als}, and $M = (HN + 1)m$.
  Denote one batch of size $m$ of random measurement tensors and their overlaps with $X$ by $A\ind{l}$ and $b\ind{l}$, respectively.
  Finally, let $\AA = \sum_{l=1}^m \ket{l}\bra{A\ind{l}}$ denote the corresponding measurement matrix of that batch.
  Assume $\AA$ satisfies the following properties with parameter $\delta$
  \begin{enumerate}
    \item\label{lbl:conditions.init}\textbf{Initialization}:
      \todo{Which norm?}
      \[
        \Norm{X - \adj{\AA}\AA X} \le \delta.
      \]
    \item\label{lbl:conditions.B}\textbf{Concentration of the $B_j$ operators}:
      Let $v_i \in \Reals^d$ with $\norm{v_i}_2 = 1$ be independent of all the $a\ind{l}_i$.
      For $j \in [N]$, define
      \[
        B_j = \frac{1}{m} \sum_{l=1}^m \left( \prod_{i\neq j} \Braket{a\ind{l}_i, v_i} \right)^2 \Ketbra{a\ind{l}}.
      \]
      Then, with high probability the smallest eigenvalue of $B_j$ should satisfy
      \[
        \lambda_\mathrm{min} \ge 1 - \delta.
      \]
    \item\label{lbl:conditions.G}\textbf{Concentration of the $G_j$ operators}:
      Let $v_i, v_i^\perp \in \Reals^d$ with $\norm{v_i}_2 = \norm{v_i^\perp}= 1$ be independent of all the $a\ind{l}_i$ and satisfy $\braket{v_i, v_i^\perp} = 0$
      For $j \in [N]$, define
      \[
        G_j = \frac{1}{m} \sum_{l=1}^m \left( \prod_{i\neq j} \Braket{a\ind{l}_i, v_i} \Braket{a\ind{l}_i, v_i^\perp} \right) \Ketbra{a\ind{l}}.
      \]
      Then, with high probability the largest eigenvalue of $G_j$ should satisfy
      \[
        \lambda_\mathrm{max} \le \delta
      \]
  \end{enumerate}
  Then, the output $Y$ of the ALS \cref{alg:als} after $H$ epochs satisfies
  \[
    \norm{X - Y} \le \epsilon
  \]
  provided $H \ge ?$.
  \todo{Fill in condition on epochs}.
\end{theorem}

The idea of the proof of \cref{thm:ana.conditions} is summarized in the following:
The initialization condition~\ref{lbl:conditions.init} guarantees that the initializer
\[
  S = \adj{A}A X = \sum_{l=1}^m b\ind{l} A\ind{l}
\]
is close to the target $X$.
However, since the rank of $S$ is generically $m$, which is much larger than $1$, we need to compress it as indicated in line~\ref{line:als.initialization} of the algorithm.
Using \cref{eq:mps.quasi_optimal}, we can then bound the error of the compressed version of $S$.
Next, we need to show that every local update reduces the distance between the updated local tensor and its true value by at least a given multiplicative constant.
Finally, combining these two fact shows that the distance of the estimated tensor and the true tensor reduces exponentially fast in $H$.\\



We start the proof of \cref{thm:ana.conditions} by examining the update step for the leftmost local tensor with $n = 1$.
Denote by $\tilde y^{h+1}_1$ the minimizer $\hat Z$ in line~\ref{line:als.min} and by $y^h_i$ the remaining local tensors of $Y$, which are kept fixed during this micro-iteration.
The empirical error as a function of $\tilde y^{h+1}_n$ then reads
\[
  F\left(\tilde y^{h+1}_1\right)
  = \sum_l {\left(  \prod_i \braket{a\ind{l}_i, x_i} - \braket{a\ind{l}_1, \tilde y^{h+1}_1} \, \prod_{i \neq 1} \braket{a\ind{l}, y^{h}_i} \right)}^2
  \label{eq:ana.l2_error}
\]
Since in the following, we only consider the optimization over $\tilde y^{h+1}_1$ and keep the other $y^h_i$ fixed, we write $\tilde y_1$ and $y_i$ (for $i > 1$), respectively, if there is no risk of confusion.
In contrast to the $(y_i)_{i > 1}$, the $\tilde y_1$ is not normalized.
In \cref{lem:ana.gradient_of_error} on page~\pageref{lem:ana.gradient_of_error}, we show that the extremal condition $\frac{\partial F(\tilde y_1)}{\partial \tilde y_1} = 0$ has the following explicit solution
\[
  \label{eq:ana.explicit_minimizer}
  \ket{\tilde y_1} = {\left( \prod_{i \neq 1} \braket{x_i, y_i} \right)} \ket{x_1} - B_1^{-1} G_1 \ket{x_1}.
\]
Here, the operators $B_1$ and $G_1$ are given by
\begin{align}
  \label{eq:ana.b_operator}
  B_1 &= \frac{1}{m} \sum_l {\left( \prod_{i \neq 1} \Braket{a\ind{l}_i,y_i} \right)}^2 \Ketbra{a\ind{l}_1} \\
  \label{eq:ana.g_operator}
  G_1 &= \frac{1}{m} \sum_l {\left( \prod_{i \neq 1} \Braket{a\ind{l}_i, x_i^\perp} \Braket{a\ind{l}_i,y_i} \right)}\Ketbra{a\ind{l}_1} \\
  x_i^\perp &= \left(\ketbra{y_i} - \1 \right) x_i.
\end{align}
Note that these are exactly the operators introduced in \cref{thm:ana.conditions}.

From \cref{eq:ana.explicit_minimizer} we see that if the overlaps of the fixed local tensors $\braket{x_i, y_i}$ ($i=2,\ldots,N$) are all close to one and the error term $B_1^{-1} G_1 \ket{x_1}$ is small, then the new value $\tilde y_1$ is almost equal to its target $x_1$.



\subsection{Numerics}%
% Implementation details; how to speed up initialization, why suitable for GPU%

