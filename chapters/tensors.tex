 % -*- root: ../thesis.tex -*-
\chapter{Tensors}
\label{chap:tensors}

In the last chapter, we discussed a solution to the phase retrieval problem based on ideas from \emph{low-rank matrix recovery}.
The latter can be summarized as follows.
Consider a matrix\footnote{%
  Here, we choose $X$ to be real to simplify some notation -- an adaption of the statements made to the complex case is straight forward.
}
$X \in \Reals^{d_1 \times d_2}$ of rank $r$ that is $X$ has only $r$ non-zero singular values
The goal is to reconstruct $X$ from $m$ linear measurements of the form
\[
  y_l = {\braket{A\ind{l}, X}}, \quad l=1,\ldots,m
  \label{eq:tensors.lin_measurements}
\]
where $\braket{A, B} = \tr (\transpose{A} B)$ denotes the Frobenius inner product, $A\ind{l} \in \Reals^{d_1 \times d_2}$ the \emph{measurement matrices}, and $y_l \in \Reals$ the corresponding measurement data.
In other words, we are looking for a solution $X$ of the linear problem defined by \cref{eq:tensors.lin_measurements}.
In case the $A_i$ span $\Reals^{d_1 \times d_2}$, this can be solved simply by computing the (pseudo-)inverse similar to \cref{sub:ortho.linear_inversion}.
However, in the underdetermined case $m < d_1 d_2$, this constitutes an ill-posed linear problem.
Nevertheless, we can still single out a unique solution in some cases by exploiting the additional structure, namely the low-rank assumption.
\todo[noline]{Check log factors}
For example, in \cref{thm:FIXME} we show that $m = \Order(d)$ random measurements sampled from an appropriate distribution are sufficient to reconstruct any positive semidefinite, Hermitian $X \in \Complex^{d \times d}$ of rank one.
More generally, we can recover any $X \in \Reals^{d_1 \times d_2}$ with $\rank X = r$ from $m = \Order(d_1 + d_n) r$ randomly chosen measurements~\cite{FIXME}.
Note that this is asymptotically optimal since we need at least $(d_1 + d_2) r$ real parameters to specify the left- and right-singular vectors of $X$.
For more details see~\cite{Li_Paper}
But \cref{thm:FIXME} not only guarantees identifiability of $X$, it also provides a positive semidefinite program to compute the reconstruction.
The existence of such an efficient algorithm is crucial since the straight-forward reconstruction via rank minimization
\[
  \estim{X} = \argmin_{X'} \rank X' \quad\mathrm{s.t.}\ \braket{A\ind{i}, X'} = y_i (i = 1, \ldots, m)
\]
is $\NP$-hard~\cite{FIXME}.

A closely related problem is studied under the name of \emph{compressed sensing}:
The goal is to recover an $s$-sparse vector $X \in \Reals^d$ from linear measurements as in \cref{eq:tensors.lin_measurements} with $\braket{\cdot, \cdot}$ denoting the Euclidean scalar product on $\Reals^d$.
Here, a vector is $s$-sparse if it has $s$ non-vanishing components with respect to a certain basis.
Therefore, the property of having low-rank may be regarded as a non-commutative analogue of sparsity since a matrix has low-rank if and only if it is sparse in its eigenbasis.
\todo[noline]{Check log factors}
Similar to the matrix case, $X$ can be recovered from $m = \Order(s \log d)$ randomly chosen measurement vectors $A\ind{l} \in \Reals^d$ using a linear program~\cite{Rauhut}.
Hence, the number of measurements required scales with the \emph{intrinsic complexity} of the sparse signal vector:
If we encode each component of $X$ using $N$ bits, we can encode the full vector using
\[
  N \times s + s \log d = \Order(s \log d)
\]
bits.
The second summand on the left hand side is necessary to encode the positions of the non-zero components.

In conclusion, both compressed sensing and low-rank matrix recovery provide techniques to efficiently reconstruct a low-complexity, compressible signal embedded in a higher-dimensional space from linear measurements.
Here, \quotes{efficiently} refers to both, low computational and sample complexity.
The latter refers to the fact that the number of measurements required scales with the intrinsic complexity of the signal and not the dimension of the ambient space as necessary for standard linear inversion.\\


% in this chapter: natural generalization compressed sensing -> lr recovery -> ? LOW LANK TENSORS!
% consider tensor in R or C d_1 \times ... \times d_N. But usually d_1 = ... = d_N
% different notions of rank:
%   - canonical rank -> captures natural structure of many problems, e.g. recommender systems; but problematic, unstable, hard computational problems
%   - here: MPS/TT rank: numerical advantageous, captures structure well of many problems in physics; especially well suited for linear local structure; recently quite popular
% connected to the MPS/TT tensor decomposition; first use: Hidden Markov model -> factorization of probabliilty distributions. Quantum Physics -> Werner (finetly correlated states); tensor train -> machine learning
% low-rank tensor = low TT rank tensor

% introduce exact notion later, but important for us: fixed rank r and local dimension d -> number of parameters scales as r^2 d N
% compare to ambient dimnsion d^N
% Question: can we recover tensor from a number of measurements scaling polynomially in all parameters with efficient algorithm?
% big problem: standard model (fully Gaussian measurements) also not efficient; partially solved in Zeljka
% in this chapter, partial result trying to answer this question for efficeint product measurements (work in progress!):
%   - introduction   to MPS
%   - recall idea for low-
%   - generalize alt min algorithm and proof idea from ... for low-rank matrices to the low-rank tensor case in ...
%   - products of Gaussians arise -> connection to products of Gaussian papers -> expansion
%   - numerical evidence



\subsection*{Relevant publications}
\begin{itemize}
  \item Ž.\ Stojanac, D.\ Suess, M.\ Kliesch: \textit{On the distribution of a product of N Gaussian random variables}, Proceedings Volume 10394, Wavelets and Sparsity XVII; 1039419 (2017)
  \item Ž.\ Stojanac, D.\ Suess, M.\ Kliesch, \textit{On products of Gaussian random variables}, arXiv:1711.10516
  \item D.\ Suess, M.\ Holzaepfel, \textit{mpnum: A matrix product representation library for Python}, ???
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Matrix Product States}
\label{sec:tensors.mps}

% graphical notation

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ALS for Tensors}

\subsection{Theory}
\subsection{Numerics}
% Implementation details; how to speed up initialization, why suitable for GPU

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Python Library mpnum}

