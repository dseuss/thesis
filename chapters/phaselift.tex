 % -*- root: ../thesis.tex -*-
\chapter{Characterizing linear-optical networks via PhaseLift}%
\label{chap:phaselift}

\todo{Metnion that proof strategy unified}

\todo{This whole intro needs work}

Linear-optical networks composed of beamsplitting and phaseshifting operations are fundamental to the quantum and classical processing of information with light.
Passive and reconfigurable linear optical circuits have been proposed and demonstrated for many applications including telecommunications~\cite{Miller_2015_Sorting}, machine learning~\cite{Shen_2017_Deep} and quantum computation~\cite{Carolan_2015_Universal} and simulation~\cite{Harris_2017_Quantum}.
With the continuing development of large-scale integrated photonic platforms~\cite{Silverstone_2016_Silicon,Seok_2016_LargeScale}, practical and reliable techniques for characterizing and validating the operation of these devices are crucial.
In this work, we present new protocols for characterizing linear optical devices with low experimental resources by exploiting a connection to the phase retrieval problem \cite{Walther_1963_Question}.

Well-known techniques for characterizing linear optical circuits include quantum process tomography with non-classical~\cite{Brien_2004_Quantum} or coherent~\cite{Keshari_2011_Quantum} states, though these approaches scale exponentially with the number of modes.
Simpler protocols tailored to linear optics have been proposed that use either single and two-photon probe states~\cite{Laing_2012_SuperStable,Dhand_2016_Accurate,Spagnolo_2017_Learning} or multimode coherent states~\cite{Keshari_2013_Direct,Tillmann_2016_On}.
The most similar scheme to the one presented here is~\cite{Keshari_2013_Direct}, where coherent light is input into single modes and split over pairs of modes with the intensity at each output measured.
While their recovery method is strikingly simple and relies only on $2n-1$ input configurations, for each configuration it requires varying over a phase shift between the two modes until maximal constructive interference is observed.
Hence, the experiment needs to be performed interactively, where the phase shift is adjusted gradually throughout an individual measurement, or a large number of phase shifter settings need to be probed.
Both alternatives require a large number of measurements to be performed in order to recover $M$ successfully.
Furthermore, by construction, the protocol from~\cite{Keshari_2013_Direct} utilizes the obtained reconstruction of the first row to recover the remaining rows of $M$.
This makes it a priori susceptible towards noise as any error in the determination of the first row propagates to the remaining rows.

In this chapter, we propose an efficient, robust, and conceptually simple technique for characterizing linear optical circuit based on recent advances in low-rank matrix recovery.
Not only is the \emph{PhaseLift} reconstruction algorithm robust to noise and efficient with respect to the number of measurements, it also comes with stringent recovery guarantees.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Device Characterization}%
\label{sec:pl.optics}


Mathematically, a linear optical device is fully characterized by its \emph{transfer matrix}, which relates the output to the input of the device by
\[
  \label{eq:pl.transfer_matrix_operators}
  \adj{a}_j \rightarrow \adj{b}_j = \sum_i M_{i,j} \adj{a}_i.
\]
Here, $\adj{a}_j$ and $\adj{b}_j$ denote the creation operators of the $j$-th input and output mode, respectively.
Determining ${M}$ experimentally is the crucial step to validate and verify a linear optical circuit.
For this purpose, we propose a protocol that can be implemented easily in an experiment using either classical laser light or single photon sources.
We start by introducing the former since its conceptually simpler.

For now, we assume that the input is described by a classical multi-mode coherent state $\ket{{\alpha}} = \ket{\alpha_1, \ldots, \alpha_n}$ with
\[
  \ket{\alpha} = \mathrm{e}^{-\frac{\ltwonorm{\alpha}}{2}} \, \sum_{k_1,\ldots,k_n}  \frac{\alpha_1^{k_1} \ldots \alpha_n^{k_n}}{\sqrt{k_1! \ldots k_n!}} \, \ket{k_1} \ldots \ket{k_n}.
\]
Then, due to \cref{eq:pl.transfer_matrix_operators}, the output is a coherent state $\ket{\beta}$ as well and its components are given by
\[
  \beta_j = \sum_k M_{j,k} \,\alpha_k.
  \label{eq:pl.coherent_transfer_matrix}
\]
Note that for an ideal, unitary transfer matrix, $\ltwonorm{\alpha} = \ltwonorm{\beta}$ as the squared norm of a coherent state vector describes its total intensity.
The standard measurable quantities in an optical experiment with coherent states are the \emph{intensities} of the output modes
\[
  I_j({\alpha})
  = \left| \beta_j \right|^2 + \epsilon_j
  = \left| \sum_k M_{j,k} \, \alpha_k \right|^2 + \epsilon_j
  \label{eq:pl.intensities}
\]
for certain coherent inputs $\ket{{\alpha}}$.
Here, $\epsilon_j$ describes noise due to statistical fluctuations or systematic errors.
A schematic of such an experiment is depicted in \cref{fig:pl.experimental.schematic} a).
Although the output coherent states~\eqref{eq:pl.coherent_transfer_matrix} are linear in ${M}$, the resulting intensity measurements~\eqref{eq:pl.intensities} are quadratic in ${M}$ and oblivious to the phases of $\beta$.
Therefore, the problem of reconstructing ${M}$ from such measurements is ill-posed and requires deliberate utilization of interference between the modes to recover the phases of $M$.

In this chapter, we propose an approach for recovering $M$ using coherent states $\ket{\alpha}$ as inputs with $\alpha$ sampled randomly from appropriate distributions.
Preparing theses states reliably is the major challenge of implementing the proposed protocol experimentally.
A first experimental demonstration is performed using the universal linear optics device from~\cite{Carolan_2015_Universal}:
The silica-on-silicon device performs a linear-optical circuit comprising 30 directional couplers and 30 tunable thermo-optic phase-shifters on six optical waveguides.
Using the setup outlined in \cref{fig:pl.experimental.schematic}, we are able to prepare any coherent input state from a single laser input in the bottom mode using the left-most cascade of couplers and phase-shifters.
The remaining triangular array of components colored blue in \cref{fig:pl.experimental.schematic} is then sufficient to implement any five mode unitary transfer matrix $M$~\cite{Reck_1994_Experimental}.
Reconfiguring the target $M$ then enables us to experimentally test the protocol across a number of configurations including Identity, Swap, and Fourier matrices as well as Haar random unitaries.

Note that for the proposed preparation scheme, it is beneficial to consider ensembles of coherent states with fixed norm, and hence, with fixed total intensity:
The preparation procedure for an arbitrary coherent state $\tilde\alpha$ is described by a transfer matrix $P(\alpha)$, where $\alpha = \frac{\tilde \alpha}{\norm{\tilde \alpha}}$ as follows
\[
  \label{eq:pl.phaselift_P}
  \tilde\alpha = P(\alpha) (\norm{\tilde\alpha} e_1).
\]
Here, $e_1$ is the first canonical basis vector, and hence, the coherent state $\ket{\norm{\tilde\alpha}}$ describes a single laser input with intensity $\norm{\tilde\alpha}^2$ in the bottom most mode.
Due to linearity, $P$ can only depend on the normalized $\alpha$.
The red colored cascade of couples and phase-shifters in \cref{fig:pl.experimental.schematic} implements exactly the preparation matrix $P(\alpha)$.
Therefore, varying the norm of the prepared input vector $\tilde\alpha$ requires to change the intensity of the input laser.
Alternatively, we could redirect parts of the light in the topmost, unobserved mode, which is challenging when the required $\norm{\alpha}$ varies a lot.\\



Although performing recovery of $M$ using only classical sources of light and photodiodes simplifies the experiment, it also has a large drawback in practice:
Our main motivation for studying linear optical devices is their application in quantum computing, which requires the use of single-photon sources.
However, readily available laser and single photon sources often have slightly different characteristics such as wavelength or polarization.
Since the properties of the components, and therefore, also the transfer matrix are generally dependent these characteristics, a characterization using coherent light is generally unsuitable for predicting the performance of the device when used with single photon sources.
Instead, the device should be evaluated under the same experimental conditions where it will be used.
For this purpose, we now turn to an experimental implementation of the idea introduced above based on single-photon sources and detectors.

The idea is to estimate the outcomes of the intensity measurements~\eqref{eq:pl.intensities} using single photon states:
If we set the preparation stage to $P(\alpha)$, but feed a single photon Fock state in the bottom waveguide, the prepared state prior to $M$ is
\[
  \label{eq:pl.phaselift.single_photon}
  \ket{\psi({\alpha})} = \sum_j \alpha_j \adj{a_j} \ket{\mathfrak{0}},
\]
where $\ket{\mathfrak{0}}$ denotes the vacuum state.
For $\ket{{\psi(\alpha)}}$ to be well-normalized, we need to choose $\ltwonorm{\alpha} = 1$.
The probability of measuring the photon at detector $j$ is then given by
\[
  p_j = \Prob(j|{\alpha}) = \left| \sum_k M_{j,k} \alpha_k \right|^2.
  \label{eq:pl.experiment.probabilities}
\]
Hence, finite-sample frequency estimates of the probabilities~\eqref{eq:pl.experiment.probabilities} are equivalent to the noisy intensity measurements~\eqref{eq:pl.intensities}.
Note that $\alpha$ is now simply a parameter vector for the achievable single-photon Fock states~\eqref{eq:pl.phaselift.single_photon}.

To estimate the probabilities~\eqref{eq:pl.experiment.probabilities}, we subsequentially feed $N$ single photon Fock states into the device such that they do not interfer with each other.
Then, the photon counting statistics is governed by a multinomial distribution, i.e.
\[
\Prob(N_1, \ldots, N_n | \alpha) = \frac{N!}{N_1! \ldots N_n!}  p_1^{N_1} \times \cdots \times p_n^{N_n}  \, \delta_{N_1 + \cdots + N_n, N}.
  \label{eq:pl.photon_stats}
\]
Here the right hand side is the probability of simultaneously measuring $N_j$ photons in the $j$-th mode for $j=1,\ldots,n$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[tbp]
  \centering
  \includegraphics[width=0.95\columnwidth]{fig/phaselift_schematic}%
  \caption{%
    \label{fig:pl.experimental.schematic}%
     Schematic of PhaseLift characterization protocol and experiment.
     a) Protocol summary using coherent states:
     A calibrated and trusted optical network is used to prepare multimode coherent states $\singleket{\alpha}$, sampled from an appropriate ensemble.
     These states are then fed into the unknown linear optical device described by the transfer matrix $M$, and the intensities at each output mode are measured.
     b) Experimental implementation using single photon sources:
     Heralded single photons are injected into the bottom waveguide of a six-mode integrated photonic device.
     A cascade of Mach-Zehnder interferometers is used to prepare single-photon states $\singleket{\psi( \alpha)}$ over the bottom five modes of the device.
     The remainder of the device is used to implement arbitrary 2, 3 and 5 dimensional unitary transformations which are to be characterized.
     Each output port is coupled to a single photon detector.
   }
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Phase Retrieval}%
\label{sec:pl.phase_retrieval}

The crucial observation of this work is that measurements~\eqref{eq:pl.intensities} closely resemble the model of the \textit{phase retrieval problem}, i.e.\ the problem of recovering a complex vector ${x} \in \Complex^n$ from $m$ intensity measurements of the form
\[
  y\ind{l}= \Abs{\braket{\alpha\ind{l}, x}}^2 + \epsilon\ind{l}
  \quad l=1,\ldots,m.
  \label{eq:pl.phase_retrieval_measurements}
\]
Here, $\alpha^{(l)} \in \Complex^n$ denote measurement vectors and $\epsilon^{(l)}$ the additive measurement errors.
The major difficulty in recovering $x$ from these intensity measurements is the loss of phase information.
In order to infer the phase information, we need to exploit interference effects by carefully selecting different measurement vectors.
Note, $x$ can only be recovered up to a global phase since $x$ and $\ee^{\ii\phi} x$ are indistinguishable from the measurements~\eqref{eq:pl.phase_retrieval_measurements} for any phase angle $\phi$.

One practical solution to the phase retrieval problem is based on its connection to the field of low-rank matrix recovery.
The quadratic measurements of $x$ in \cref{eq:pl.phase_retrieval_measurements} can be rewritten as
\[
  \left| \langle {x}, \alpha^{(l)} \rangle \right|^2
  = \tr \left( (\ket{\alpha^{(l)}}\bra{\alpha^{(l)}}) (\ket{{x}}\bra{{x}}) \right).
\]
This \quotes{lifts} the phase retrieval problem to the problem of recovering the positive semi-definite (psd) rank-1 matrix $\ketbra{x}$ from linear measurements.
Note that an efficient solution to this problem needs to exploit the low-rank constraint, as we have embedded the low-complexity signal $\ketbra{x}$ into a $n^2$ dimensional ambient space.
This problem -- and its generalization to arbitrary low-rank matrices -- have been studied extensively in the field of \emph{low-rank matrix recovery}, see e.g.~\cite{Ahmed_2014_Blind,Candes_2009_Exact,Candes_2011_Tight,Recht_2010_Guaranteed,Gross_2011_Recovering,Chen_2015_IncoherenceOptimal} for a highly incomplete list of references.

The fundamental idea is to find the matrix $Z$ with the smallest rank that is compatible with the observations.
As an example, consider the idealized noiseless case of \cref{eq:pl.phase_retrieval_measurements}, i.e.\ $\epsilon\ind{l} = 0$.
Then, we can reconstruct $\ketbra{x}$ using the following rank-minimization problem
\[
  \begin{split}
    \underset{{Z}}{\textrm{minimize}} &\quad \rank Z \\
    \textrm{subject to} &\quad  \tr\left( \ketbra{\alpha\ind{l}} \, Z \right) = y_l \quad (l=1,\ldots,m)
  \end{split}
  \label{eq:pl.rank_minimization}
\]
provided the $\alpha\ind{l}$ suffice to single out $\ketbra{x}$.
However, rank minimization is $\NP$-hard in general~\cite{Boyd_2004_Convex}, and therefore, \cref{eq:pl.rank_minimization} cannot be solved efficiently.
Nevertheless, there are algorithms for recovering $\ketbra{x}$ that are computationally efficient with only a slight overhead in the sample complexity.
Here, we consider the following convex algorithm termed \emph{PhaseLift}~\cite{Candes_2013_Phaselift}
\[
  \label{eq:pl.PhaseLift}
  \begin{split}
    \underset{{Z}}{\textrm{minimize}} & \quad \sum_{l=1}^m \left| \tr \left( \ket{\alpha^{(l)}} \bra{\alpha^{(l)}} \, {Z} \right) - y^{(l)} \right| \\
    \textrm{subject to} &\quad  {Z} \geq 0.
  \end{split}
\]
From the minimizer $Z^\sharp$ of \cref{eq:pl.PhaseLift}, we obtain the recovered signal vector ${ x}^\sharp$ as follows:
Consider the eigenvalue decomposition of $Z^\sharp$
\[
  Z^\sharp = \sum_i \lambda_i \ketbra{z_i}
\]
with $\ltwonorm{z_i} = 1$ and $\lambda_1 \ge \lambda_2 \ge \ldots \lambda_n$.
Then, we set
\[
  x^\sharp = \sqrt{\lambda_1} z_1.
  \label{eq:pl.vector_from_matrix}
\]

Several analytic proofs of convergence have been established for phase retrieval via PhaseLift.
With few notable exceptions~\cite{Kech_2016_Explicit}, these are probabilistic in nature and assume that each measurement vector is chosen from an appropriate distribution.
Probabilistic in this context means that we allow for a small probability w.r.t.\ the random sampled measurement vectors of failing to reconstruct $x$.
Paradigmatic examples are the Gaussian and the uniform (spherical) measurement ensemble~\cite{Candes_2013_Phaselift}:
In case of the Gaussian ensemble, the components $\alpha\ind{l}_i$ of the measurement vectors are i.i.d.\ standard complex Gaussian random variables.
In the uniform scheme, the $\alpha^{(l)}$ are chosen uniformly from the complex unit sphere.
The two are closely related, as the latter arises by normalizing all Gaussian vectors to a fixed length.
However, these two measurement ensembles are often too demanding for practical applications, which led to a large body of work proving similar recovery guarantees  for measurement ensembles that feature less randomness~\cite{Gross_2014_Partial,Kueng_2014_Low,Kueng_2014_Low,Kueng_2016_Low} or additional structure tailored to specific applications~\cite{Candes_2013_Phaselift,Gross_2017_Improved,Voroninski_2013_Quantum,Kueng_2015_Low}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Theory for Characterization via Phase Lift}%
\label{sec:pl.theory}

The main theoretical contribution of this chapter is a recovery guarantee for a measurement ensemble motivated by the experimental architecture of linear optical devices:
The \emph{randomly erased complex Rademacher} (RECR) ensemble defined below is easier to implement experimentally by the circuit depicted in \cref{fig:pl.experimental.schematic} than e.g.\ the uniform ensemble.
In \cref{sub:pl.recr}, we introduce this sampling scheme and show that it can be used for phase retrieval.
\Cref{sub:pl.characterization} is then concerned with applying the ideas from phase retrieval to the problem of recovering transfer matrices of linear optical circuits.
We also discuss the advantages of the RECR ensemble for our particular application in this section.


\subsection{The RECR ensemble}%
\label{sub:pl.recr}

The uniform ensemble introduced in \cref{sec:pl.phase_retrieval} is well-suited for theoretical analysis.
However, this sampling scheme places high demands on practical implementations as it necessitates the ability to prepare any input state $\ket{\alpha}$ with $\alpha$ from the complex unit sphere.
Therefore, we propose an alternative measurement ensemble that lends itself better to implementations in linear optics:
For $p \in [0,1]$, we define a \emph{randomly erased complex Rademacher} (RECR) random variable $a$ to be distributed according to
\[
  a =
  \begin{cases}
    +1 & \textrm{with prob. } p/4 \\
    +\ii & \textrm{with prob. } p/4 \\
    0 & \textrm{with prob. } 1-p \\
    -\ii & \textrm{with prob. } p/4 \\
    -1 & \textrm{with prob. } p/4
  \end{cases}.
  \label{eq:pl.recr_definition}
\]
Here, the constant $1 - p$ is referred to as the \emph{erasure probability}.
For the (unnormalized) RECR measurement model, we sample the components $\alpha\ind{l}_k$ of the input vectors $\alpha\ind{l}$ according to \cref{eq:pl.recr_definition}.
We also consider the normalized version below.
Since there are only four different values for the phases of the components~\eqref{eq:pl.recr_definition}, the RECR scheme is easier to implement experimentally as we discuss in \cref{sub:pl.characterization}.

Note that a non-zero erasure probability is crucial for phase retrieval as the following example shows:
Denote by $e_j$ the canonical basis vectors.
For $p=1$, the complex Rademacher measurement vectors cannot distinguish between the signals $x = e_1$ and $x = e_2$ from measurements~\eqref{eq:pl.phase_retrieval_measurements} even in the idealized noiseless case $\epsilon\ind{l} = 0$.\\



The rest of this section is devoted to proving recovery guarantees for the RECR ensemble for phase retrieval via PhaseLift~\eqref{eq:pl.PhaseLift}.
In order to be more self-contained, we also provide recovery guarantees for the Gaussian and uniform ensembles, which are well known~\cite{Candes_2012_Solving,Demanet_2014_Stable}.
For this purpose, we develop a unified proof strategy inspired by Ref.~\cite{Dirksen_2017_On}, who derived strong result for sparse vector recovery using similar assumptions, and Ref.~\cite{Kabanava_2015_Stable} in the non-commutative setting.
\todo{Mention Richards contribution}
In the following we consider ensembles of measurement vectors $\alpha$ that satisfy the following moment conditions.

\begin{definition}%
  \label{def:pl.attentive}
  A random vector $\alpha \in \Complex^n$ is said to be \emph{attentive} if there are positive constants $C_\mathrm{I}$, $C_\mathrm{SI}$, and $C_\mathrm{SG}$ such that the following conditions are satisfied.
  \begin{itemize}
    \item \emph{Isotropy on $\mathbb{C}^n$:} for every $z \in \Complex^n$
    \[
      \mathbb{E} \left[ | \langle \alpha,  z \rangle |^2 \right] = C_\mathrm{I} \|  z \|_{\ell_2}^2
      \label{eq:pl.tight_frame}
    \]

    \item \emph{Sub-Isotropy on $\Hermitian^n$:} denote by $\Hermitian^n$ the set of all Hermitian $n \times n$ matrices.
      Then, we the following condition should hold for every $Z \in \Hermitian^n$
    \begin{align}
      \mathbb{E} \left[ \langle \alpha,  Z \alpha \rangle^2 \right] \geq C_\mathrm{SI} \|  Z \|_2^2 \label{eq:pl.sub_isotropy}
    \end{align}

    \item \emph{Sub-Gaussian tail behavior:} For every normalized $ z \in \mathbb{C}^n$ ($\|  z \|_{\ell_2}=1$), $| \langle \alpha,  z \rangle|$ is sub-Gaussian in the sense that its moments obey
    \[
      \mathbb{E} \left[ | \langle \alpha,  z \rangle|^{2N} \right] \leq C_\mathrm{SG} N! \quad N \in \mathbb{N}.
    \label{eq:pl.subexponential}
    \]
  \end{itemize}
\end{definition}

The following proposition shows that these conditions are satisfied by the unnormalized Gaussian and RECR ensembles as well as the normalized uniform ensemble.
However, we do not have a proof that the practically relevant normalized RECR ensemble is attentive.
Therefore, we are going to treat it separately below.

\begin{proposition}%
  \label{prop:gauss+recr_requirements}
  The following measurement ensembles are attentive according to \cref{def:pl.attentive}:
  \begin{enumerate}
    \item\label{item:gauss+recr_requirements.gaussian} \emph{Gaussian sampling scheme:} $\alpha \in \mathbb{C}^n$ chosen from the standard complex normal distribution $\mathcal{N}(0,\tfrac{1}{2}\1_n)+ i \mathcal{N}(0,\tfrac{1}{2}\1_n)$.
    In this case
    \[
      C_\mathrm{I} = C_\mathrm{SI} = C_\mathrm{SG} = 1.
    \]

    \item\label{item:gauss+recr_requirements.uniform} \emph{Uniform sampling scheme:} $\alpha \in \mathbb{C}^n$ chosen uniformly from the complex sphere with radius $\sqrt{n}$.
    In this case
    \[
      C_\mathrm{I} = 1, \; C_\mathrm{SI} = \frac{n}{n+1}, \; C_\mathrm{SG} = \prod_{k=1}^{N-1} \frac{n}{n+k} \leq 1.
    \]

    \item\label{item:gauss+recr_requirements.recr} \emph{(unnormalized) Randomly Erased Complex Rademacher (RECR) sampling scheme:} the components of $\alpha \in \mathbb{C}^n$ are chosen independently from the distribution~\eqref{eq:pl.recr_definition}.
    The constants depend only on the erasure probability $1-p \in [0,1]$:
    \[
      C_\mathrm{I} = p,\; C_\mathrm{SI} = p \min \left\{p,1-p \right\}, \; C_\mathrm{SG} = \mathrm{e}^{\frac{3}{2}}.
    \]
  \end{enumerate}
\end{proposition}

\begin{proof}
  For case~\ref{item:gauss+recr_requirements.gaussian}, consider $\alpha \in \mathbb{C}^n$ be a standard (complex) Gaussian vector and fix any $ z \in \mathbb{C}^n$.
  Then, the random variable $\langle \alpha, z \rangle$ is an instance of a standard (complex normal) random variable $a = \tfrac{\|  z \|_{\ell_2}}{\sqrt{2}} \left(a_R + i a_I\right)$ with $a_R, a_I \sim \mathcal{N}(0,1)$.
  In turn, $|a|^2 = \frac{\|  z \|_{\ell_2}^2}{2} (a_R^2 + a_I^2)$ is a rescaled version of a $\chi^2$-distributed random variable with two degrees of freedom.
  The moments of such a random variable are well-known and we obtain
  \[
    \mathbb{E} (| \langle \alpha, z \rangle|^{2N})= \left( \frac{ \|  z \|_{\ell_2}}{\sqrt{2}}\right)^N \times 2^N N! = \|  z \|_{\ell_2}^N N! \; .\label{eq:pl.moments_gauss}
  \]
  From this, we can readily infer $C_\mathrm{SG} = 1$, and the special case $N=1$  yields $C_\mathrm{I}=1$.

  For the remaining expression, use an eigenvalue decomposition $ Z = \sum_{k=1}^d \zeta_k | z^{(k)} \rangle \langle  z^{(k)}|$ (with normalized eigenvectors $ z^{(k)}\in \mathbb{C}^n$) and note that the random variables $|\langle  a, z^{(1)} \rangle|,\ldots, | \langle  a, z^{(n)} \rangle|$ are independently distributed and obey \cref{eq:pl.moments_gauss}.
  Consequently:
  \begin{align}
    \mathbb{E} \left[ \tr \left(  A  Z \right)^2 \right]
    =& \mathbb{E} \left[ \left( \sum_{k=1}^d \zeta_k | \langle \alpha, z^{(k)} \rangle|^2 \right)^2 \right] \\
    =& \sum_{k \neq l} \zeta_k \zeta_l \mathbb{E} \left[ |\langle \alpha, z^{(k)} \rangle|^2 \right] \mathbb{E} \left[ | \langle  a, z^{(l)} \rangle|^2 \right]
    + \sum_{k=1}^d \zeta_k^2 \mathbb{E} \left[ | \langle  a,  z^{(k)} \rangle|^4 \right] \\
    =& \sum_{k \neq l} \zeta_k \zeta_l \| z^{(k)} \|_{\ell_2}^2 \|  z^{(l)} \|_{\ell_2}^2 + 2 \sum_{k=1}^d \zeta_k^2 \|  z^{(k)} \|_{\ell_2}^4
    = \sum_{k,l=1}^d \zeta_k \zeta_l + 2\sum_{k=1}^d \zeta_k^2 \\
    =& \tr ( Z)^2 + \tr ( Z^2)
    \geq \|  Z \|_2^2,
  \end{align}
  which implies $C_\mathrm{SI} = 1$.\\



  Now consider the case~\ref{item:gauss+recr_requirements.uniform}, where $\alpha$ is chosen uniformly from the complex sphere with radius $\sqrt{n}$.
  This in turn implies that the distribution of $\alpha \in \mathbb{C}^n$ is invariant under arbitrary unitary transformations.
  Techniques from representation theory -- more precisely: Schur's Lemma -- then imply
  \[
    \label{eq:pl.from_schur}
    \mathbb{E} \left[ (|\alpha \rangle \! \langle \alpha| )^{\otimes N} \right] =
    %\mathbb{E} \left[ U^{\otimes N} (|a_0 \rangle \! \langle a_0|)^{\otimes N} (U^\dagger)^{\otimes N} \right] = \binom{n+N-1}{n}^{-1} \| a_0\|_{\ell_2}^N P_{\vee^N} =
    n^N \binom{n+N-1}{N}^{-1}  P_{\vee^N},
  \]
  see e.g.\ \cite[Lemma~1]{Scott_2006_Tight}.
  Here, $ P_{\vee^N}$, denotes the projector onto the totally symmetric subspace $\bigvee\!^N \subset \left( \mathbb{C}^n \right)^{\otimes N}$.
  Note that $\left(| z \rangle \! \langle  z| \right)^{\otimes N} \in \bigvee\!^N$ and, moreover $2 \mathrm{tr} \left(  P_{\vee^2}  Z^2 \right)= \|  Z \|_2^2 + \mathrm{tr} ( Z)^2$ for any matrix $ Z$, see e.g.\ \cite[Lemma~17]{Kueng_2016_Low}.
  Consequently,
  \begin{align}
    \mathbb{E} \left[ | \langle\alpha, z \rangle|^2 \right]
    =& \mathrm{tr} \left( | z \rangle \! \langle  z| \, \mathbb{E} \left[ |\alpha \rangle \! \langle\alpha| \right] \right)
    = \mathrm{tr} \left( | z \rangle \! \langle  z| \mathbb{I} \right) = \|  z \|_{\ell_2}^2, \\
    \mathbb{E} \left[
    \langle\alpha| Z |\alpha \rangle^2 \right]
    =& \tr \left( \mathbb{E} \left[ (|\alpha \rangle \! \langle\alpha|)^{\otimes 2} \right]  Z^{\otimes 2} \right)
    = \frac{n}{n+1} \left( \|  Z \|_2^2 + \mathrm{tr}( Z)^2 \right) \geq \frac{n}{n+1} \|  Z \|_2^2, \\
    \mathbb{E} \left[ | \langle\alpha,  z \rangle |^{2N} \right]
    =& \mathrm{tr} \left(\mathbb{E} \left[ (|\alpha \rangle \! \langle\alpha|)^{\otimes N} \right]  (| z\rangle \! \langle  z|)^{\otimes N}  \right)
    = n^N \binom{n+N-1}{N}^{-1} \|  z \|_{\ell_2}^{2N} \\
    =& N! \frac{n^N (n-1)!}{(n+N-1)!} \leq N!,
  \end{align}
  which implies $C_\mathrm{I}=1$, $C_\mathrm{SI} = \frac{n}{n+1}$ and $C_\mathrm{SG}=1$.\\



  Finally, consider the case~\ref{item:gauss+recr_requirements.recr}, with $\alpha$ sampled from the unnormalized RECR ensemble.
  Let $\alpha_k = \langle  e_k, \alpha\rangle$, where $ e_1,\ldots, e_n$ is the orthonormal basis with respect to which the RECR vector is defined.
  Theses components obey $\mathbb{E}\left[ \alpha_k \right] = \mathbb{E} \left[ \cc{\alpha}_k \right] = 0$, as well as $\mathbb{E} \left[ |\alpha_k|^2 \right] = p$.
  For any $ z \in \mathbb{C}^n$ we then have
  \begin{align}
    \mathbb{E} \left[| \langle  \alpha,  z\rangle |^2 \right]
    =& \sum_{i,j=1}^n \mathbb{E} \left[ \cc{\alpha}_i \alpha_j \right] \langle  e_i |  z \rangle \langle  z |  e_j \rangle = p \sum_{i=1}^n | \langle  e_i,  z \rangle|^2 = p \|  z \|_{\ell_2}^2.
  \end{align}

  Now, Fix $ Z \in \Hermitian^n$ and compute
  \begin{align}
    \mathbb{E} \left[ \langle \alpha |  Z | \alpha \rangle^2 \right]
    =& \sum_{i,j,k,l} \mathbb{E} \left[ \bar{\alpha}_i \alpha_j \cc{\alpha^\prime_k} \alpha^\prime_l \right] \langle  e_i| Z|  e_j \rangle \langle  e_k | Z|  e_l \rangle \\
    =& \sum_{i} \mathbb{E} \left[ | \alpha_i |^4 \right] \langle  e_i| Z| e_i \rangle^2 + \sum_{i \neq k} \mathbb{E} \left[ | \alpha_i |^2 | \alpha_k|^2 \right] \left( \langle  e_i| Z| e_i \rangle \langle  e_k| Z| e_k \rangle + \langle  e_i| Z| e_k \rangle \langle  e_k|  Z| e_i \rangle \right) \\
    =& p \sum_{i=1}^n \langle  e_i| Z| e_i \rangle^2 + p^2 \sum_{i \neq k} \left( \langle  e_i| Z| e_i \rangle \langle  e_k| Z| e_k \rangle + \langle  e_i| Z| e_k \rangle \langle  e_k | Z|  e_i\rangle \right) \\
    =& p^2 \sum_{i,k=1}^n \left( \langle  e_i| Z| e_i \rangle \langle  e_k| Z| e_k \rangle + \langle  e_i| Z| e_k \rangle \langle  e_k | Z|  e_i\rangle \right) + p(1-2 p) \sum_{i=1}^n \langle  e_i| Z| e_i \rangle^2 \\
    =& p^2 \left( \tr ( Z)^2 + \| Z\|_2^2 \right) + p (1-2 p) \sum_{i=1}^n \langle  e_i|  Z |  e_i \rangle^2 \\
    \geq& p^2 \| Z\|_2^2 + p(1-p) \sum_{i=1}^n \langle  e_i | Z| e_i \rangle^2
  \end{align}
  To proceed, we consider the following two cases:
  \begin{itemize}
    \item[$p \leq 1/2$]: This implies $p(1-2p) \geq 0$ and consequently
      \begin{align}
        \mathbb{E} \left[ \langle \alpha | Z| \alpha \rangle^2 \right] \geq p^2 \|  Z \|_2^2.
      \end{align}
    \item[$p \geq 1/2$]: Use $\sum_{i=1}^n \langle i| X|i \rangle^2 \leq \| X \|_2^2$ to conclude
      \begin{align}
        \mathbb{E} \left[ \langle \alpha |  Z | \alpha \rangle^2 \right]
        \geq ( p^2 - p|1-2p|) \| Z\|_2^2 = p(1-p) \|  Z \|_2^2.
      \end{align}
  \end{itemize}
  This shows that the RECR example is sub-isotropic on $\Hermitian^n$.

  Finally, fix $ z \in \mathbb{C}^n$ with $\|  z \|_{\ell_2}=1$ and note that $|\alpha_k| \leq 1$ together with the independence of $\alpha_k,\alpha_l$ for $k \neq l$ implies
  \begin{align}
    \mathbb{E} \left[ \exp \left( | \langle \alpha,  z \rangle|^2 \right) \right]
    =& \mathbb{E} \left[ \prod_{k=1}^n \exp \left( | \alpha_k|^2 |z_k|^2 \right) \prod_{k \neq l} \exp \left( \cc{\alpha}_k \alpha_l \cc{z}_k z_l \right) \right] \nonumber \\
    \leq& \exp \left( \|  z \|_{\ell_2}^2 \right) \prod_{k \neq l} \mathbb{E} \left[  \exp \left( \cc{\alpha}_k \alpha_l \cc{z}_k z_l \right)  \right]. \label{eq:pl.moment_aux1}
  \end{align}
  Now note that for $k \neq l$, $\cc{\alpha}_k \alpha_l$ is again a RECR random variable $\tilde{\alpha}_{k,l}$, but with erasure probability $1-p^2$.
  Moreover, every RECR random variable $\alpha$ can be decomposed into the product of two independent random variables: $ \alpha= \eta \omega$, where $\eta$ is a Rademacher random variable and $\omega \in \left\{0, 1,i \right\}$ obeys $| \omega | \leq 1$.
  Consequently
  \begin{align}
    \mathbb{E} \left[ \exp \left( \bar{\alpha}_k \alpha_l \bar{z}_k z_l \right) \right]
    =& \mathbb{E} \left[ \exp \left( \tilde{\alpha}_{k,l} \bar{z}_k z_l \right) \right]
    = \mathbb{E}_{\omega} \left[ \mathbb{E}_\eta \left[ \eta \omega \bar{z}_k  z_l \right] \right]
    = \mathbb{E}_{\omega} \left[ \cosh \left( \omega \bar{z}_k z_l \right) \right] \\
    \leq & \mathbb{E}_\omega \left[ \exp \left( |\omega \bar{z}_k z_l|^2/2 \right) \right]
    \leq  \exp \left( \frac{|z_k|^2 |z_l|^2}{2} \right),
  \end{align}
  where we have used the standard estimate $\cosh (x) \leq \exp \left( |x|^2/2 \right)$ $\forall x \in \mathbb{C}$, as well as $| \omega| \leq 1$. Inserting this bound into \eqref{eq:pl.moment_aux1} yields
  \begin{align}
    \mathbb{E} \left[ \exp \left( | \langle  \alpha,  z \rangle|^2 \right) \right]
    \leq \exp \left( \|  z \|_2^2 \right) \prod_{k \neq l} \exp \left( \frac{|z_k|^2 |z_l|^2}{2} \right)
    \leq \exp \left( \|  z \|_2^2 + \frac{1}{2}\|  z \|_{\ell_2}^4 \right) = \mathrm{e}^{\frac{3}{2}},
  \end{align}
  because $\|  z \|_{\ell_2}=1$.
  Markov's inequality shows that this exponential bound implies a subexponential tail bound for the random variable $| \langle  \alpha, z \rangle|^2$:
  \begin{align}
    \Prob \left[ | \langle \alpha, z \rangle|^2 \geq t \right]
    =& \Prob \left[ \exp \left( | \langle  \alpha, z \rangle|^2 \right) \geq \exp \left( t \right) \right]
    \leq \frac{ \mathbb{E} \left[ \exp \left( | \langle \alpha,  z \rangle|^2 \right) \right]}{\exp (t)} \leq \mathrm{e}^{\frac{3}{2}-t}.
  \end{align}
  This in turn implies the following bound on the moments:
  \begin{align}
    \mathbb{E} \left[ | \langle  \alpha, z \rangle|^{2N} \right]
    =  N \int_0^\infty \Prob\left[ | \langle  \alpha, z \rangle|^2\geq t \right] t^{N-1} \mathrm{d}t \leq N \mathrm{e}^{\frac{3}{2}} \int_0^\infty \mathrm{e}^{-t} t^{N-1} \mathrm{d}t
    = \mathrm{e}^{\frac{3}{2}} N!,
  \end{align}
  where we have used a well-known integration formula for moments, see e.g.\ \cite[Prop.~7.1]{Foucart_2013_Mathematical}, as well as integration by parts.
\end{proof}

In this work, we consider a slight generalization of \cref{def:pl.attentive}, which we state now.
\begin{definition}%
  \label{def:pl.super_attentitive}
  We say that a random vector $\alpha \in \Complex^n$ is \emph{super-attentive}, if there is an attentive random vector $\tilde\alpha$ and a function $f \Complex^n \to \Reals$ with $f(\tilde\alpha) \ge 1$ almost surely such that $\alpha = f(\tilde\alpha) \tilde \alpha$.
\end{definition}
The main example of a super-attentive distribution in this work is a normalized RECR vector with length $\sqrt{n}$.
Denote by $\tilde\alpha$ an unnormalized RECR vector and set $f(\tilde\alpha) = \sqrt{n} / \ltwonorm{\tilde\alpha}$, then
\[
  \alpha = f(\tilde\alpha) \alpha
  \label{eq:pl.normalized_recr}
\]
is a normalized RECR vector.
Note that every super-attentive random vector is attentive trivially.

The conditions in \cref{def:pl.attentive} naturally appear in the proof of the following theorem, which is the fundamental technical part of this work.
It is used to provide rigorous recovery guarantees for phase retrieval via PhaseLift.

\begin{proposition}%
  \label{prop:pl.nsp}
  Suppose that $m = Cn$ vectors $\alpha\ind{1},\ldots,\alpha\ind{m} \in \mathbb{C}^n$ have been chosen independently at random from an attentive ensemble.
  Let $Z\geq 0$ and $x \in \Complex^n$.
  Then, the measurement operator
  \[
    \label{eq:pl.measurement_operator_rank1}
    \mathcal{A}(Z) = \sum_l \tr \left(\ketbra{\alpha\ind{l}}  Z\right) \, {e_l}
  \]
  satisfies
  \[
    \label{eq:pl.rec_guarantee_nsp}
    \Fnorm{Z - \ketbra{x}}
    \leq \frac{1}{m} \max \left\{ \tau, \frac{6}{\nu} \right\}  \Norm{\mathcal{A}\left( Z - \ketbra{x} \right)}_{\ell_1}.
  \]
  with probability at least $1- 3\mathrm{e}^{-\gamma m}$.
  Here, $C$ and $\gamma$ denote suitable positive constants.
\end{proposition}

Note that the measurement operator notation~\eqref{eq:pl.measurement_operator_rank1} is simply a shorthand for
\[
  y\ind{l} = \braket{\alpha\ind{l}, Z\alpha\ind{l}} \quad\quad (l=1,\ldots,m).
\]
We postpone the proof of this proposition to \cref{sub.nsp_proof} in order to state the main results of this section, namely the recovery guarantees for phase retrieval using the measurement ensembles from~\cref{prop:gauss+recr_requirements}.

The version presented here is a substantial generalization of existing results regarding Gaussian and uniform measurement ensembles~\cite{Candes_2012_Solving,Demanet_2014_Stable}.

\begin{theorem}%
  \label{thm:pl.phaselift_noisy}
  Suppose that $m = Cn$ vectors $\alpha\ind{1},\ldots,\alpha\ind{m} \in \mathbb{C}^n$ have been chosen independently at random from a super-attentive ensemble.
  Then, the optimizer $X^\sharp$ of the convex program~\eqref{eq:pl.PhaseLift} satisfies
  \[
    \Fnorm{{X}^\sharp - \ketbra{x}} \leq \frac{C'  \norm{\epsilon}_{\ell_1}}{m}.
    \label{eq:pl.noisy_recovery_bound}
  \]
  with probability at least $1 - 3\mathrm{e}^{-\gamma m}$ for some constant $\gamma > 0$.
  Here, $\fnorm{\cdot}$ denotes the Hilbert-Schmitt norm $\fnorm{Z}^2 = \tr \left( {Z} {Z}^\dagger \right)$, while $C,C'$ and $\gamma$ represent constants of sufficient size.
  Furthermore, $\norm{\epsilon}_{\ell_1}$ is a bound on the total noise of all measurements~\eqref{eq:pl.phase_retrieval_measurements}
  \[
    \norm{\epsilon}_{\ell_1} = \sum_{l=1}^m \abs{\epsilon\ind{l}}.
    \label{eq:pl.phaselift_noisy.noise_bound}
  \]
\end{theorem}

\begin{proof}[Proof of \cref{thm:pl.phaselift_noisy}]
  Denote by $\tilde\alpha\ind{l}$ the vectors corresponding to $\alpha\ind{l}$ and by $f$ the scaling function from \cref{def:pl.super_attentitive}.
  Then, the measurements outcomes $y\ind{l}$ can be mapped to measurement outcomes of $\tilde\alpha\ind{l}$ by
  \[
    \label{eq:pl.phaselift_noisy_recr.unnormalized_measurements}
    \tilde y\ind{l}
    := \frac{1}{f(\tilde\alpha)^{2}} \, y\ind{l}
    = \Abs{\braket{\tilde\alpha\ind{l}, x}}^2 + \tilde\epsilon\ind{l}.
  \]
  with the rescaled error vectors given by
  \[
    \tilde\epsilon\ind{l} = \frac{1}{f(\tilde\alpha)^{2}} \, \epsilon\ind{l}.
  \]
  Now, \cref{prop:pl.nsp} implies that a measurement operator $\tilde{\mathcal{A}}$ containing $m \geq C n$ measurements sampled from an attentive distribution satisfies \cref{eq:pl.rec_guarantee_nsp} with probability at least $1-3 \mathrm{e}^{-\gamma m}$.
  Conditioned on this event, we have for any $Z \ge 0$ and $x \in \Complex^n$
  \[
    \Fnorm{Z - \ketbra{x}}
    \le \frac{C'}{2m} \Norm{\tilde{\mathcal{A}}(Z) - \tilde y + \tilde \epsilon}_{\ell_1}
    \le \frac{C'}{2m} \left( \norm{\tilde \epsilon}_{\ell_1} + \norm{\tilde{\mathcal{A}}(Z) - \tilde y} \right),
    \label{eq:phaselift_noisy_recr.findal_bound}
  \]
  with $C' = 2 \max \left\{\tau, 6/\nu \right\}$.
  For the first summand, we have.
  \[
    \norm{\tilde\epsilon}_{\ell_1}  = \sum_l \Abs{  \frac{1}{{f(\tilde\alpha\ind{l})}^2} \, \epsilon\ind{l}} \le \norm{\epsilon}_{\ell_1}
  \]
  since $f(\tilde\alpha\ind{l} \ge 1$.
  For the second summand on the right hand side of \Cref{eq:phaselift_noisy_recr.findal_bound}, the same argument gives
  \begin{align}
    \norm{\tilde{\mathcal{A}}(Z) - \tilde y}_{\ell_1}
    &= \sum_l \Abs{  \braket{\tilde\alpha\ind{l}, Z \tilde\alpha\ind{l}} - \tilde y\ind{l}  } \\
    &= \sum_l \frac{1}{{f(\tilde\alpha\ind{l})}^2}  \Abs{ \braket{\alpha\ind{l}, Z \alpha\ind{l}} - y\ind{l}  } \\
    &\le \norm{\mathcal{A}(Z) - y}_{\ell_1}
  \end{align}
  PhaseLift -- the convex optimization problem~\eqref{eq:pl.PhaseLift} -- minimizes the right hand side of this bound over all $Z \geq 0$.
  Since $Z = \ketbra{x}$ is a feasible point of this optimization, we can conclude that the minimizer $Z^\sharp$ obeys
  \[
    \| \mathcal{A}( Z^\sharp) - {y} \|_{\ell_1} \leq \| \mathcal{A}(|{x} \rangle \! \langle {x}|)-{y} \|_{\ell_1} = \| {\epsilon} \|_{\ell_1}
  \]
  which concludes the proof.
\end{proof}

The constants $C$, $C'$, and $\gamma$ implicitly depend on the ensemble constants $C_\mathrm{I}$, $C_\mathrm{SI}$, and $C_\mathrm{SG}$ and can in principle be extracted from the proof.
Note that although we are recovering $\ketbra{x}$, which is embedded in the $n^2$ dimensional space of all $n \times n$ matrices, the demand on the number of measurements $m$ in \cref{thm:pl.phaselift_noisy} scales linearly in the original problem's dimension $n$.
This is optimal up to the constant multiplicative factor $C$.
Analytical bounds on this constant $C$ are usually too pessimistic to be practical and it is widely believed that
\(
  m = 4n - 4
\)
such measurements are actually sufficient, that is $C = 4 + o(n)$~\cite{Heinosaari_2013_Quantum}.\\


Recall from \cref{eq:pl.vector_from_matrix} that we obtain the recovery of the signal vector ${x}^\sharp$ from the minimizer ${X}^\sharp$ of the PhaseLift program~\cref{eq:pl.PhaseLift} via an eigenvalue decomposition.
In~\cite{Candes_2012_Solving} it was shown that \cref{eq:pl.noisy_recovery_bound} implies
\[
  \min_{0 \leq \phi \leq 2 \pi} \, \| {x}^\sharp - \mathrm{e}^{\ii \phi} {x} \|_{\ell_2}
  \leq C'' \frac{\| \epsilon \|_{\ell_1} }{m \| {x} \|_{\ell_2}}
, \label{eq:vectorial_noisy_bound}
\]
where $C''$ denotes another constant of sufficient size.
In words, we are able to recover the original signal $x$ up to a global phase and up to an error that is determined by the signal-to-noise ratio.


\subsection{Proof of \cref{prop:pl.nsp}}%
\label{sub.nsp_proof}

In this section we present a proof for the fundamental \cref{prop:pl.nsp}.
Our analysis is inspired by Ref.~\cite{Dirksen_2017_On} (who derived strong results for sparse vector recovery using similar assumptions) and Ref.~\cite{Kabanava_2015_Stable} in the non-commutative setting.
Moreover, Krahmer and Liu considered a real-valued version of the problem addressed here~\cite{Krahmer_2018_Phase}.

Our analysis is based on two fundamental results in random matrix theory.
First, the assumption of subgaussian tails~\eqref{eq:pl.subexponential} implies strong bounds on the operator norm of matrices of the form $\sum_{k=1}^m \ketbra{\alpha\ind{l}}$:

\begin{theorem}[Variant of Theorem 5.35 in~\cite{Vershynin_2010_Introduction}]%
  \label{thm:bernstein}
  Suppose that $\alpha\ind{1},\ldots,\alpha\ind{m}$ are independent copies of a subgaussian random vector obeying \cref{eq:pl.subexponential} with constant $C_\mathrm{SG}$.
  Let
  \[
    \tilde{ H} = \frac{1}{m} \sum_{k=1}^m \left( a_k \ketbra{\alpha\ind{l}} - \mathbb{E} \left[ a_k \ketbra{\alpha\ind{l}} \right] \right),
    \label{eq:pl.Htilde}
  \]
  where $a_k \in \mathbb{C}$ and $\abs{a_k} \leq 1$.
  Then,
  \begin{align}
    \Prob \left[ \opnorm{\tilde{ H}} \geq t \right]
    \leq
    \begin{cases}
      2 \exp \left( 2 \ln (3) n  - \frac{mt^2}{8 C_\mathrm{SG}} \right) & 0 \leq t \leq 2C_\mathrm{SG}, \\
      2 \exp \left( 2 \ln (3) n - \frac{m}{2} \left( t - C_\mathrm{SG} \right)  \right) & t \geq 2 C_\mathrm{SG},
    \end{cases}
  \end{align}
  where $\opnorm{\cdot}$ denotes the operator norm.
\end{theorem}

The second result is a generalization of \quotes{Gordon's escape through a mesh}-Theorem \cite{Gordon_1988_On} (a random subspace avoids a subset provided the subset is small in some sense).
The version we use here is due to Mendelson~\cite{Mendelson_2015_Learning,Koltchinskii_2015_Bounding}, see also see also \cite{Tropp_2014_Convex}.

\begin{theorem}[Mendelson's small ball method]%
  \label{thm:mendelson}
  Suppose that the measurement operator $\mathcal{A}:\Hermitian^n \to \mathbb{R}^m$ contains $m$ independent copies $A\ind{l}$ of a random matrix $ A \in \Hermitian^n$, that is
  \[
    \label{eq:pl.measurement_operator_definition}
    \mathcal{A}(Z) = \sum_{l=1}^m \tr (A\ind{l}  Z) \,  {e_l}
  \]
  with $e_l$ denoting the $l$-th canonical basis vector.
  For $D \subset \Hermitian^n$ and $\xi >0$ define
  \begin{align}
    Q_\xi (D,  A) =& \inf_{ Z \in D}\Prob \left[ | \tr (A\ind{l}  Z) | \geq \xi \right] \quad &\textrm{(marginal tail funtion)}, \label{eq:pl.marginal_tail_function}\\
    W_m (D,  A) =& 2 \, \mathbb{E} \left[ \sup_{ Z \in D} \tr \left(  Z  H \right) \right] \quad &\textrm{(mean empirical width)},
  \end{align}
  where
  \[
    H= \frac{1}{\sqrt{m}} \sum_{l=1}^m \eta_l A\ind{l}.
  \]
  Here, the $\eta_l$ are independent Rademacher random variables, i.e.\ $\Prob(\eta_l = 1) = \Prob(\eta_l=-1) = \frac{1}{2}$.
  Then for any $\xi >0$ and $t >0$
  \[
    \frac{1}{\sqrt{m}}\inf_{ Z \in D} \| \mathcal{A}( Z) \|_{\ell_1} \geq \xi \sqrt{m} Q_{2\xi}(D,  A) -  W_m (D,  A)-\xi t \label{eq:pl.mendelson}
  \]
  with probability at least $1-\mathrm{e}^{-2t^2}$.
\end{theorem}

The following two propositions summarize several results presented in \cite{Kabanava_2015_Stable} and adapt them to the problem of phase retrieval.

\begin{proposition}%
  \label{prop:nsp_implication}
  Let $\Sphere^{n^2-1}=\left\{  Z \in \Hermitian^n\colon \|  Z \|_2=1 \right\}$ be the (Frobenius norm) unit sphere and $\mathcal{B}_1 = \mathrm{conv} \left\{ \pm | x \rangle \! \langle  x| \colon  x \in \Sphere^{n-1} \right\}$ the trace-norm ball in $\Hermitian^n$.
  Define
  \[
    D := \Sphere^{d^2-1} \cap 3 \mathcal{B}_1.
    \label{eq:pl.D}
  \]
  Also, let $\mathcal{A}(Z) = \sum_{l=1}^m \tr ( A\ind{l}  Z ) \,  {e_l}$ be a measurement operator that obeys
  \begin{align}
      \frac{ \tau}{m} \| \mathcal{A}( Z) \|_{\ell_1} \geq& \norm{ Z}_2 \quad \forall  Z \in D \label{eq:pl.nsp}\\
      \| \frac{1}{\nu m}\sum_{l=1}^m  A\ind{l} -  \mathbb{I} \|_\infty \leq& \frac{1}{6}\label{eq:pl.approx_povm}
  \end{align}
  for some $\tau,\nu >0$.
  Then, the following relation holds for any $ Z \geq 0$ and any $\ketbra x$:
  \[
    \label{eq:pl.rec_guarantee}
    \Fnorm{Z - \ketbra{x}}
    \leq \frac{1}{m} \max \left\{ \tau, \frac{6}{\nu} \right\}  \Norm{\mathcal{A}\left( Z - \ketbra{x} \right)}_{\ell_1}.
  \]
\end{proposition}
\begin{proof}
  In the proof we will frequently use the decomposition $ Z =  Z_1+ Z_c$ for $ Z$ with eigenvalue decomposition $ Z = \sum_{k=1}^n \lambda_k | z^{(k)} \rangle \! \langle  z^{(k)}|$.
  Assuming $\lambda_1 \ge \ldots \ge \lambda_n$, $Z_1 = \lambda_1 | z^{(1)} \rangle \! \langle  z^{(1)}|$ is the leading rank-one component and $ Z_c =  Z- Z_1$ is the ``tail''.
  Note that, in particular, $ Z =  Z_1$ if and only if $ Z$ has unit rank.
  \Cref{eq:pl.rec_guarantee} is invariant under re-scaling, so we may w.l.o.g.\ assume $\|  Z-|{x} \rangle \! \langle {x}|\|_2=1$.
  We treat the following two cases separately:
  \begin{align}
    \mathrm{I.)} \quad& \| ( Z-|{x} \rangle \! \langle {x}|)_1 \|_1 \geq \frac{1}{2} \| ( Z-|{x} \rangle \! \langle {x}|)_c \|_1, \label{eq:pl.nsp_case1} \\
    \mathrm{II.)} \quad & \| ( Z-|{x} \rangle \! \langle {x}|)_1 \|_1 < \frac{1}{2} \| ( Z-|{x} \rangle \! \langle {x}|)_c \|_1. \label{eq:pl.nsp_case2}
  \end{align}
  Note that I.) implies
  \begin{align}
    \|  Z-|{x} \rangle \! \langle {x}| \|_1 \leq &\| ( Z-|{x} \rangle \! \langle {x}|)_1 \|_1 + \| ( Z-|{x} \rangle \! \langle {x}|)_c \|_1 \leq 3 \| ( Z-|{x} \rangle \! \langle {x}|)_1 \|_1 \\
    = & 3 \| ( Z-|{x} \rangle \! \langle {x}|)_1 \|_2 \leq 3 \|  Z- |{x} \rangle \! \langle {x}| \|_2 = 3
  \end{align}
  which in turn implies that $ Z-| {x} \rangle \! \langle {x}|$ is contained in $3 \mathcal{B}_1$.
  Thus, \eqref{eq:pl.nsp} is applicable and yields
  \[
  \|  Z - |{x} \rangle \! \langle {x}| \|_2 \leq  \frac{\tau}{m} \| \mathcal{A}( Z-|{x} \rangle \! \langle {x}|) \|_{\ell_1}
  \]
  which establishes \cref{eq:pl.rec_guarantee} for the case~\eqref{eq:pl.nsp_case1}.\\



  For the second case, we use a consequence of von Neumann's trace inequality, see e.g. \cite[Theorem~7.4.9.1]{Horn_1994_Topics}:
  Let $ A,  B$ be matrices with singular values $\sigma_k ( A),\sigma_k ( B)$ arranged in non-increasing order.
  Then
  \[
    \|  A -  B \|_1 \geq \sum_{k=1}^n | \sigma_k ( A) - \sigma_k ( B)|
  \]
  This relation implies
  \begin{align}
    \|  Z \|_1 =& \| |{x} \rangle \! \langle {x}| - (|{x} \rangle \! \langle {x}|- Z) \|_1
    \geq \sum_{k=1}^n \left| \sigma_k (| x \rangle \! \langle  x|) - \sigma_k (| x \rangle \! |\langle  x|-  Z ) \right| \\
    \geq & \sigma_1 (| x \rangle \langle  x|) - \sigma_1 \left( | x \rangle \! \langle  x| -  Z \right)+ \sum_{k=2}^n \sigma_k \left( | x \rangle \! \langle  x| -  Z\right) \\
    =&  \| | x \rangle \! \langle  x| \|_1  - \| (| x \rangle \! \langle  x| -  Z)_1 \|_1 + \|(| x \rangle \! \langle  x| - Z)_c \|_1 \\
    >& \| | x \rangle \! \langle  x| \|_1 + \frac{1}{2} \| (| x \rangle \! \langle  x|- Z)_c \|_1,
  \end{align}
  where the last inequality follows from \eqref{eq:pl.nsp_case2}. Consequently,
  \begin{align}
    \| | x \rangle \! \langle  x| -  Z \|_1
    =& \| (| x \rangle \! \langle  x| -  Z)_1 \|_1 + \| (| x \rangle \! \langle  x|- Z)_c \|_1
    \leq \frac{3}{2} \| (| x \rangle \! \langle  x|-  Z )_c \|_1 \nonumber \\
    < & 3 \left( \|  Z \|_1 - \| | x \rangle \! \langle  x| \|_1 \right). \label{eq:pl.nsp_aux2}
  \end{align}
  Now, positive semidefiniteness of both $ Z$ and $\ket{ x}\bra{ x}$ together with assumption~\eqref{eq:pl.approx_povm} implies
  \begin{align}
    \|  Z \|_1 - \| |{x} \rangle \! \langle {x}| \|_1
    =& \tr ( Z-|{x} \rangle \! \langle {x}|) =  \tr \left( \mathbb{I} \left(  Z-| {x} \rangle \! \langle x|\right) \right) \\
    =&  \tr \left( \left( \mathbb{I} - \frac{1}{\nu m} \sum_{k=1}^m A\ind{l} \right)  Z-|{x} \rangle \! \langle {x}| \right) + \frac{1}{\nu m} \sum_{k=1}^m \tr \left( A_k ( Z-|{x} \rangle \! \langle {x}|) \right) \\
    \leq &  \left\|\mathbb{I}- \frac{1}{ \nu m} \sum_{k=1}^m A_k \right\|_\infty \|  Z-|{x} \rangle \! \langle {x}| \|_1 + \frac{1}{\nu m} \| \mathcal{A}(|{x} \rangle \! \langle {x}|- Z) \|_{\ell_1} \\
    \leq &  \frac{1}{6 } \|  Z-|{x} \rangle \! \langle {x}| \|_1 + \frac{1}{\nu m} \| \mathcal{A}(|{x} \rangle \! \langle {x}|- Z) \|_{\ell_1}.
  \end{align}
  Inserting this into \eqref{eq:pl.nsp_aux2} yields
  \begin{align}
  \| | x \rangle \! \langle  x| -  Z \|_1 < \frac{1}{2} \| |{x} \rangle \! \langle {x}|- Z \|_1 +  \frac{3}{\nu m} \| \mathcal{A}(|{x} \rangle \! \langle {x}|- Z) \|_{\ell_1}
  \end{align}
  which implies the claim for case II in \eqref{eq:pl.nsp_case2}.
\end{proof}


\begin{lemma}
  Let $D$ be the set introduced in \cref{eq:pl.D} and let $ A = \ketbra\alpha$, where $\alpha$ satisfies \cref{eq:pl.sub_isotropy,eq:pl.subexponential}.
  Then, the marginal tail function~\eqref{eq:pl.marginal_tail_function} obeys
  \[
    Q_\xi (D,  A) \geq  C_Q \left( 1-  \frac{\xi^2}{C_\mathrm{SI}}\right)^2  \quad \forall 0 \leq \xi \leq \sqrt{C_\mathrm{SI}},
  \]
  where $C_Q>0$ is a sufficiently small constant.
\end{lemma}
\begin{proof}
  Fix $ Z \in D$, then $\|  Z \|_2 =1$ by definition of $D$.
  Note that sub-isotropy \eqref{eq:pl.sub_isotropy} and the Paley-Zygmund inequality imply for any $\xi \in [0,1]$
  \begin{align}
    \Prob \left[ | \langle  a|  Z | a \rangle| \geq \xi \right]
    \geq & \Prob \left[ \langle  \alpha|  Z | \alpha \rangle^2 \geq \frac{\xi^2}{C_\mathrm{SI}} \mathbb{E} \left[ \langle  \alpha| Z| \alpha \rangle^2 \right] \right] \\
    \geq & \left(1-\frac{\xi^2}{C_\mathrm{SI}}\right)^2 \frac{\mathbb{E} \left[ \langle  \alpha | Z | \alpha \rangle^2 \right]^2}{\mathbb{E} \left[ \langle  \alpha|  Z | \alpha \rangle^4 \right]}.
  \end{align}
  Sub-isotropy ensures that the numerator is lower bounded by $C_\mathrm{SI}^2 \|  Z \|_2^4 = C_\mathrm{SI}^2$.
  In order to derive an upper bound on the denominator, we use the constraint $\|  Z \|_1 \leq 3$ for any $ Z \in D$ together with the subgaussian tail behavior \eqref{eq:pl.subexponential} of $\alpha$.
  Insert an eigenvalue decomposition $ Z = \sum_{i=1}^n \lambda_i | z^{(i)} \rangle \! \langle  z^{(i)}|$ (with $\lambda_i \in \mathbb{R}$ and $ z^{(i)} \in \Sphere^{n-1}$) and note
  \begin{align}
    \mathbb{E} \left[ \langle  \alpha|  Z | \alpha \rangle^4 \right]
    \leq & \sum_{i_1,i_2,i_3,i_4=1}^n | \lambda_{i_1} \lambda_{i_2} \lambda_{i_3} \lambda_{i_4} | \mathbb{E} \left[ \prod_{k=1}^4 | \langle  \alpha,  z^{(i_k)} \rangle|^2 \right]. \label{eq:pl.Q_aux1}
  \end{align}
  Now fix $ z^{(i_1)},\ldots, z^{(i_4)}$ and use the inequality of arthimetic and geometric means as well as the fundamental relation between $\ell_p$-norms ($\|  v \|_{\ell_1} \leq k^{1-\frac{1}{k}} \|  v \|_{\ell_k}$ for $v \in \mathbb{R}^k$) to conclude
  \begin{align}
    \mathbb{E} \left[ \prod_{k=1}^4 | \langle  \alpha, z^{(i_k)}\rangle |^2 \right]
    \leq \frac{1}{4} \sum_{k=1}^4 \mathbb{E} \left[ | \langle  \alpha,  z^{(i_k)} \rangle|^8 \right]
    \leq C_\mathrm{SG} 4!,
  \end{align}
  where the last inequality follows from condition \eqref{eq:pl.subexponential}.
  Consequently,
  \begin{align}
    \mathbb{E} \left[ \langle  \alpha|  Z |  \alpha\rangle^4 \right]
    \leq C_\mathrm{SG} 4! \sum_{i_1,i_2,i_3,i_4} | \lambda_{i_1} \lambda_{i_2} \lambda_{i_3} \lambda_{i_4} |
    = 24 C_\mathrm{SG} \|  Z \|_1^4 \leq 24 \times 3^4 C_\mathrm{SG},
  \end{align}
  because $ Z \in D$ implies $\|  Z \|_1 \leq 3$.
  In summary,
  \begin{align}
    \Prob \left[ | \langle  \alpha|  Z | \alpha \rangle| \geq \xi \right]
    \geq \left(1-\frac{\xi^2}{C_\mathrm{SI}}\right)^2 \frac{\mathbb{E} \left[ \langle  \alpha|  Z | a \rangle^2 \right]^2}{\mathbb{E} \left[ \langle  \alpha|  Z | \alpha \rangle^4 \right]}
    \geq \left(1-\frac{\xi^2}{C_\mathrm{SI}}\right)^2 \frac{C_\mathrm{SI}^2}{1944C_\mathrm{SG}}
  \end{align}
  and the bound on $Q_\xi (D, A)$ with $C_Q = \frac{C_\mathrm{SI}^2}{1944 C_\mathrm{SG}}$ follows from the fact that this lower bound holds for any $ Z \in D$.
\end{proof}


\begin{lemma}[Bound on the mean empirical width]
  Let $D$ be the set introduced in \cref{eq:pl.D} and let $ H = \frac{1}{\sqrt{m}} \sum_{l=1}^m \eta_l \ketbra{\alpha\ind{l}}$, where each $\alpha\ind{l}$ is subexponential in the sense of \eqref{eq:pl.subexponential} and $m \geq \frac{2 \ln (3)}{C_\mathrm{SG}} n$.
  Then there exists a constant $C_\mathrm{W} >0$ such that
  \[
    W_m (D, A) \leq C_\mathrm{W} \sqrt{n}.
  \]
\end{lemma}
\begin{proof}
  Note that by construction $D \subset 3 \mathcal{B}_1$, and consequently,
  \begin{align}
    W_m (D,  A) = 2 \mathbb{E} \left[ \sup_{ Z \in D} \tr ( Z  H) \right] \leq 6 \mathbb{E} \left[ \sup_{ Z \in \mathcal{B}_1} \tr ( Z  H) \right] = 6 \mathbb{E} \left[ \|  H \|_\infty  \right], \label{eq:pl.Wm_hoelder}
  \end{align}
  where the last equality follows from the duality of trace and operator norm.
  Now note that $\tilde{ H} = \sqrt{m}  H$ is of the form~\eqref{eq:pl.Htilde}, where each $\alpha\ind{l}$ is an independent Rademacher random variable.
  \Cref{thm:bernstein} thus implies
  \begin{align}
    \Prob \left[\|  H \|_\infty \geq t \right]
    \leq
    \begin{cases}
     2 \times 9^n \exp \left( - \frac{t^2}{8 C_\mathrm{SG}} \right) & t \leq 2C_\mathrm{SG} \sqrt{m}, \\
    2 \times 9^n \exp \left( - \frac{\sqrt{m}}{2} \left( t - C_\mathrm{SG} \sqrt{m} \right) \right) & t \geq 2 C_\mathrm{SG} \sqrt{m}
    \end{cases}
    \label{eq:pl.Wm_tails}
  \end{align}
  and we can bound $\mathbb{E} \left[ \|  H \|_\infty \right]$ by using the absolute moment formula,
  %$
  %\mathbb{E} \left[ \| H \|_\infty \right] = \int_0^\infty \Prob \left[ \| H \|_\infty \geq t \right] \mathrm{d}t,
  %$
  see e.g.\ \cite[Propostion~7.1]{Foucart_2013_Mathematical}, and bounding the effect of the tails via \eqref{eq:pl.Wm_tails}.
  To this end, we split the real line into three intervals $[0, c \sqrt{n}], [c\sqrt{n}, 2 C_\mathrm{SG} \sqrt{m}], [2 C_\mathrm{SG} \sqrt{m},\infty[$, where $c$ is a constant that we fix later:
  \begin{align}
    \mathbb{E} \left[ \| H\|_\infty \right]
    &=& \int_0^\infty \Prob \left[ \| H\|_\infty \geq t \right] \mathrm{d}t \\
    &\leq& \int_0^{c \sqrt{n}} 1 \mathrm{d}t + 2 \times 9^n \left( \int_{c \sqrt{n}}^{2 C_\mathrm{SG} \sqrt{m}} 2 \exp \left( - \frac{t^2}{8 C_\mathrm{SG}} \right) \mathrm{d}t
     +  \mathrm{e}^{\frac{m C_\mathrm{SG}}{2}} \int_{2 C_\mathrm{SG} \sqrt{m}}^\infty \exp\left( - \frac{\sqrt{m}t}{2}  \right) \mathrm{d} t \right)\\
    &\leq & c \sqrt{n} + 2 \times 9^n \left( \int_{c \sqrt{n}}^{2 C_\mathrm{SG} \sqrt{m}}  \exp \left( - \frac{t^2}{8 C_\mathrm{SG}} \right) \mathrm{d}t + \frac{2}{\sqrt{m}} \mathrm{e}^{-\frac{C_\mathrm{SG} m}{2}}\right).
  \end{align}
  For the remaining Gauss integral, we use $\frac{t}{c \sqrt{n}} \geq 1\; \forall t \geq c\sqrt{n}$ to conclude
  \begin{align}
    \int_{c \sqrt{n}}^{2 C_\mathrm{SG} \sqrt{m}}  \exp \left( - \frac{t^2}{8 C_\mathrm{SG}} \right) \mathrm{d}t
    %\leq & \int_{C \sqrt{n}}^{4 \mathrm{e}^2 \sqrt{m}} \frac{t}{C \sqrt{n}}  \exp \left( - \frac{t^2}{32 \mathrm{e}^2} \right) \mathrm{d} t
    \leq  \int_{c \sqrt{n}}^\infty \frac{t}{c \sqrt{n}}  \exp \left( - \frac{t^2}{8 C_\mathrm{SG}} \right) \mathrm{d} t
    %=& - \frac{32 \mathrm{e}^2}{C\sqrt{n}} \exp \left( - \frac{t^2}{32 \mathrm{e}^2}\right) |_{t=C \sqrt{n}}^{t=\infty}
    = \frac{8 C_\mathrm{SG}}{c \sqrt{n}} \exp \left( - \frac{c^2 n}{8 C_\mathrm{SG}} \right).
  \end{align}
  Now, fixing $c = 4 \sqrt{\ln (3)C_\mathrm{SG}}$ assures $\exp \left( -\frac{c^2 n}{8 C_\mathrm{SG}}\right) = 9^{-n}$ and consequently
  \begin{align}
    \mathbb{E} \left[ \|  H \|_\infty \right]
    \leq & 4  \sqrt{ \ln (3) C_\mathrm{SG} n} + \frac{4 \sqrt{C_\mathrm{SG}}}{\sqrt{ \ln (3) n}} + \frac{4}{\sqrt{m}} \mathrm{e}^{2 \ln (3) n - C_\mathrm{SG} m} \\
    %\leq &8 \mathrm{e} \sqrt{ \ln (3) n} + \frac{8 \mathrm{e}}{\sqrt{ \ln (3) n}} + \frac{8 \mathrm{e}}{\sqrt{4 \ln (3) n}} \\
    \leq & 4\sqrt{C_\mathrm{SG}} \left( \sqrt{ \ln (3) n} + \frac{2}{\sqrt{ \ln (3) n}} \right) \leq 12 \sqrt{ \ln (3) C_\mathrm{SG} n}.
  \end{align}
  where the second inequality follows from $m \geq \frac{2 \ln (3)}{C_\mathrm{SG}} n$. Inserting this bound into \eqref{eq:pl.Wm_hoelder} yields the claim with $C_\mathrm{W} = 72 \sqrt{ \ln (3) C_\mathrm{SG}}$.
\end{proof}

\begin{proof}[Proof of \cref{prop:pl.nsp}]
  Now we are ready to apply Mendelson's small ball method \eqref{eq:pl.mendelson}.
  For $D$ defined in \eqref{eq:pl.D} and measurements $ A_l = \ketbra{\alpha\ind{l}}$ with $\alpha_l$ obeying \cref{eq:pl.sub_isotropy,eq:pl.subexponential}, the bounds from the previous Lemmas imply
  \begin{align}
    \frac{1}{\sqrt{m}}\inf_{ Z \in D} \|\mathcal{A}( Z) \|_{\ell_1} \geq \xi \sqrt{m} C_Q \left( 1- \frac{4 \xi^2}{C_\mathrm{SI}} \right)^2 - 2 C_\mathrm{W} \sqrt{n} - \xi t \quad \forall \xi \in (0, 1/\sqrt{C_\mathrm{SI}}), \forall t \geq 0
  \end{align}
  with probability at least $1- \mathrm{e}^{-2t^2}$. We choose $\xi = \sqrt{C_\mathrm{SI}}/4$ and $t = \gamma_1 \sqrt{m}$, where $\gamma_1 = \frac{9 C_Q}{32}$ and obtain with probability at least $1-\exp \left( -2 \gamma_1 m \right)$:
  \begin{align}
    \frac{1}{\sqrt{m}}\inf_{ Z \in D} \|\mathcal{A}( Z) \|_{\ell_1} \geq & \frac{9 C_Q\sqrt{C_\mathrm{SI}}}{64} \sqrt{m} -  C_\mathrm{W}\sqrt{n} - \frac{\sqrt{C_\mathrm{SI}}}{4} \frac{9 C_Q}{32} \sqrt{m} \\
    = & C_\mathrm{W} \left( \frac{9 C_Q \sqrt{C_\mathrm{SI}}}{128 C_\mathrm{W}} \sqrt{m} - \sqrt{n} \right).
  \end{align}
  Setting $m = C n$ with $C = \left( \frac{256 C_\mathrm{W}}{9 C_Q \sqrt{C_l}} \right)^2$ implies
  \[
    \frac{1}{\sqrt{m}} \inf_{ Z \in D} \| \mathcal{A}( Z) \|_{\ell_1} \geq 2 C_\mathrm{W} \sqrt{n} = \frac{2 C_\mathrm{W}}{\sqrt{C}} \sqrt{m}
  \]
  with probability at least $1- \mathrm{e}^{-2 \gamma_1 m}$.
  For $\tau = \frac{ 2 C_\mathrm{W}}{\sqrt{C}}$, the first claim in \cref{prop:pl.nsp} follows from rearranging this expression and using $\|  Z \|_2=1$ for all $ Z \in D$.\\


  Let us now move on to establishing the second statement \eqref{eq:pl.approx_povm}:
  Isotropy \eqref{eq:pl.tight_frame} implies
  \begin{align}
    \frac{1}{ C_\mathrm{I} m} \sum_{l=1}^m \ketbra{\alpha\ind{l}} - \mathbb{I}
    = \frac{1}{C_\mathrm{SG} m} \sum_{l=1}^m \left( \ketbra{\alpha\ind{l}} - \mathbb{E} \left[ \ketbra{\alpha\ind{l}} \right] \right)
  \end{align}
  and each $\alpha\ind{l}$ has subgaussian tails by assumption \eqref{eq:pl.subexponential}.
  Thus, \cref{thm:bernstein} is applicable and setting $t= \min \left\{\frac{1}{6},2 C_\mathrm{SG} \right\}$ yields
  \begin{align}
    \Prob \left[ \left\| \frac{1}{C_\mathrm{I} m} \sum_{l=1}^m \ketbra{\alpha\ind{l}} -  \mathbb{I} \right\|_\infty \geq \frac{1}{6} \right]
    & \leq 2 \exp \left( 2 \ln (3) n - \frac{C_\mathrm{I} m \min\left\{ 1/6, 2 C_\mathrm{SG} \right\}}{8 C_\mathrm{SG}} \right) \\
    & \leq 2 \exp \left( - \gamma_2 m \right),
  \end{align}
  where the second inequality follows from $m \geq C n$, provided that $C$ is sufficiently large.
  Finally, we use the union bound  for the overall probability of failure and set $\gamma := \min \left\{ 2 \gamma_1,\gamma_2 \right\}$.
\end{proof}


\subsection{Characterization via PhaseLift}%
\label{sub:pl.characterization}

In this section, we are going to apply the results from the last section to the original problem of recovering the transfer matrix of a linear optical circuit.
The measured intensity at detector $j$ as given by \cref{eq:pl.intensities} exclusively provides us with information about the $j$-th row vector of ${M}$:
\[
  I_j({\alpha})
  = \left| \sum_{k=1}^n M_{j,k} \alpha_k \right|^2 + \epsilon_j
  = \left\vert  \langle \cc{{M}_j}, \alpha \rangle  \right\vert^2 + \epsilon_j. \quad 1 \leq j \leq n
  \label{eq:pl.intensities_as_overlap}
\]
Here, we have defined ${M}_j$ as the row vectors of ${M}$.
Since the measured intensities in \cref{eq:pl.intensities_as_overlap} exactly resemble the measurement model of the phase retrieval problem in \cref{eq:pl.phase_retrieval_measurements}, we can use the ideas introduced in \cref{sec:pl.phase_retrieval} to recover $M$:
For this purpose, we propose the following protocol:
\begin{enumerate}
  \item sample $m$ random coherent input vectors $\alpha^{(l)}$ from an appropriate ensemble,
  \item measure the $m \times n$ intensities $I_1(\alpha^{(l)}), \ldots, I_n ( \alpha^{(l)})$ with $l=1,\ldots,m$, and
  \item use PhaseLift~\eqref{eq:pl.PhaseLift} to recover each ${M}_j$ individually.
\end{enumerate}

In \cref{sub:pl.recr}, we introduced multiple ensembles to choose the $\alpha\ind{l}$ from.
However, not all of these are equally well suited for the problem at hand.
First and foremost, we recall from \cref{sec:pl.optics} that input vectors with constant norm $\ltwonorm{\alpha\ind{l}} = 1$ are better suited for the setup depicted in \cref{fig:pl.experimental.schematic}.
% Otherwise, one would need to introduce an additional mode to redirect \quotes{extraneous} intensity or vary the output power of the laser.
% Furthermore, since the experimental implementation presented in \cref{sec:pl.optics} estimates the intensities from single photon counting rates, $\ltwonorm{\alpha}$ corresponds to the total probability of measuring a photon, and hence, should be fixed to one.
Also, recall that the RECR sampling scheme was conceived with our application in linear optics in mind:
One major drawback of the uniform scheme is that each component may take any possible value for its complex phase.
In contrast, the RECR scheme has only four possible values for the phase shift, namely $\frac{k \pi}{2}$ for $k=1,\ldots,4$.
Therefore, the reconfigurable phase shifters in the implementation outlined in \cref{fig:pl.experimental.schematic} can be calibrated to these values.
A similar argument applies to the magnitudes of the RECR components, which can only assume $n$ possible values due to the additional normalization constraint $\ltwonorm{\alpha\ind{l}} = 1$.
However, the current linear architecture does not benefit from this additional constraints.
Using a tree-like structure in the preparation stage could further improve the practical performance of the PhaseLift reconstruction using RECR vectors.
We discuss this idea further in the conclusion and outlook section.\\



The rest of this section is devoted to adapting the results from \cref{sub:pl.recr} to derive rigorous performance guarantees for the proposed characterization protocol outlined above.
We start by stating its rigorous version.

\begin{protocol}[\emph{Reconstruction of the transfer matrix ${M}$}]%
  \label{prot:pl.detailed_reconstruction}
  Let ${M}$ be an arbitrary $n \times n$ transfer matrix as defined in~\eqref{eq:pl.coherent_transfer_matrix}.
  In order to approximately recover it, sample $m = Cn$ random coherent input states $\ket{{\alpha}^{(1)}},\ldots,\ket{{\alpha}^{(m)}}$, with $\alpha\ind{l}$ chosen from the uniform or RECR scheme normalized such that $\ltwonorm{\alpha\ind{l}} = 1$.
  Measure the $mn$ intensities
  \[
    y_j^{(l)} = \left| \sum_i M_{j,i} \, \alpha_i^{(l)} \right|^2 + \epsilon_j^{(l)} \quad \forall 1 \leq j \leq n, \quad 1 \leq l \leq m,
  \]
  where $\epsilon_j^{(l)}$ denotes the additive noise at detector site $j$ when measuring the intensity resulting from input state  $\ket{{\alpha}^{(l)}}$.
  For each $1 \leq j \leq n$, solve the semi-definite program
  \begin{align}
    {Z}^\sharp_{j} = \underset{{Z} \in \Hermitian^n}{\argmin}& \quad \sum_{l=1}^m \left| \tr \left( (|{\alpha}^{l} \rangle \langle {\alpha}^{(l)} | )  Z \right) - y_j^{(l)} \right| \label{eq:tmat_recovery_program}\\
    \textrm{subject to} & \quad {Z} \geq 0 \nonumber
  \end{align}
  and let $\cc{M}_j^\sharp$ be the complex conjugate of the eigenvector of ${Z}^\sharp_{j}$ corresponding to its largest eigenvalue rescaled to have length $\Norm{\cc{M}_j^\sharp}_{\ell_2} = \sqrt{ \| {Z}^\sharp_{j} \|_\infty}$.
  Then, we estimate ${M}$ by
  \[
    {M}^\sharp =
    \begin{pmatrix}
      \transpose{{M^\sharp}_1} \\ \vdots \\  \transpose{{M^\sharp}_n}
    \end{pmatrix}.
    \label{eq:pl.transfermat_estimator}
  \]
\end{protocol}

Note that \cref{eq:pl.transfermat_estimator} simply amounts to stacking the separately recovered row vectors $ M_j^\sharp$.
Now, a simple extension of \cref{thm:pl.phaselift_noisy,thm:pl.phaselift_noisy_recr} yield a similar performance guarantee for \cref{prot:pl.detailed_reconstruction}:
Due to the similarity of the intensity measurements~\eqref{eq:pl.intensities_as_overlap} for a single row $M_j$ of the transfer matrix and the measurements assumed in \cref{thm:pl.phaselift_noisy}, the latter guarantees recovery of said row with high probability by means of PhaseLift~\eqref{eq:pl.PhaseLift}.
In order to succinctly state the final result, we introduce some additional notation.
Define the total noise at detector $j$ (measured in $\ell_1$-norm) to be
\[
  \epsilon_j^{\mathrm{tot}}= \sum_{l=1}^m \abs{\epsilon_j^{(l)}}
  \label{eq:pl.error_term_summand}
\]
and the overall noise strength:
\[
  \epsilon^{\mathrm{tot}} = \sqrt{ \sum_{j=1}^n {\epsilon_j^{\mathrm{tot}}}^2}.
\]
This formulation allows for treating the different output modes and their detector noise levels individually.
In particular, we do not require a universal type of noise for all detectors, but allow for taking into account detector dependent noise of different strength, i.e.\ varying noise levels.

\begin{corollary}[Performance guarantee for Protocol~\ref{prot:pl.detailed_reconstruction}]%
  \label{cor:pl.performance_guarantee}
  The reconstruction ${M}^\sharp$ of any transfer matrix ${M}$ by means of Protocol~\ref{prot:pl.detailed_reconstruction} satisfies
    \[
      \min_{{\mu}: \abs{\mu_j} = 1} \left\|  {M}^\sharp -  \mathrm{diag}(\mu_1,\ldots,\mu_n) {M} \right\|_2
      \leq C \frac{n \epsilon^\mathrm{tot}}{m \nu}.
      \label{eq:pl.total_bound}
    \]
  with probability at least $1 - \Order \left( \mathrm{e}^{-\gamma m}\right)$.
  Here, $C$ and $\gamma$ are positive constant of sufficient size and
  \[
    \nu = \min_{1 \leq j \leq n} \| {M}_j \|_{\ell_2}.
    \label{eq:pl.definition_normconst}
  \]
\end{corollary}
Recall that $\mathrm{diag}(\mu_1, \ldots, \mu_n)$ are the row-phases of ${M}$, which are unrecoverable from the intensity measurements~\eqref{eq:pl.intensities}.
We include the additional correction~\eqref{eq:pl.definition_normconst} to deal with possible loss.
Unitary transfer matrices satisfy $\nu = 1$.

\begin{proof}
  For any fixed row vector ${M}_j$,
  \[
    \min_{0 \leq \phi \leq 2 \pi}\left\| {M}_j^\sharp - \mathrm{e}^{i \phi} {M}_j \right\|_{\ell_2} \leq C' n \min
    \left\{
    \| {M}_j \|_{\ell_2}, \frac{ \epsilon_j^{\mathrm{tot}}}{m \| {M}_j \|_{\ell_2}}
    \right\}.
    \label{eq:noisy_reconstruction_vectorial_bound}
  \]
  follows directly from \cref{thm:pl.phaselift_noisy}.
  Note that the additional $n$ factor compared to \cref{eq:vectorial_noisy_bound} is due to the normalization of the input vectors.
  The input vectors need to be scaled by $\sqrt{n}$ in order to be able to apply \cref{thm:pl.phaselift_noisy}.

  Before we can move on to determine the remaining row vectors ${M}_{i}$ ($i \neq j$) of ${M}$, it is important to point out that the recovery guarantees of \cref{thm:pl.phaselift_noisy,thm:pl.phaselift_noisy_recr} are \emph{universal}: one instance of randomly chosen measurement vectors suffices to recover \emph{any} vector ${x} \in \Complex^n$.
  This allows for applying this reconstruction guarantee to all $n$ row vectors ${M}_j$ simultaneously.
  The total noise bound~\eqref{eq:pl.total_bound} now follows from the entry-wise definition of the Frobenius norm:
  \begin{align}
    \min_{{\mu}}\left\|  {M}^\sharp -  {D} ({\mu}) {M} \right\|_2 ^2
    &= \min_{0 \leq \phi_1,\ldots,\phi_n \leq 2 \pi}
    \sum_{j=1}^n \left\| {M}_j^\sharp - \mathrm{e}^{i \phi_j} {M}_j \right\|_{\ell_2}^2 \\
    &= \sum_{j=1}^n \min_{0 \leq \phi_j \leq 2 \pi} \left\| {M}_j^\sharp - \mathrm{e}^{i \phi_j} {M}_j \right\|_{\ell_2}^2 \\
    \label{eq:pl.third_line}
    & \leq C^2 n^2 \sum_{j=1}^n  \min \left\{ \| {M}_j \|_{\ell_2}^2, \frac{ \eta_{(j)}^2}{m^2 \|{M}_j \|_{\ell_2}^2} \right\} \\
    &\leq \left(C n\right)^2 \sum_{j=1}^n \frac{\eta_{j}^2}{m^2 \|{M}_j \|_{\ell_2}^2} \\
    & \leq \frac{\left(Cn \right)^2}{m^2 \nu} \sum_{j=1}^n \eta_{j}^2 \\
    &= \left( C n \frac{\eta^{\mathrm{tot}}}{m \nu} \right)^2,
  \end{align}
  Here, we have used \cref{eq:vectorial_noisy_bound} for each summand in \cref{eq:pl.third_line}.
\end{proof}


The performance guarantee above has an interesting consequence for experimental design:
The right hand side of \cref{eq:pl.total_bound} is mainly determined by the signal-to-noise ratio $\frac{\nu}{\epsilon^\mathrm{tot} / m}$.
Remarkably, the noise term does not become smaller for increasing $m$.
To be more precise, let us assume that each detection error  is independent and normally distributed with standard deviation $\sigma$, i.e.\ $\epsilon\ind{l}_j \sim \Normal(0, \sigma^2)$.
Then,
\[
  \Exp \epsilon^\mathrm{tot}_j = \frac{1}{m} \sum_l \Exp \epsilon\ind{l}_j = \Exp \epsilon\ind{1}_j = \sqrt{\frac{2}{\pi}} \sigma,
\]
and hence, the expected error $\Exp \epsilon^\mathrm{tot}$ is independent of $m$.
Of course, the standard deviation of $\epsilon^\mathrm{tot}_j$ scales as $\frac{1}{\sqrt{m}}$.
Therefore, an increase of $m$ past the threshold $Cn$ in \cref{cor:pl.performance_guarantee} mainly influences the (exponentially small) failure probability.
In other words, said corollary implies that once the sampling threshold is reached, there is not much use to further increase the number of measurements as the error bound~\eqref{eq:pl.total_bound} is primarily determined by the uncertainties of a single measurements.
Hence, any additional experimental time should be invested to reduce the uncertainty of the single measurements, e.g.\ by increasing the number of single photon events used to estimate $I_j(\alpha)$.

By studying the assumptions and the proof of the underlying \cref{thm:pl.phaselift_noisy}, we also note that this behavior is to be expected.
Since this theorem does not assume any statistical properties of the noise, but only assumes that the noise $\epsilon\ind{l}$ is bounded, we cannot expect a statistical improvement by increasing the number of measurements.
For example, if we assume a constant, purely systematic error $\epsilon\ind{l} = c$ for all measurements, then no improvement can be expected even for $m\to\infty$.

However, it should be kept in mind that \cref{cor:pl.performance_guarantee} only provides necessary conditions for recovery, which are not optimal in the large $m$ regime.
Therefore, we perform numerical simulations in the next chapter to further investigate how experimental time should be spent.
In other words, we study the question how the reconstruction performs as a function of $m$ when the total experimental time budget is fixed.
Note that this behavior has already been explored in the context of quantum state tomography via compressed sensing in~\cite{Flammia_2012_Quantum}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}%
\label{sec:pl.results}

\subsection{Numerical Results}%
\label{sub:pl.results.numerics}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[tbp]
  \begin{subfigure}{.475\columnwidth}
    \includegraphics[width=\linewidth]{fig/phaselift_sim_gaussian}
    \caption{\label{sfig:pl.simplot.gaussian}%
      Uniform sampling
    }
   \end{subfigure}
  \begin{subfigure}{.475\columnwidth}
    \includegraphics[width=\linewidth]{fig/phaselift_sim_recr}
    \caption{\label{sfig:pl.simplot.recr}%
      RECR sampling
    }
   \end{subfigure}
  \caption{\label{fig:pl.simplot}%
    Simulated recovery-probability using the two different sampling schemes under noisy measurements with $\sigma = 0.05$.
    For each given dimension, the transfer matrices to be recovered consist of 97 Haar random unitaries as well as the identity, the swap-matrix, and the discrete Fourier transform.
    The red line indicates the conjectured phase transition at $4 n - 4$.
  }
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We demonstrate the practical applicability of the PhaseLift characterisation protocol using simulated experiments.
The simulation depicted in \cref{fig:pl.simplot} aims to visualize the performance guarantees from \cref{cor:pl.performance_guarantee}:
For each given dimension $n$, we choose 100 target unitaries.
Each of these is reconstructed by means of \cref{prot:pl.detailed_reconstruction} with a varying number of measurements $m$.
The input vectors are sampled from the uniform ensemble in \cref{sfig:pl.simplot.gaussian} and from the normalized RECR ensemble in \cref{sfig:pl.simplot.recr}.
For the measurement noise $\epsilon_j$ from \cref{eq:pl.intensities_as_overlap}, we assume independent, centered Gaussian noise with standard deviation $\sigma = 0.05$.
The density plots show the fraction of successfully recovered unitaries.
Here, the criterion for success is whether the distance of the reconstruction $M^\sharp$ measured in Frobenius norm is smaller than the threshold  $4 \sigma n$ in accordance with the error bound~\eqref{eq:pl.total_bound}.

\Cref{sfig:pl.simplot.gaussian,sfig:pl.simplot.recr} show a pronounced phase transition around $m = 4n$.
This demonstrates the high sample efficiency of the PhaseLift reconstruction.
Not only does the number of measurements scale linearly in the system size -- as rigorously proven in \cref{cor:pl.performance_guarantee} -- but the scaling coefficient is small as well.\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
  \begin{subfigure}{.475\columnwidth}
    \includegraphics[width=\linewidth]{fig/phaselift_sim_errorscaling_dft_5}
    \caption{\label{sfig:pl.simerror.five}%
      $n = 5$
    }
   \end{subfigure}
  \begin{subfigure}{.475\columnwidth}
    \includegraphics[width=\linewidth]{fig/phaselift_sim_errorscaling_dft_10}
    \caption{\label{sfig:pl.simerror.ten}%
      $n = 10$
    }
   \end{subfigure}
  \caption{\label{fig:pl.simerror}%
    Simulated reconstruction error from RECR measurements for the with fixed time budget as a function of the number of measurements for two different circuit sizes.
    The total photon number in for each reconstruction is $N = 4600 \times t$.
    Then, the output for each input vector $\alpha$ is a multinomial distribution with the number of trials given by $\frac{M}{m}$.
    Intensities for the PhaseLift reconstruction is estimated for 100 samples of each distribution.
    The solid line indicates the mean and the colored areas the $0.025$ and $0.975$ quantiles.
  }
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


In the simulations depicted in \cref{fig:pl.simplot}, we assumed a constant noise level for each $m$.
Therefore, the lab-time required for taking the data or, put differently, the number of single photon events required for estimating the intensities increases linear in $m$.
We now investigate the question posed at the end of \cref{sub:pl.characterization}, namely how the reconstruction performs as a function of $m$ when the total experimental time budget is fixed.
Recall from \cref{eq:pl.photon_stats} that the single photon counting statistics is given by a multinomial distribution with number of trials $N$ given by the total photon number and the probabilities $p_j$ given by the expectation values in \cref{eq:pl.experiment.probabilities}.
Denote by $N\ind{l}$ the number of photons used to estimate the output intensities for a single photon input state $\ket{\psi(\alpha\ind{l})}$ with $l=1,\ldots,m$.
In \cref{fig:pl.simerror}, we depict the reconstruction error with the total number of photons used for reconstruction $N = \sum_l N\ind{l}$ kept fixed.
To be more precise, we choose the total number of photons $N$ as a multiple of the counting rate from the experiment $\Gamma = 4600\,s^{-1}$ introduced in \cref{sec:pl.optics}, i.e.
\[
  N = t \times \Gamma,
  \label{eq:pl.counts_from_time}
\]
where $t$ is the time spent only on taking the single-photon data.
Therefore, we have $N\ind{l} = \frac{N}{m}$, where $m$ is the number of preparation vectors.
The reconstructions in \cref{fig:pl.simerror} are then performed by randomly sampling 100 outcomes from the output counting statistics of each input state.
For larger $m$, the $N\ind{l}$ becomes smaller, and therefore, the statistical error in each estimated intensity grows.

First, we see from \cref{fig:pl.simerror} that -- as expected -- taking more data by increasing $t$ improves the overall reconstruction quality.
Also note that the reconstruction error is approximately independent of $m$ above a certain threshold.
This clearly shows that the recovery guarantee in \cref{cor:pl.performance_guarantee} is not tight for larger values of $m$, as the right hand side of \cref{eq:pl.total_bound} grows with the individual statistical error of each measurement.

From \cref{fig:pl.simerror}, one could conclude that there is no advantage of taking a small value of $m$ in the experiments.
This conclusion rests on the assumption that the total number of photons is the figure of merit that best describes an experimentalist's budget.
However, in the concrete experimental architecture introduced in \cref{sec:pl.optics}, this is not the case:
In reality, the number of distinct settings for the reconfigurable chip is more critical for the time required to perform a given experiment.
This is due to the fact that switching the reconfigurable phase shifters and couplers takes more time that the actual data taking process.
Therefore, if we take into account this additional cost for reconfiguring the preparation stage on the chip, reconstructions with a smaller number of more precise measurements perform better when the \quotes{time budget} is kept fixed.



\subsection{Experimental Results}%
\label{sub:pl.results.experiment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.95\textwidth]{fig/phaselift_ex_overview}
  \caption{%
     \label{fig:experimental.overview}%
     Comparing reconstructions from experimental data for different target transfer matrices and sampling schemes.
     For each matrix and sampling scheme, we subsample $m = 5n$ preparation vectors and the corresponding measured intensities from the experimental data 100 times.
     To estimate the quality of the reconstruction, we plot the discrepancy between the PhaseLift reconstruction and an alternative method.
     The diamonds indicate the median and the colored area sketches the distribution of this reconstruction discrepancy.
     In the left picture, the reference is obtained through a HOM-dip reconstruction as discussed in the appendix.
     However, since this technique is too costly for larger dimensions, the five dimensional reconstructions on the right are only compared in magnitude to a reference from single photon data, which is observation to all phase information.
     Since for $n=2$ there are only six distinct RECR vectors up to a global phase, only the median is shown in these cases.
     For more details on the data analysis see the supplemental material.
  }
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
  \centering
  \includegraphics[width=\columnwidth]{fig/phaselift_ex_targetref}
  \caption{%
     \label{fig:experimental.targetref}%
     Same as \cref{fig:experimental.overview}, but the reconstructions are compared to the theoretical target unitaries.
     \quotes{HOM-dip} refers to the reconstructions used as references in \cref{fig:experimental.overview}.
     We do not show the results for the 5 dimensional unitaries since the corresponding HOM-dip reconstructions were too costly to take.
  }
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
  \centering
  \includegraphics[width=0.95\columnwidth]{fig/phaselift_ex_details}
  \caption{%
   \label{fig:pl.experimental.details}%
   Reconstruction errors for a random $5 \times 5$ matrix from experimental data.
   For each picture, we plot the mean (solid) as well as the $0.025$ and $0.975$ quartiles over 100 samples.
   In the left picture, each sample consists of a recovery from $m$ preparation vectors and the corresponding photon counts measured over $t = 30\,\mathrm{s}$
   In the right picture we fix a randomly selected set of $m=20$ preparation vectors and run the recovery with the photon counts from $t$ randomly selected time bins, each of which is one second long.
   The constant shift $E_0$ in the right picture is equal to the mean error for $t=29$ and it serves the purpose to equalize any differences between the Gaussian and the RECR reconstructions due to the choice of preparation vectors.
  }
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To demonstrate its practical utility, we use the PhaseLift protocol to perform experimental reconstruction on the reconfigurable integrated photonic circuit introduced in \cref{sec:pl.optics}
In \cref{fig:experimental.overview}, we show the reconstruction error of the PhaseLift approach.
Since our aim is to benchmark the performance of the characterization technique, and not the performance of the chip itself, we compare to  other reconstructions obtained through established but more costly techniques:
For the smaller transfer matrices of dimension two and three, we perform a complete HOM-dip-reconstruction based on two photon interference as described in \cref{sub:pl.hom_dip}.
However, since this is infeasibly costly for the five-dimensional transfer matrices, we only compare these to single-photon reconstructions of the absolute values of the transfer matrix components.
For more details, see \cref{sec:pl.experimental_details}.

The number of input vectors used in each PhaseLift reconstruction is $m = 5n$.
This slight overhead compared to the conjectured and numerically observed phase transition in \cref{fig:pl.simplot} is used to counteract systematic errors in the preparation of the input vectors.
To be more precise, we conjecture that the main source of error in this experiment is due comparatively large errors in preparation of certain input states.
By increasing $m$ slightly, these effects average out and provide in total a smaller reconstruction error.

We see that the PhaseLift reconstructions and the references agree well for most settings in \cref{fig:experimental.overview}
Even without exploiting the possible advantages of the RECR ensemble due to a better calibration, it generally performs as well as the uniform ensemble.
Both display a similar behavior: for a fixed number of modes, the deviations are generally larger for the random unitaries compared to the more structured identity and Fourier transfer matrices.
The errors for the two-dimensional transfer matrices are slightly smaller than for the corresponding three-dimensional transfer matrices as expected from \cref{eq:pl.total_bound}.
Furthermore, the currently used sequential arrangement of the Mach-Zehnder interferometers in the preparation stage of the experiment leads to higher deviations of the actual prepared compared to the intended measurement vectors with an increase in $n$.
Of course, this leads to larger reconstruction errors as well -- possible solutions to this problem are discussed in the conclusions.
For the reference reconstruction, we also expect larger deviations with an increase in the size of the transfer matrix, since errors from reconstructing one row accumulate in the error for other rows as well.
Note that the errors for the five-dimensional transfer matrices are relatively small since they only take into account the absolute values of the components and neglect all phases.\\



In \cref{fig:experimental.targetref}, we directly compare the performances of the reconstruction protocols, namely of the PhaseLift reconstruction and the HOM-dip reconstruction.
In contrast to \cref{fig:experimental.overview}, we use the theoretical target unitary as a reference.
Generally, the errors of the PhaseLift reconstructions and the HOM-dip reconstructions are of the same order of magnitude.
This is despite the fact that the HOM-dip reconstruction is not just insensitive to the row phases, but also to the column phases.
Therefore, the reported errors for the HOM-dip reconstruction are minimized over both row- and column phases instead of just the row phases for the PhaseLift reconstruction.
The additional free parameters in the minimization may lead to overfitting, and hence, to an underestimation of the actual error of the HOM-dip reconstruction.\\


Finally, we study the influence of varying the number of measurements and the statistical error on each measurement in \cref{fig:pl.experimental.details}.
In the left picture, we randomly select $m$ measurements from the existing data for 100 times and plot the mean as well as the spread of the error.
Except for a larger error for very few measurements, the RECR ensemble performs equally well as the Gaussian ensemble.
We see that the error saturates at a non-zero value, which might be due to systematic or statistical errors.
Also, note that the reconstruction error already saturates around $m = 4n = 20$ measurements.

To further investigate the source of the reconstruction error, we vary the statistical error of the measurements $\epsilon\ind{l}$ in the right picture by changing the number of photons used to estimate $I_j(\alpha\ind{l})$ in terms of \cref{eq:pl.experiment.probabilities}.
Since the experimental data consists of photon counts per one second time bin, we can vary the number of total photons by randomly selecting $t$ time bins per reconstruction.
In the right hand picture of \cref{fig:pl.experimental.details}, we fixed a random subset of 20 measurements and observe the reconstruction error as a function of $t$.
Furthermore, we normalized the reconstruction error with a constant offset $E_\infty$ for each measurement scheme to cancel out reconstruction errors due to the different choice of measurement vectors.
As we can see in the left image, the spread of the reconstruction error for $m=20$ is still on the order of $\pm 0.1$, and therefore, dominating the smaller error due to statistical fluctuations.
We chose $E_\infty$ such that the mean error for the largest value of $t$ is 0.
First, we see that the reconstruction error due to statistical uncertainty is small compared to the total error even for very small values of $t$.
This is due to the large counting rates of modern single photon experiments, which is $\Gamma \approx 4600\,\mathrm{s}^{-1}$ in this case.
Therefore, the main source of error in this experiment is due to systematic errors such as an relatively inaccurate preparation of the vectors $\alpha\ind{l}$.
We already mentioned that this can be rectified by better calibrating the preparation stage, which is much easier to perform for the RECR ensemble than for the uniform ensemble.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion and Outlook}

\todo{Fix this}
In this work, we introduce a solution to the problem of characterizing linear optical devices based on recent advances in phase retrieval and low-rank matrix recovery.
The PhaseLift reconstruction outlined in \cref{prot:pl.detailed_reconstruction} can be used to reconstruct any transfer matrix using only intensity measurements and classical states of light as inputs, which are chosen at random from an appropriate ensemble.
Not only do the number of measurements required for this approach scale linearly in the number of modes of the device, the theory behind it also provides stringent error bars for the reconstruction.
As the major contribution of this work, we proof recovery guarantees for the RECR ensemble, which is especially suited for the application in linear optical devices.
We also report on a successful experimental implementation of the PhaseLift characterisation protocol based on a universally reconfigurable six waveguide device.
The results from this experiment show that although the experimental effort of our approach is much lower, it provides a similar performance compared to other characterisation techniques.

Another possible extension of the experimental work is the development of a dedicated state-preparation circuit.
In the current architecture, possible errors in the preparation state for the coherent state inputs $\ket{\alpha}$ add up linearly due to the serial wiring in the preparation stage.
With a tree-like arrangement of the directional couplers, we might be able to also benefit from the fact that the RECR scheme allows only for two possible values for the intensity of each input mode $\abs{\alpha_i}$.
