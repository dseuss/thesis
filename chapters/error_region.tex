 % -*- root: ../thesis.tex -*-
\chapter{Uncertainty Quantification for quantum state estimation}
\label{chap:error}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction to Statistics}
\label{sec:error.intro}

% two main flavours of statistics...
% task of parameter estimation, model
% parametric model: state space \Omega
However, even if our model describes the data perfectly we cannot exactly recover this value from a finite amount of data due to statistical fluctuations.
The concept of error bars, or more generally error regions, allows for quantifying the uncertainty of a given estimate. 

% uncertatiny quantification, problem with point estimators

\subsection{Frequentist Statistics}
\label{sub:intro.frequentist}

In the frequentist (or orthodox) framework, the probability of an outcome of a random experiment is defined in terms of its relative frequency of occurrence when the number of repetitions goes to infinity~\cite{Keynes_2007_Treatise,Kiefer_2012_Introduction}.
More precisely, denote the number of repetitions of an experiment by $T$ and the number of times the event under consideration $x$ occurred by $n_T$. 
Then, a Frequentist interprets the probability $\Prob(x)$ as the statement that if $T \to \infty$, $\frac{n_T}{T} \to \Prob(x)$.

% probabliities = hypothetical frequencies, not repeatable
For the task of parameter estimation, we assume that the observed data are generated from the parametric model with \quotes{true} parameter $\theta \in \Omega$, which is unknown.
From a finite number of observations $X_1, \ldots X_N$, we must construct an estimate for $\theta$ that is close to the true value in some sense.
The function $\hat\theta$ that maps observations to such an estimate is called a point estimator.
The quality of an estimator is measured by its risk function.
A risk function commonly used for continuous parameter spaces is the mean square error
\[
  \label{eq:frequentist.mean_square}
  \mathcal{L}_{\hat\theta}(\theta) := \Exp_\theta \left( \Norm{\theta - \hat\theta(X_1, \ldots, X_N)}^2 \right).
\]
Note that \cref{eq:frequentist.mean_square} -- and therefore the performance of a given estimator -- still depends on the unknown true value $\theta$.
Strategies to make statements independent of the true value include 
\begin{definition}
  \label{def:frequentist.optimality_conditions}
  \begin{itemize}
    \item $\hat\theta$ is called a \emph{uniformly best} estimator, if for all other estimators $\hat\theta'$ and all values of the true parameter $\theta \in \Omega$
    \[
      \mathcal{L}_{\hat\theta}(\theta) \le \mathcal{L}_{\hat\theta'}(\theta).
    \]

    \item $\hat\theta$ is called \emph{minimax}, if for all other estimators $\hat\theta'$
    \[
      \sup_{\theta\in\Omega} \mathcal{L}_{\hat\theta}(\theta) \le \sup_{\theta\in\Omega} \mathcal{L}_{\hat\theta'}(\theta).
    \]

    \item $\hat\theta$ is called \emph{best on average} w.r.t.\ a distribution of the true value $\theta \sim \Theta$ if for all other estimators $\hat\theta'$
    \[
      \Exp_{\theta \sim \Theta} \mathcal{L}_{\hat\theta}(\theta) \le  \Exp_{\theta \sim \Theta} \mathcal{L}_{\hat\theta'}(\theta).
    \]

    \item $\hat\theta$ is called \emph{admissible} if there is no other estimator $\hat\theta'$ such that
    \[
      \forall\theta \in \Omega\colon \mathcal{L}_{\hat\theta'}(\theta) \le \mathcal{L}_{\hat\theta}(\theta) 
      \quad\mbox{ and }\quad
      \exists\theta \in \Omega\colon \mathcal{L}_{\hat\theta'}(\theta) < \mathcal{L}_{\hat\theta}(\theta) 
    \]
  \end{itemize}
\end{definition}
\todo{Citation}
\todo{Discussion of different properties}
\todo{Choice is arbitrary!}
\todo{How does this connect with definition of probability?}


% examples (also depends on notion of volume), admissability
However, as already mentioned in the introduction, point estimators cannot convey uncertainty in the estimate. 
For this purpose we need to introduce a precise notion of \quotes{error bars}, namely \emph{confidence regions}.
A confidence region $\CR \subset \Omega$ with coverage $\alpha \in [0,1]$ is a region estimator -- that is a function that maps observed data to a subset of the parameter space -- such that the true parameter is contained in $\CR$ with probability greater than $\alpha$
\[
  \label{eq:frequentist.coverage}
  \forall \theta\in\Omega \colon \Prob_\theta\left( \CR(X_1, \ldots, X_N) \ni \theta \right) \ge \alpha.
\]
Similar to point estimators, \cref{eq:frequentist.coverage} does not uniquely determine a confidence region construction.
Furthermore, additional constraints are necessary to exclude trivial constructions such as the following:
Take the region estimator, which is always equal to the full parameter space independent of the data $\CR(X_1, \ldots X_N) = \Omega$, then 
\[
  \Prob_\theta\left(  \CR(X_1, \ldots, X_N) \ni \theta  \right) = 1 \ge \alpha
\]
for all confidence levels $\alpha$.
Although, this construction trivially fulfils the coverage condition~\eqref{eq:frequentist.coverage}, it does not provide useful information on the uncertainty as it does not restrict the parameter space at all.
Therefore, we have to impose a notion of what constitutes a good confidence region.

Clearly, if we have two confidence regions $\CR_1$ and $\CR_2$ with the same confidence level $\alpha$ and $\CR_1 \subset \CR_2$, then $\CR_1$ is more informative.
More generally, smaller regions should be preferred since they convey more confidence in the estimate and exclude more alternatives.
Therefore, measures of size such as (expected) volume or diameter are commonly used as risk functions for region estimators.
This leads to similar definitions as in \cref{def:frequentist.optimality_conditions} for confidence regions with the additional constraint that \cref{eq:frequentist.coverage} is fulfilled.

\todo{Today: Asymptotic optimal, adaptive, ...}