 % -*- root: ../thesis.tex -*-

\chapter{Uncertainty quantification for quantum state estimation}%
\label{chap:error}


Due to the intrinsic randomness of quantum mechanics, any information we obtain about a quantum system through measurements is subject to statistical uncertainty.
Consequently, one should not only report the final result of an experiment, but also answer the question whether this result is statistically reliable or simply arose due to chance.
This motivated the recent development of techniques for uncertainty quantification in quantum state estimation (QSE)~\cite{Kohout_2012_Robust,Ferrie_2014_High,Shang_2013_Optimal,Faist_2015_Practical,Christandl_2012_Reliable,Audenaert_2009_Quantum,Audenaert_2008_Asymptotic}.
Despite the large body of work concerned with this problem, none of these constructions are known to be both optimal and computationally feasible.

In this chapter, we investigate the question whether the lack of an efficient algorithm for computing optimal error regions in QSE simply is due to a lack of imagination or if there are fundamental restrictions that prevent the existence of such an algorithm.
We provide evidence that the latter case is true:
Based on the generally accepted conjecture that $\P \neq \NP$ in computational complexity we show that no such algorithm can exist.
More specifically, we show that the computational intractability does not simply arise due to the  general difficulties of statistics in high dimensions.
Instead, by considering models which render the unconstrained problem tractable, we show that the computational intractability is caused by the \emph{quantum mechanical shape constraints}:
For $\varrho$ to constitute a valid quantum state, it has to be a Hermitian, positive semi-definite (psd) matrix with unit trace.
The Hermiticity and trace constraints are linear and easily satisfiable by means of a suitable linear parametrization.
In contrast, the psd constraint is non-linear, and hence, more problematic to satisfy.

The motivation for studying uncertainty quantification under quantum constraints is that these constraints can lead to a significant reduction in uncertainty.
This is particularly evident if the true state is close to the boundary of state space, e.g.\ in the case of almost pure states, which are of interest in quantum information processing experiments.
In this case, it is plausible that a large fraction of possible estimates that seem compatible with the observations can be discarded, as they lie outside of state space.

Indeed, it is known that taking the quantum constraints into account can result in a dramatic -- even unbounded -- reduction in uncertainty.
Prime examples are results that employ positivity to show that even \emph{informationally incomplete} measurements can be used to identify a state with arbitrarily small error~\cite{Cramer_2010_Efficient,Gross_2010_Quantum,Gross_2011_Recovering,Flammia_2012_Quantum,Nickl_2013_Confidence,Kalev_2015_Quantum}.
More precisely, these papers describe ways to rigorously bound the size of a confidence region for the quantum state based only on the observed data and on the knowledge that the data comes from measurements on a valid quantum state.
While these uncertainty bounds can always be trusted without further assumptions, only in very particular situations have they been proven to actually become small.
These situations include the cases where the true state is of low rank~\cite{Flammia_2012_Quantum,Nickl_2013_Confidence}, or admits an economical description as a matrix-product state~\cite{Cramer_2010_Efficient}.
It stands to reason that there are further cases -- not yet identified -- for which the size of an error region can be substantially reduced simply by taking into account the quantum shape constraints.\\


Throughout this work, we consider the task of non-adaptive QSE, where a fixed set of measurements specified by the choice of positive operator valued measure (POVM) is performed on independent copies of the system.
For a given POVM, the measurement outcomes of this setup can be described in terms of a generalized linear model (GLM) with parameter $\varrho$ -- the quantum mechanical state or density matrix of the system.
The well-established theory of GLMs then provides methods for inferring $\varrho$~\cite{Mccullagh_1989_Generalized}.

However, what sets the task of QSE apart from inference in GLMs in general are the additional quantum mechanical shape constraints.
Nevertheless, there are estimators for $\varrho$ that on the one hand always yield a psd density matrix, and on the other hand, are well-understood theoretically with near-optimal performance and scalable to intermediate sized quantum experiments~\cite{Paris_2004_Quantum}.
Unfortunately, the same cannot be said for error bars for $\varrho$.\\


This chapter is structured as follows:
We introduce the necessary concepts from statistical inference and computational complexity in \cref{sec:error.stat} and~\ref{sec:error.complexity}, respectively.
Then, in \cref{sec:error.intro} we provide more details on QSE and the
One of the main results of this work concerning the proof of computational intractability of frequentist confidence region is given in \cref{sec:error.ortho}.
The Bayesian counterpart for credible regions is the topic of \cref{sec:error.bayesian}.
Finally, we conclude this chapter with remarks on limitations of the hardness results as well as possible future work in \cref{sec:error.outlook}.

\subsection*{Relevant publications}
\begin{itemize}
  \item D.\ Suess, ≈Å.\ Rudnicki, T.\ O.\ Maciel, D.\ Gross: \textit{Error regions in quantum state tomography: computational complexity caused by geometry of quantum states}, New J.\ Phys.\ 19 093013 (2017)
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction to Statistical Inference}%
\label{sec:error.stat}

The objective of statistical inference is to obtain information about the distribution of a random variable $X$ from observational data.
Here, we focus on the special case of inference in parametric models, which can be described as follows~\cite{Wasserman_2013_All}:
A \emph{parametric model} is a family of distributions ${\{\Prob_\theta\}}_{\theta \in \Omega}$ labeled by a finite number of parameters $\theta$, where $\Omega \subset \Reals^k$ is called the \emph{state space} of the model.
For simplicity, we only consider the scalar case $k=1$ for now.
Then, any function $\estim\theta$ mapping observations to the space $\Reals$ containing the parameter space is called a \emph{point estimator} for the parameter $\theta$.
In general, we do not require $\estim\theta$ to map to the state space $\Omega$ as this restriction would preclude many relevant estimators such as the linear inversion estimator introduced in \cref{sub:ortho.linear_inversion}.
However, since we are interested in learning about the distribution of $X$, not every estimator is equally useful.
Indeed, the goal should be to find an estimator, which yields the parameter value that describes the observed data best.
What we mean by \quotes{best} in this context not only depends on the specific model and what the estimate is supposed to be used for, but also on the fundamental interpretation of probability.
Broadly speaking, there are two different interpretations of probability, namely the frequentist (or orthodox) interpretation and the Bayesian interpretation~\cite{Hajek_2012_Interpretations,Caves_2000_Probabilities}, which lead to distinct schools of inference~\cite{Kiefer_2012_Introduction,Bolstad_2007_Introduction,Wasserman_2013_All}.
Although the two approaches yield the same results for very simple models or -- under mild regularity assumptions -- in the limit of many measurements, they generally differ and in some cases even yield contradictory results~\cite[Sec. 11.9]{Wasserman_2013_All}.

So far we have only discussed point estimators, which yield a single value for the parameters.
However, even if our model describes the data perfectly for some choice of $\theta$, we cannot exactly recover this value from a finite amount of data due to statistical fluctuations in general.
The concept of error bars, or more specifically error regions, allows for quantifying the uncertainty of a given estimate.
In \cref{sub:stat.frequentist}, we introduce the basic concepts of frequentist inference and the corresponding notion of \emph{confidence regions}.
\Cref{sub:stat.bayesian} is concerned with Bayesian inference and the corresponding notion of error regions, namely \emph{credible regions}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Frequentist Statistics}%
\label{sub:stat.frequentist}

In the frequentist framework, the probability of an outcome of a random experiment is defined in terms of its relative frequency of occurrence when the number of repetitions goes to infinity~\cite{Keynes_2007_Treatise,Kiefer_2012_Introduction}.
More precisely, denote the number of repetitions of an experiment by $N$ and the number of times the event under consideration $x$ occurred by $n_N$.
Then, a frequentist interprets the probability $\Prob(x)$ as the statement that $\frac{n_N}{N} \to \Prob(x)$ as $N \to \infty$.

For the task of parameter estimation, we assume that the model is well-specified, i.e.\ that the observed data are generated from the parametric model with the fixed \quotes{true} parameter $\theta_0 \in \Omega$, which is unknown.
From a finite number of observations $X_1, \ldots X_N$, we must construct an estimate $\hat\theta(\vec X)$ for $\theta_0$.
Although there are some intuitive approaches to this problem such as the method of moments~\cite[Sec.\ 9.2]{Wasserman_2013_All}, the \emph{principle of maximum likelihood} is often employed to construct an estimator with good frequentist properties.
First, let us introduce the \emph{likelihood function} of the model
\[
  \Loss(\theta; \vec X) = \Prob_\theta(\vec X) = \prod_{i=1}^N \Prob_\theta(X_i),
  \label{eq:stat.frequentist.likelihood_function}
\]
where we have assumed independence of the samples for the second equality.
The \emph{maximum likelihood estimator} (\apprv{MLE}) is then defined by\footnote{%
  Note that we have constrained the values of the MLE to the state space $\Omega$ of the model.
  Otherwise, the right hand side of \cref{eq:stat.frequentist.mle} might not be well defined.
}
\[
  \estim\theta_\mathrm{MLE}(\vec x) = \operatorname{argmax}_{\theta \in \Omega} \Loss(\theta; \vec x).
  \label{eq:stat.frequentist.mle}
\]
The justification for using~\eqref{eq:stat.frequentist.mle} is that under mild conditions on the model, the MLE posses many appealing properties~\cite[Sec.\ 9.4]{Wasserman_2013_All} such as consistency and efficiency:
Consistency means that the MLE converges to the true value $\theta_0$ in probability as $N \to \infty$ and efficiency roughly means that among all well behaved estimators, the MLE has the smallest variance in the large sample limit.

A more flexible approach to the problem of how to single out a \quotes{good} estimator is formalized in statistical decision theory~\cite{Casella_2008_Statistical,Lehmann_1998_Theory}.
For this purpose, we need to introduce a \emph{loss function}
\[
  \label{eq:stat.frequentist.loss_function}
  \Loss\colon \Omega \times \Reals \to \Reals,  (\theta_0, \estim\theta) \mapsto \Loss(\theta_0, \estim\theta),
\]
which measures the discrepancy between the true value $\theta_0$ and an estimate $\estim\theta(\vec X)$.
Keep in mind that since the $X_i$ are random variables, so is $\estim\theta(\vec X)$ and its loss.
In order to asses the estimator $\hat\theta$, that is the function mapping data to an estimate, we evaluate the average loss or \emph{risk function}
\[
  \label{eq:stat.frequentist.risk}
  \Risk(\theta_0, \hat\theta) = \Exp_{\theta_0}(\Loss(\theta_0, \hat\theta(\vec X)))
  = \int \Loss(\theta_0, \hat\theta(\vec x)) \, \Prob_{\theta_0}(\vec x) \, \dd x.
\]
Then, the problem of finding a good estimator reduces to the problem of finding an estimator that yields small values for \cref{eq:stat.frequentist.risk}.
However, note that the risk function~\eqref{eq:stat.frequentist.risk} for a given estimator still depends on the unknown true value.
To obtain a single-number summary of the performance of an estimator for all possible values of $\theta_0$, there are different strategies such as maximizing or averaging the risk function over all $\theta_0$~\cite[Sec.\ 12.2]{Wasserman_2013_All}.
By minimizing these risks over all possible estimators, we try to determine a single estimator that performs best in the worst case or on average, respectively.
However, in the following we introduce a less ambitious notion of optimality, namely \emph{admissibility}.
\begin{definition}{\cite[Def. 12.17]{Wasserman_2013_All}}%
  \label{def:stat.frequentist.admissability_point_estimators}
  A point estimator $\estim\theta$ is \emph{inadmissible} if there exists another estimator $\estim\theta'$ such that
  \begin{align*}
    \Risk(\theta_0, \estim\theta') \le \Risk(\theta_0, \estim\theta) &\quad\mbox{for all } \theta_0 \in \Omega \\
    \Risk(\theta_0, \estim\theta') <   \Risk(\theta_0, \estim\theta) &\quad\mbox{for at least one } \theta_0 \in \Omega.
  \end{align*}
  Otherwise, we call $\estim\theta$ \emph{admissible}.
\end{definition}
In words, $\estim\theta$ is admissible if there is no other estimator $\estim\theta'$ that performs at least as good as $\estim\theta$ and strictly better for at least one value of the true value $\theta_0$.

The choice of loss function generally depends on the problem at hand and determines the properties of the corresponding estimator.
Some examples for commonly used loss functions include the $0/1$-loss for discrete parameter models
\[
  \Loss(\theta_0, \theta) =
  \begin{cases}
    1 & \theta_0 = \theta \\
    0 & \mathrm{otherwise}
  \end{cases},
  \label{eq:stat.frequentist.01loss}
\]
and the mean squared error (MSE)
\[
  \Loss(\theta_0, \theta) = {\left( \theta_0 - \theta \right)}^2,
  \label{eq:stat.frequentist.l2loss}
\]
which is often used for continuous parameter models, e.g.\ $\Omega = \Reals$.
The use of the MSE loss is often motivated by the fact that it gives the same results as the principle of maximum likelihood for many problems.
As an example, consider the task of estimating the mean from iid normal random variables $X\ind{k} \sim \Normal(\theta_0, \sigma)$ with $\sigma \in \Reals_+$ known.
In this case, a straightforward estimator for $\theta_0$ is the \emph{empirical mean}
\[
  \label{eq:stat.frequentist.empirical_mean}
  \bar X := \frac{1}{m} \sum_{i = 1}^m X\ind{k},
\]
which is admissible with respect to MSE~\cite[Thm.\ 12.20]{Wasserman_2013_All}.
Furthermore, $\bar X$ is also the MLE as a transformation to log-likelihood shows:
\[
  \label{eq:stat.frequentist.mle_derivation}
  \begin{split}
    \argmax_\theta \Loss(\theta; x\ind{1}, \ldots, x\ind{m})
    &= \argmax_\theta \sum_{k=1}^m \log \Prob_\theta(X\ind{k}=x\ind{k}) \\
    &= \argmin_\theta \sum_{k=1}^m \frac{1}{2\sigma^2} {\left( x\ind{k} - \theta \right)}^2,
  \end{split}
\]
where we have used that the $X\ind{k}$ are independent and that the logarithm is monotonic increasing.
Furthermore, we discarded all contributions independent of $\theta$ is the second line.
Finally, note that the right hand side of the last equation is minimized by the choice $\theta = \bar{x}$, which proofs the claim.

Now we consider a multivariate generalization of the above problem, that is the task of estimating $\vec\theta_0 \in \Reals^d$ from a linear Gaussian model $\vec X \sim \Normal(\vec\theta_0, \1)$ with unit covariance matrix.
Suppose we only have a single observation $\vec X$, i.e.\ $m=1$.
Since all the components $X_i$ are independent, one expects that a separate estimation of the components via $\bar{\vec X} = \vec X$ constitutes a \quotes{good estimator}.
Indeed, the same computation as in \cref{eq:stat.frequentist.mle_derivation} shows that the empirical mean is the MLE for this model as well.
However, Stein shocked the statistical community when he proved that for $d \ge 3$, this estimator is inadmissible~\cite{Stein_1956_Inadmissibility}.
It can be shown that the \emph{James-Stein} estimator
\[
  \estim{\vec\theta}_{\mathrm{S}} := \max\left\{0, \left( 1 - \frac{d - 2}{\norm{\vec X}_2} \right) \right\} \vec X
  \label{eq:stat.frequentist.stein_estimator}
\]
has smaller MSE risk than the empirical mean for all values of $\vec \theta_0$~\cite{Stein_1956_Inadmissibility,Lehmann_1998_Theory}.
It is often referred to as a \emph{shrinkage estimator} because it shrinks the empirical mean estimate $\vec X$ towards 0.
Note however, that the choice of the origin as the fix point of the shrinkage operation is arbitrary -- the James-Stein estimator outperforms the empirical mean estimator for any choice of fix point.
Interestingly, \cref{eq:stat.frequentist.stein_estimator} shows that by taking into account all the components $X_i$, we can improve the MSE of the mean-estimator even if the $X_i$ are independent.
However, this only applies to the \emph{simultaneous} MSE error of all the components of $\vec \theta_0$.
The James-Stein estimator~\eqref{eq:stat.frequentist.stein_estimator} cannot be used to improve the MSE of a single component.
\Cref{eq:stat.frequentist.stein_estimator} can also be generalized to the case of more than one observation, i.e.\ $m > 1$.
Finally, note that the James-Stein estimator is also not admissible:
More elaborate shrinkage estimators are known to outperform~\eqref{eq:stat.frequentist.stein_estimator} w.r.t.\ the MSE.
Even worse, to the best of the author's knowledge, no admissible construction exists for estimating the mean of a $d$-variate Gaussian w.r.t.\ MSE for $d \ge 3$.\\


As already mentioned in the introduction, point estimators cannot convey uncertainty in the estimate.
For this purpose, we need to introduce a precise notion of \quotes{error bars}, namely \emph{confidence regions} in the framework of frequentist statistics.
\begin{definition}\label{def:stat.frequentist.cr}
  Consider a statistical model with $k$ parameters, that is $\Omega \subset \Reals^k$.
  A confidence region $\CR$ with coverage $\alpha \in [0,1]$ is a region estimator -- that is a function that maps observed data $x$ to a subset $\CR(x) \subset \Reals^k$ of the space containing the state space -- such that the true parameter is contained in $\CR$ with probability greater than $\alpha$:
  \[
    \label{eq:stat.frequentist.coverage}
    \forall \theta_0\in\Omega \colon \Prob_{\theta_0}\left( \CR(X) \ni \theta_0 \right) \ge 1 - \alpha.
  \]
\end{definition}
Note that the coverage condition~\eqref{eq:stat.frequentist.coverage} is not a probabilistic statement about the true parameter $\theta_0$ for fixed observed data $x$.
Instead, \cref{def:stat.frequentist.cr} should be interpreted as a statistical guarantee for the region estimator $\CR$:
Say we repeat an experiment $m$ times and obtain data $x\ind{1}$, \ldots $x\ind{M}$ from a distribution with true parameter $\theta_0$.
Then, in the limit $m \to \infty$ at least a fraction of $1-\alpha$ of the regions $\CR(x\ind{1})$, \ldots, $\CR(x\ind{m})$ contain the true parameter $\theta_0$.
In other words, the probabilistic statement in~\eqref{eq:stat.frequentist.coverage} refers to the random variable $\CR(X)$ for fixed $\theta_0$.

Similarly to point estimators, \cref{eq:stat.frequentist.coverage} does not uniquely determine a confidence region construction.
Additional constraints are necessary to exclude trivial constructions such as the following:
Take the region estimator that is always equal to the full parameter space independent of the data $\CR(X) = \Omega$, then
\[
  \Prob_\theta\left(  \CR(X) \ni \theta_0  \right) = 1 \ge 1 - \alpha
\]
for all confidence levels $\alpha$.
Although, this construction trivially fulfils the coverage condition~\eqref{eq:stat.frequentist.coverage}, it does not provide useful information on the uncertainty as it does not restrict the parameter space at all.
Therefore, we have to impose a notion of what constitutes a good confidence region by introducing a loss function.
Clearly, if we have two confidence regions $\CR_1$ and $\CR_2$ with the same confidence level and $\CR_1 \truesubset \CR_2$, then $\CR_1$ is more informative.
More generally, smaller regions should be preferred since they convey more confidence in the estimate and exclude more alternatives.
Therefore, measures of size such as (expected) volume or diameter are commonly used as loss functions for region estimators.
We now introduce a notion of optimality similar to~\cref{def:stat.frequentist.admissability_point_estimators}.
\begin{definition}{\cite[Def. 2.2]{Joshi_1969_Admissibility}}%
  \label{def:stat.frequentist.admissible}
  A confidence region $\CR$ for the parameter estimation of $\theta_0 \in \Omega$ is called (weakly) \emph{admissible} if there is no other confidence region $\CR'$ that fulfils
  \begin{enumerate}
    \item(equal or smaller volume) $\Vol(\CR'(x)) \le \Vol(\CR(x))$ for almost all observations $x$.
    \item(same or better coverage) $\Prob_{\theta_0}(\CR' \owns \theta_0) \ge \Prob_{\theta_0}(\CR \owns \theta_0)$ for all $\theta_0 \in \Omega$.
    \item(strictly better) strict inequality holds for one $\theta_0 \in \Omega$ in (ii) or on a set of positive measure in (i).
  \end{enumerate}
\end{definition}
This notion of optimality uses \quotes{pointwise} volume as a risk function instead of the average volume.
The conditions in \cref{def:stat.frequentist.admissible} are stated only for \quotes{almost all} $x$ since one can always modify the region estimators on sets of measure zero without changing their statistical performance.
In \cref{sub:ortho.optimal} we show that this notion of admissibility is especially suitable for studying constrained inference problems.
% A different approach more in line with the previous discussion is to state condition (i) in terms of the expected volume with the average taken over the data $X$ for a fixed true parameter $\theta_0$~\cite[Def.~7.1]{Joshi_1969_Admissibility}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bayesian Statistics}%
\label{sub:stat.bayesian}


Let us now introduce the Bayesian point of view on statistical inference:
In the Bayesian interpretation, probabilities do not describe frequencies in the limit of infinitely many repetitions, but they reflect subjective degrees of belief.
Put differently, in the Bayesian framework randomness reflects one's ignorance or lack of knowledge of the value of a parameter.
In contrast to frequentist inference, this enables us to make probabilistic statements about the values of parameters even for a single fixed set of observations.
Generally, Bayesian inference for a parametric model is carried out in the following way:
First, we choose a \emph{prior distribution} $\Prob(\theta)$, which expresses our belief about the parameter $\theta$ before taking any data into account.
Given an observation $X$, the distribution of $\theta$ is updated according to Bayes' rule~\cite{Bolstad_2007_Introduction,Gelman_2014_Bayesian}
\[
  \label{eq:stat.bayesian.bayes_rule}
  \Prob(\theta | X) = \frac{\Prob(X | \theta) \Prob(\theta)}{\Prob(X)}.
\]
Here, $\Prob(X|\theta)$ is the \emph{likelihood function} of the model analogous to~\eqref{eq:stat.frequentist.likelihood_function} and $\Prob(\theta | X)$ is the \emph{posterior distribution} -- or short posterior -- of $\theta$.
Of course, the update in \cref{eq:stat.bayesian.bayes_rule} is not limited to a single observation and can be iterated for independent data.

Computing the Bayesian update~\eqref{eq:stat.bayesian.bayes_rule} analytically is possible only in a few rare cases:
If for a given likelihood function, the prior and the posterior are in the same family of distributions, the prior is called a \emph{conjugate prior} for the likelihood function.
For example, consider a Gaussian random variable $X \sim \Normal(\theta, \tau^2)$ with known variance $\tau^2$.
If we assume a Gaussian prior $\theta \sim \Normal(\mu, \sigma^2)$, we have~\cite[Eq.\ (2.10)]{Gelman_2014_Bayesian}
\[
  \theta \vert X=x \sim \Normal (\mu', \sigma'^2)
\]
with
\[
  \label{eq:stat.bayesian.kalman}
  \mu' = \frac{ \frac{\mu}{\sigma^2} + \frac{x}{\tau^2} }{ \frac{1}{\sigma^2} + \frac{1}{\tau^2} },
  \quad\mbox{and}\quad
  \sigma' = \left( \frac{1}{\sigma^2} + \frac{1}{\tau^2} \right)^{- \tfrac{1}{2}}.
\]
In other words, an update of a Gaussian prior with a Gaussian likelihood function yields a Gaussian posterior distribution.
Although there are other well-known conjugate priors with explicit formulas for the parameter update, in practice the Bayesian update~\eqref{eq:stat.bayesian.bayes_rule} can only be approximated numerically.
Commonly used methods include sampling techniques such as Markov Chain Monte Carlo and Sequential Monte Carlo~\cite{Gelman_2014_Bayesian} as well as variational Bayes~\cite{Fox_2012_Tutorial}.\\


Note that the posterior distribution encodes all the information on $\theta$ we have.
To summarize important features of the posterior, we introduce point and region estimators similar to the frequentist case:
One commonly used estimator is the Bayesian mean estimator (BME) given by
\[
  \hat\theta_\mathrm{BME}(X) = \int \theta \, \Prob(\theta | X) \, \dd\theta.
\]
A justification for the BME is that it minimizes (under normal circumstances~\cite{Lehmann_1998_Theory,Banerjee_2005_On}) the expected risk
\(
  \Exp \left( \Risk(\theta, \hat\theta(X)) \right),
\)
provided the corresponding loss function $\Loss$ is a \emph{Bregman divergence}.
Note that the expectation is taken over $\theta$ w.r.t.\ the posterior with observation $X$.
Another example of a point estimator is the \emph{maximum a posteriori} (MAP) estimator.
As the name suggests, the MAP estimator is obtained by maximizing the posterior~\eqref{eq:stat.bayesian.bayes_rule}.
Since this does not require the expensive computation of the denominator in \cref{eq:stat.bayesian.bayes_rule} and -- at least local -- minima of the posterior can be found efficiently via gradient descent, the MAP estimator is often used for high dimensional problems in machine learning~\cite{Murphy_2012_Machine} that otherwise would be intractable.

Let us now introduce the appropriate concept of error regions in the Bayesian framework.
\begin{definition}%
  \label{def:stat.bayesian.cr}
  A \emph{credible region}\footnote{%
    We use the same letter for confidence and credible regions when the meaning is clear from the context.
  }
  $\CR$ with credibility $1-\alpha$ is a subset of the parameter space $\Omega$ containing at least mass $1-\alpha$ of the posterior
  \[
    \label{eq:stat.bayesian.credibility}
    \Prob(\theta \in \CR | X_1, \ldots, X_N) \ge 1-\alpha.
  \]
\end{definition}
Notice the different notion of randomness compared to \cref{eq:stat.frequentist.coverage}:
Confidence regions are random variables due to their dependence on the data and \cref{eq:stat.frequentist.coverage} demands that the true value is contained in the confidence region with high probability.
Here, \quotes{probability} refers to (possibly hypothetical) repetitions of the experiment -- no statement can be made for a single run of an experiment with given outcomes.
In contrast, the definition of credible regions~\eqref{eq:stat.bayesian.credibility} only refers to probability w.r.t.\ the posterior, and therefore, is well defined even for a single run of the experiment.

In order to single out \quotes{good} credible regions, we need to introduce a notion of optimality.
As argued in \cref{sub:stat.frequentist}, smaller error regions are generally more informative.
Therefore, good credible regions are minimal-volume credible regions (MVCRs) or credible regions with the smallest diameter.
Although in the following we deal with MVCRs w.r.t.\ a geometric notion of volume, some authors have proposed to measure the volume of a region by its prior probability~\cite{Evans_2006_Optimally,Shang_2013_Optimal}.
In case the posterior has probability density $\Pdf$ w.r.t.\ the volume measure under consideration, the MVCR is given by highest posterior sets~\cite{Ferrie_2014_High}
\[
  \label{eq:stat.bayesian.mvcr}
  \CR = \{ \theta \in \Omega\colon \Pdf(\theta) \ge \lambda \}.
\]
The constant $\lambda$ is determined by the saturation of the credibility level condition~\eqref{eq:stat.bayesian.credibility}.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction to Computational Complexity Theory}%
\label{sec:error.complexity}

The objective of computational complexity theory is to classify computational problems according to their inherent difficulty.
Broadly speaking, we quantify the difficulty by the runtime needed to solve the problem.
Other resources, which one may want to consider, are the amount of memory or communication between parties required.

At a first glance, an answer to the question whether a given problem can be solved efficiently might depend on what we consider as a \quotes{computer}:
Clearly, a modern high-performance cluster can solve problems in seconds, which would take a human equipped with pen and paper more than a lifetime to finish.
Maybe surprisingly, it turns out that a single, simple mathematical model -- the \emph{Turing machine} -- describes the capabilities and restrictions of almost all physical implementations of computation well enough for the purpose of complexity theory.
The only conceivable exception so far are quantum computers that exploit quantum mechanical phenomena.
Although so far no unconditional proof of an advantage of quantum computers exists, there is overwhelming evidence that they are able to solve problems efficiently for which there is no efficient classical algorithm.
Note that quantum computers only provide efficient algorithms for problems that are considered hard classically.
The class of uncomputable functions is the same for classical and quantum computers~\cite{Arora_2009_Computational}.




This section introduces the main concepts needed for the hardness results of \cref{sec:error.ortho} and \ref{sec:error.bayesian}.
Especially the definition of $\NP$-hard computational problems and polynomial-time reductions is crucial for the rest of this chapter.
For a more thorough treatment, we refer the reader to~\cite{Arora_2009_Computational,Garey_2002_Computers}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
  \centering
  \input{tikz/error_turingmachine.tex}
  \caption{%
    Illustration of a Turing machine with alphabet $\Gamma = \{0, 1, \blank\}$ and register $Q = \{q_0, q_h, q_1, \ldots, q_N\}$.
    }%
  \label{fig:complexity.turingmachine}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A Turing machine (TM) can be thought of as a simplified and idealized mathematical model of an electronic computer.
It is defined by a tuple $(\Gamma, Q, \delta)$ and figuratively speaking consists of the following parts as shown in \cref{fig:complexity.turingmachine}:
\begin{itemize}
  \item An infinite tape, that is a bi-directional line of cells that can take the values from a finite set $\Gamma$ called the \emph{alphabet}.
    $\Gamma$ must contain a designated symbol $\blank$ called \quotes{blank}.
  \item A register that can take on values from a finite set of states $Q$.
    $Q$ must contain the initial state $q_0$ and the halting state $q_h$.
  \item A transition function
    \[
      \delta\colon Q \times \Gamma \to Q \times \Gamma \times \{L, S, R\},
    \]
    which describes the \quotes{programming} of the TM.
\end{itemize}
The operation of a TM can be summarized as follows.
Initially, the reading head of the tape is over a certain cell and the register is in the initial state $q_0$.
Furthermore, we assume that only a finite number of tape cells have a value different from $\blank$ -- these are referred to as the input.
For one step of the computation, denote by $\gamma \in \Gamma$ the value of tape cell under the reading head and by $q \in Q$ the current value of the register.
The action of the TM is determined by the transition function
\[
  (q', \gamma', h) = \delta(q, \gamma).
\]
as follows:
The register is set to value $q'$, the tape head overwrites the cell with symbol $\gamma'$, and it moves depending on the value of $h$:
If $h = L$ or $h = R$, it moves one cell to the left or right, respectively and if $h = S$ it stays in the current position.
This cycle repeats until the register takes on the halting state $q_h$.
If the TM halts, the state of the tape with leading and trailing blanks removed is taken as its output.
Note that the definition of a TM given above is one of many; others may include additional scratch tapes or have tapes that only extend to infinity only in one direction~\cite{Arora_2009_Computational}.
However, they are all equivalent, i.e. the architecture given above can simulate all the other architectures with a small overhead and vice versa.
Furthermore, we can assume $\Gamma = \{0, 1, \blank\}$, that is, we use a binary encoding for all non-blank values of the alphabet.

We now formalize the notion of runtime of a TM and the complexity of a given problem.
Denote by
\[
  \BitStrings = \bigcup_{n \in \Naturals} \{0, 1\}^n
\]
the set of all finite bit strings.

\begin{definition}{\cite[Def.\ 1.3]{Arora_2009_Computational}}%
  \label{def:complexity.runtime}
  Let $f\colon \BitStrings \to \BitStrings$ and $T\colon \Naturals \to \Naturals$ be some functions.
  Furthermore, let $M$ be a Turing machine.
  We say $M$ \emph{computes} $f$ in $T(n)$-time if for every input $x \in \BitStrings$, whenever $M$ is initialized with input $x$, it halts with $f(x)$ written on its output tape in at most $T(\abs{x})$ steps.
  Here, $\abs{x}$ denotes the length of $x$.
\end{definition}

In words, \cref{def:complexity.runtime} gives a formal notion of the runtime of a TM for computing a function $f$ in terms of the number of elemental steps required.
But as stated in the introduction, we are more interested in the intrinsic complexity of computing the function $f$ instead of a specific implementation.
For this purpose, we introduce \emph{complexity classes}, which are sets of functions that can be computed within given resource bounds.
The most important examples of complexity classes pertain to boolean functions $f\colon \BitStrings \to \{0, 1\}$, which correspond to decision problems.
The corresponding set of \quotes{truthy} inputs $L = \{x \ in \BitStrings\colon f(x) = 1\}$ is referred to as a \emph{language}.
\begin{definition}%
  \label{def:complexity.p}
  Let $T\colon \Naturals \to \Naturals$ be some function.
  We say that a language $L$ is in $\DTIME(T(n))$ if and only if there is a TM $M$ that computes $f$ in time $c \times T(n)$ for some constant $c > 0$.
  The complexity class $\P$ is then defined by
  \[
    \P = \bigcup_{\lambda \ge 1} \DTIME(n^\lambda).
  \]
\end{definition}
In words, $\P$ is the set of all languages that can be decided by a TM in a number of steps that scales polynomially in the input size.
The problems in $\P$ are considered to be efficiently solvable.
Therefore, in order to show that some problem is \quotes{easy}, we just have to provide a TM, or put differently an algorithm, that decides the problem in polynomial time.
On the other hand, showing that no polynomial-time algorithm exists for a given problem shows that it is computationally hard.
However, proving the nonexistence of efficient algorithms for many natural computational problems has turned out to be a tremendous challenge -- notwithstanding deep results for computational models strictly less powerful than the TM model~\cite[Part Two]{Arora_2009_Computational}.
Hence, we are going to follow a less ambitious, but very fruitful strategy:
Instead of proving that a given problem is infeasible to solve efficiently, we are comparing its computational complexity to other problems that are conjectured to be computationally infeasible.
If we can show that the problem under consideration is at least as hard as many other problems, which could not be solved efficiently by a myriad of computer scientists in the last decades, then we have strong evidence to believe it is intrinsically hard.
This idea is formalized in the definition of polynomial-time reductions.

\begin{definition}{Def. 2.2 from \cite{Arora_2009_Computational}}
  \label{def:complexity.reducibility}
  A language $L \subset \BitStrings$ is polynomial-time (Karp) reducible to a Language $L' \subset \BitStrings$ denoted by $L \le_p L'$, if there is a polynomial-time computable function $f\colon \BitStrings \to \BitStrings$ such that for all $x \in \BitStrings$ we have $x \in L \iff f(x) \in L'$.
\end{definition}

Less formally speaking, if we have $L \le_p L'$ then $L'$ is at least as hard to decide as $L$.
Indeed, by using the reduction $f$, we can turn any TM deciding $L'$ into a TM deciding $L$ with at most polynomial runtime overhead.
Particularly, if additionally $L' \in \P$ then so is $L$.
Conversely, if no efficient algorithm for $L$ exists and $L \le_p L'$, then there cannot be an efficient algorithm $L'$.
The latter observation is the basis of the strategy mentioned above:
We show that a given problem is computationally hard by establishing that is at least as hard as a large class of other problems, which have withstood numerous attempts at solving them efficiently so far.
For this purpose we introduce the complexity class $\NP$.

\begin{definition}
  \label{def:complexity.np}
  A language $L \subset \BitStrings$ is in $\NP$ if there exists a polynomial-time computable function $M$ such that for every $x \in \BitStrings$
  \[
    x \in L \iff \exists u \in \{ 0, 1 \}^{p(\abs{x})} s.t.\ M(x, u) = 1.
  \]
  The function $M$ is called the verifier and for $x \in L$, the bitstring $u$ is called a certificate for $x$.
\end{definition}

Clearly, $\P \subset \NP$ with $p(\abs{x}) = 0$.
On the other hand, the question whether $\NP \subset \P$, and hence, $\P = \NP$ is one of the major unsolved problems in math and science~\cite{Cook_2000_P,Aaronson_????_P,Garey_2002_Computers}.
One argument against this hypothesis goes as follows:
Whereas the problems in $\P$ are considered to be easily solvable, the problems in $\NP$ are at least easily checkable given the certificate.
In other words the question whether $\P = \NP$ boils down to the question whether finding the solution to a problem is harder than verifying whether a given solution is correct.
However, these philosophical considerations are not the main reason for the importance of \cref{def:complexity.np} from a computer scientific point of view.
The main justification of the class $\NP$ is twofold:
On the one hand, there are a myriad of problems known to be in $\NP$ and many of them have resisted all efforts of finding a polynomial-time algorithm.
On the other hand, there is a tremendous number of problems known to be at least as hard as any problem in $\NP$~\cite{Garey_2002_Computers} -- these are referred to as $\NP$-hard problems:

\begin{definition}
  \label{def:complexity.hard_and_complete}
  We say that a language $L$ is $\NP$-hard if for every $L' \in \NP$ we have $L' \le_p L$.
  We say that a language $L$ is $\NP$-complete if $L \in \NP$ and $L$ is $\NP$-complete.
\end{definition}

Clearly, for any $\NP$-hard problem $L$, $L \le_p L'$ implies that $L'$ is $\NP$-hard as well, and hence, any problem in $\NP$ is polynomial-time reducible to $L'$.
Therefore, an efficient algorithm for $L'$ would provide an efficient algorithm for a large number of other problems, which are widely considered difficult and which have been confounding experts for years.
This fact is taken as very strong evidence that $L'$ cannot be solved efficiently.
As an example of an $\NP$-complete problem, we consider the \emph{number partition} problem.
\begin{problem}\label{prob:complexity.partition}
  Given a vector $\vec a \in \mathbb{N}^d$, decide whether there exists a vector $\vec \psi$ with
  \[
    \forall{k}\:\psi_{k}\in\left\{ -1,1\right\} \;\textrm{ and }\quad \vec a \cdot \vec\psi=0.
    \label{eq:ellpos.partition_vector}
  \]
\end{problem}
In case there is such a vector $\vec \psi$ one says that the instance $\vec a$ allows for a partition because the sum of components of $\vec a$ labeled by $\psi_i = 1$ is equal to the sum of components $a_i$ labeled by $\psi_i = -1$.
For a proof of $\NP$-hardness of \cref{prob:complexity.partition}, see~\cite{Garey_2002_Computers}.\\


The question remains what the notion of $\NP$-hardness means in practice.
Due to its strict definition in terms of worst-case runtime needed for any instance, \cref{def:complexity.np} leaves open many possibilities for the existence of \quotes{good-enough} solutions.
First, approximative or probabilistic algorithms often suffice for all practical purposes and are not bounded by $\NP$-hardness results~\cite{Garey_2002_Computers,Arora_2009_Computational}.
A prime example is the number partition problem~\ref{prob:complexity.partition}:
It is often considered the \quotes{easiest hard problem} because although it is $\NP$-hard, highly efficient approximative algorithms for it exist~\cite{Kellerer_1997_Efficient}.
Furthermore, considering only the worst-case behavior is often too pessimistic in practice.
A more appropriate classification of the difficulty of \quotes{typical} instances is given in terms of \emph{average case} complexity~\cite{Arora_2009_Computational}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction to QSE}
\label{sec:error.intro}


The goal of state estimation is to provide a complete description of an experimental preparation procedure from experimental feasible measurements.
In the case of QSE, this complete description is given in terms of the density matrix $\varrho$ of the system~\cite{Paris_2004_Quantum}.
Another common procedure performed in quantum experiments is \emph{quantum process estimation}, where the goal is to recover a \emph{quantum channel}~\cite{Nielsen_2010_Quantum}.
However, the task of process estimation can be mapped to QSE by way of the Choi-Jamiolkowski isomorphism~\cite{Nielsen_2010_Quantum,Jezek_2003_Quantum,Altepeter_2003_AncillaAssisted}, and therefore, we are only concerned with the problem of QSE in this work.

Since its inception in the fifties~\cite{Fano_1957_Description}, QSE has proven to be a crucial experimental tool -- in particular in quantum information-inspired setups.
It has been used to characterize quantum states in a large number of different platforms \cite{Brien_2004_Quantum,Lundeen_2009_Tomography,Terriza_2004_Triggered,Karpinski_2008_FiberOptic,Rippe_2008_Experimental,Steffen_2006_Measurement,Childress_2006_Coherent,Riebe_2007_Quantum,Schwemmer_2014_Experimental} and even been scaled to systems with dimension on the order of 100~\cite{Haeffner_2005_Scalable}.
However, as the Hilbert space dimensions of quantum systems implemented in the lab grow, it is unclear whether this approach to \quotes{quantum characterization} will continue to make sense.

Acquiring and post-processing the data necessary for fully-fledged QSE is already prohibitively costly for intermediated sized quantum experiments.
As an extreme example, the eight qubit MLE reconstruction with bootstrapped error bars in~\cite{Haeffner_2005_Scalable} required weeks of computation in 2005 (private communication, see~\cite{Gross_2010_Quantum}).
Some sol
This problem can be partially mitigated by means of more efficient algorihtms~\cite{Smolin_2012_Maximum,Qi_2013_Quantum,Hou_2016_Full,Shang_2017_Superfast} or approaches exploiting structural assumptions to improve the sampling and computational complexity of state estimation~\cite{Cramer_2010_Efficient,Gross_2010_Quantum,Flammia_2012_Quantum,Schwemmer_2015_Systematic,Baumgratz_2013_Scalablea,Baumgratz_2013_Scalable}.

There is also the question what use is a giant density matrix with millions of entries to an experimentalist?
In many cases, the full quantum state contains more information than necessary.
For example, consider the case when a single number such as the fidelity of the state in the lab w.r.t.\ some target state is sufficient whether the experiment works \quotes{sufficiently well} or not.
For this purpose, a variety of theoretical tools for \emph{quantum hypothesis testing}, \emph{certification}, and scalar \emph{quantum parameter estimation}~\cite{Donnell_2015_Quantum,Audenaert_2008_Asymptotic,Guehne_2009_Entanglement,Flammia_2011_Direct,Schwemmer_2015_Systematic,Li_2016_Optimal} have been developed in the past years that avoid the costly step of full QSE.\\

However, there remain use cases that necessitate fully-fledged QSE.
We see a particularly important role in the emergent field of \emph{quantum technologies}:
Any technology requires means of certifying that components function as intended and, should they fail to do so, identify the way in which they deviate from the specification.
As an example, consider the implementation of a quantum gate that is designed to act as a component of a universal quantum computing setup.
One could use a certification procedure -- direct fidelity estimation, say -- to verify that the implementation is sufficiently close to the theoretical target that it meets the stringent demands of the quantum error correction threshold.
If it does, the need for QSE has been averted.
However, should it fail this test, the certification methods give no indication \emph{in which way} it deviated from the intended behavior.
They yield no actionable information that could be used to adjust the preparation procedure.
The pertinent question \quotes{what went wrong} cannot be cast as a hypothesis test.
Thus, while many estimation and certification schemes can -- and should -- be formulated without resorting to full state estimation, the above example shows that QSE remains an important primitive.

\subsection{Existing work on error regions}
In practice (e.g.~\cite{Haeffner_2005_Scalable}), uncertainty quantification for tomography experiments is usually based on general-purpose resampling techniques such as \quotes{bootstrapping}~\cite{Efron_1994_Introduction}.
A common procedure is this: For every fixed measurement setting, several repeated experiments are performed.
This gives rise to an empirical distribution of outcomes for this particular setting.
One then creates a number of simulated data sets by sampling randomly from a multinomial distribution with parameters given by the empirical values.
Each simulated data set is mapped to a quantum state using maximum likelihood estimation.
The variation between these reconstructions is then reported as the uncertainty region.
There is no indication that this procedure grossly misrepresents the actual statistical fluctuations.
However, it seems fair to say that its behavior is not well-understood.
Indeed, it is simple to come up with pathological cases in which the method would be hopelessly optimistic:
E.g.\ one could estimate the quantum state by performing only one repetition each, but for a large number of randomly chosen settings.
The above method would then spuriously find a variance of zero.

On the theoretical side, some techniques to compute rigorously defined error bars for quantum tomographic experiments have been proposed in recent years.
The works of Blume-Kohout~\cite{Kohout_2012_Robust} as well as Christandl, Renner, and Faist~\cite{Christandl_2012_Reliable,Faist_2015_Practical} exhibit methods for constructing confidence regions for QST based on likelihood level sets.
While very general, neither paper provides a method that has both a runtime guarantee and also adheres to some notion of non-asymptotic optimality~\cite{Kiefer_2012_Introduction,Cam_2012_Asymptotic}.

Some  authors have proposed a \quotes{sample-splitting} approach, where the first part of the data is used to construct an estimate of the true state, whereas the second part serves to construct an error region around it~\cite{Flammia_2012_Quantum} (based on~\cite{Flammia_2011_Direct}), as well as~\cite{Carpentier_2015_Uncertainty}.
These approaches are efficient, but rely on specific measurement ensembles (operator bases with low operator norm), approach optimality only up to poly-logarithmic factors, and -- in the case of~\cite{Flammia_2012_Quantum, Flammia_2011_Direct} -- rely on adaptive measurements.

Regarding Bayesian methods, the \emph{Kalman filtering} techniques of~\cite{Audenaert_2008_Asymptotic} provide a efficient algorithm for computing credible regions.
This is achieved by approximating all Bayesian distributions over density matrices by Gaussians and restricting attention to ellipsoidal credible regions.
The authors develop a heuristic method for taking positivity constraints into account -- but the degree to which the resulting construction deviates from being optimal remains unknown.
A series of recent papers aim to improve this construction by employing the \emph{particle filter} method for Bayesian estimation and uncertainty quantification
\cite{Granade_2016_Practicala,Wiebe_2015_Bayesian,Ferrie_2014_High}.
Here, Bayesian distributions are approximated as superpositions of delta distributions and credible regions constructed using Monte Carlo sampling. These methods lead to fast algorithms and are more flexible than Kalman filters with regard to modelling prior distributions that may not be well-approximated by any Gaussian. However, once more, there seems to be no rigorous estimate for how far the estimated credible regions deviate from optimality.
Finally, the work in~\cite{Shang_2013_Optimal} constructs optimal credible regions w.r.t.\ a different notion of optimality:
Instead of penalizing sets with larger volume, they aim to minimize the prior probability as suggested by~\cite{Evans_2006_Optimally}.


\subsection{The QSE statistical model}
We now introduce the statistical model and the corresponding likelihood function used for the rest of this chapter.
The first major assumption is that the system's state in the lab is sufficiently stable for the duration of the experiment.
Therefore, we can assume that all data is generated from a fixed, but unknown state $\varrho_0 \in \States$, where
\[
  \States := \left\{ \varrho \in \Complex^{d \times d}\colon \adj{\varrho} = \varrho, \tr \varrho = 1, \varrho \ge 0 \right\}
\]
denotes the state space of density matrices.
Note that this assumption is not necessary for QSE in general, see e.g.~\cite{Granade_2016_Practicala} for Bayesian methods that allow for tracking time-dependent states.


Since we consider the case of non-adaptive state estimation, the measurements performed are characterized by a fixed POVM $\{E_k\}_{k=1}^m$ and the probability of the event $k$ when the system is in the state $\varrho_0$ is given by the \emph{Born rule}
\[
  p_k = \tr E_k \varrho_0.
\]
However, in reality the quantum expectation values are never observed directly.
Instead, when the experiment is repeated on $N$ independent copies of $\varrho_0$, the observations are counts $n_i \ge 0$ with $\sum_i n_i = N$
following a multinomial distribution
\[
  \label{eq:intro.multinomial}
  \Prob_{\vec p}(n_1, \ldots, n_m) = \frac{N!}{n_1! \cdots n_m!} \, p_1^{n_1} \times \cdots \times p_m^{n_m}.
\]
In case $N$ is large and all the $p_i$ are sufficiently large, the multinomial distribution~\eqref{eq:intro.multinomial} is local asymptotic normal~\cite{Severini_2005_Elements}.
Therefore, we can approximate \cref{eq:intro.multinomial} by a Gaussian distribution
\[
  \label{eq:intro.gauss_approx}
  \Prob_{\vec p}(y_1, \ldots, y_m) \approx \Pdf_{\vec p, \Sigma}(y_1, \ldots, y_m)
\]
with $y_i = \frac{n_i}{N}$ and $\Pdf_{\vec p, \Sigma}$ denoting the probability density of a Gaussian distribution with mean $\vec p$ and covariance matrix $\Sigma = \diag(\vec p) - \vec p \vec p^T$.
Hence, under this Gaussian approximation, the relative counts $y_i$ are given in terms of a \emph{linear Gaussian model} with Gaussian likelihood function
\[
  \label{eq:intro.gauss_likelihood}
  \Loss(\varrho; \vec y) = \Prob_{\vec p}(\vec y)
\]
defined in terms of \cref{eq:intro.gauss_approx} and the probabilities $p_k = \tr E_k \varrho$.
However, if some of the $p_{i}$ are close or equal to zero, which happens for rank deficient $\varrho_{0}$, local asymtotic normality (LAN) -- that is the approximation in \cref{eq:intro.gauss_approx} -- does not hold.
In~\cite{Scholten_2016_Behavior} the authors discuss some implications of the lack of LAN in QSE.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Hardness results for confidence regions}
\label{sec:error.ortho}

In this section we are going to present the first major result of our work~\cite{Suess_2016_Error} concerned with frequentist confidence regions in QSE.
Optimal confidence regions for high-dimensional parameter estimation problems in are generally intricate even without additional constraints as there are only few elementary settings, where optimal confidence regions are known and easily characterized.

Since the goal of this work is to demonstrate that quantum shape constraints severely complicate even \quotes{classically} simple confidence regions, in the further discussion we restrict the discussion to a simplified setting:
We focus on confidence ellipsoids for Gaussian distributions, which are one of the few easily characterizable examples.
Furthermore, Gaussian distributions arise in the limit of many measurements due to \emph{local asymptotic normality}.
In this section we show that even characterizing these highly simplifying ellipsoids with the quantum constraints taken into account constitutes a hard computational problem.
This simplification is motivated by the goal to show that the computational intractability exclusively stems from the quantum constraints and that it is not caused by difficulties of high-dimensional statistics in general.
Furthermore, any less restrictive formulation encompassing this simplified setting must be at least as hard to solve.

In conclusion, although exploiting the physical constraints may help to reduce the uncertainty tremendously as mentioned in the introduction, doing so in an optimal way is computationally intractable in general.
Therefore, our work can be interpreted as a trade-off between computational efficiency and statistical optimality in QSE.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Optimal confidence regions for quantum states}
\label{sub:ortho.optimal}

As already indicated in the introduction, additional constraints on the parameter $\theta_0$ under consideration can be exploited to possibly improve any confidence region estimator.
This is especially clear for notions of optimality with a loss function stated in terms of a volume measure $\Vol(\cdot)$, as we will show in this section.
Therefore, assume that $\theta_0 \in \Omega_\mrc$, where the constrained parameter space $\Omega_\mrc \subset \Omega$ has non-zero measure w.r.t.\ $\Vol$.
We consider an especially simple procedure to take the constraints into account, namely truncating all $\theta \notin \Omega_\mrc$ from tractable confidence regions for the unconstrained problem.

Although such an ad-hoc approach does not seem to exploit the constraints in an optimal way, it has multiple advantages as we discuss in more detail in \cref{sub:ortho.hard}:
First and foremost, some notions of optimality, e.g.\ admissibility, are preserved under truncation as shown in \cref{lem:ortho.admissible_truncation}.
In other words, there are notions of optimality such that truncation of an optimal confidence region for the unconstrained problem gives rise to an optimal region for the constrained one.
Furthermore, as already mentioned in the introduction, our goal is to show that the intractability arises purely due taking the constraints imposed by quantum mechanics into account.
Therefore, we start from a tractable solution in the unconstrained setting and show that even this simple approach of taking the constraints into account leads to an computationally intractable problem.
Finally, the truncation approach simplifies the discussion but as we discuss in \cref{sub:ortho.hard}, our results apply to a much larger class of confidence regions such as likelihood ratio based Gaussian ellipsoids.\\


We start by showing that the truncation of confidence regions preservers admissibility.
Notice that \cref{def:stat.frequentist.admissible} can be stated for both, the unconstrained estimation problem $\theta_0 \in \Omega$ as well as the constrained estimation problem $\theta_0 \in \Omega_\mrc$.
The question is: How are admissible confidence regions for the constrained setting related to admissible confidence regions for the unconstrained estimation problem?
\begin{lemma}\label{lem:ortho.admissible_truncation}
  Let $\CR$ denote an admissible confidence region for the unconstrained estimation problem for the parameter $\theta_0 \in \Omega$.
  Then, the truncated region estimator $\CR^\cap := \CR\cap\Omega_\mrc$ is an admissible confidence region for the constrained problem with $\theta_0 \in \Omega_\mrc$.
\end{lemma}
\begin{proof}
  Under the assumption that $\CR^\cap$ is not admissible, there must exist a better confidence region $\CR^+$ for the constrained parameter estimation problem.
  W.l.o.g.\ assume that both $\CR^+$ and $\CR^\cap$ have the same coverage.
  Therefore, we must have $\Vol(\CR^+(\vec y)) \leq \Vol(\CR^\cap(\vec y))$ for almost all observations $\vec y \in \Reals^m$, and there is a set $Y \subset \Reals^m$ of non-zero measure such that $\Vol(\CR^+(\vec y)) < \Vol(\CR^\cap(\vec y))$ for $\vec y \in Y$.
  Define a new confidence region for the unconstrained problem
  \[
    \CR' := \CR^+ \cup \CR^c,
    \label{eq:ortho.new_region}
  \]
  where $\CR^{c}=\CR\setminus \CR^{\cap}$ denotes the compliment of $\CR^{\cap}$ in $\CR$.
  Then, $\CR'$ has the given coverage level, since $\CR^+$ provides coverage for $\theta_0 \in \Omega_\mrc$, whereas $\CR^c$ provides coverage for the case $\theta_0 \in \Omega \setminus \Omega_\mrc$.
  Furthermore, we have for almost all $\vec y$
  \[
    \label{eq:ortho.volumes}
    \begin{split}
      \Vol(\CR'(\vec y))
      &= \Vol(\CR^+(\vec y)) + \Vol(\CR^c(\vec y)) \\
      &\leq \Vol(\CR^\cap(\vec y)) + \Vol(\CR^c(\vec y)) \\
      &= \Vol(\CR(\vec y)).
    \end{split}
  \]
  Finally, strict inequality holds in Eq.~\eqref{eq:ortho.volumes} for all $\vec y \in Y$ due to the assumption on $\CR^+$.
  However, this would imply $\CR$ not being admissible in contradiction to the assumptions of the Lemma.
\end{proof}
A similar procedure of computing optimal point estimators for constrained problems by modifying an optimal estimator for the unconstrained problem was introduced for the maximum likelihood estimator in~\cite{Smolin_2012_Maximum}.\\



One criticism raised against the use of the truncated confidence regions is the possibility that they may yield empty realizations and, hence, are considered \quotes{unphysical}~\cite{Feldman_1998_Unified}.
However, according to the standard definition in \cref{sub:stat.frequentist}, a procedure that reports 95\% confidence regions is allowed to give any result 5\% of the time.

Furthermore, a different strategy often adopted for point estimator is to use an unconstrained parametrization for the constrained parameter space.
A typical example is a coin toss model with bias $p \in [0, 1]$.
Instead of $p$, the problem can also be parametrized in terms of of log-odds $\log\frac{p}{1 - p}$, which can take any value in $(-\infty,\infty)$.
Similar, one could use the following parametrization for quantum states guaranteed to give a positive semidefinite, Hermitian matrix with trace 1
\[
  \rho(X) = \frac{X \adj X}{\tr X \adj X}
\]
with $X \in \mathbb{C}^{d \times d}$.
Although this parametrization can certainly be advantageous for point estimation, it is unlikely to be helpful for uncertainty quantification:
While $X$ and $\rho(X)$ carry equivalent information, the size of a region measured in \quotes{$X$-space} is hardly related to the size of a region in the physical state space unless one chooses highly unnatural volume measures.
This is necessarily so, as any map from an unbounded space onto the compact quantum state space must grossly distort the geometry.
So, having obtained a \quotes{small confidence region} in parameter space does not imply that the state has been well-estimated w.r.t.\ any physically relevant metric.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Confidence Regions from Linear Inversion}
\label{sub:ortho.linear_inversion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
  \centering
  \input{tikz/error_truncation_construction.tex}%
  \caption{\label{fig:ortho.geometry}%
    Geometric construction of confidence region for $\estim\varrho$.
    Quantum states are mapped by a measurement matrix $A$ to the respective quantum expectation values $\vec y$.
    Conversely, the pre-image of a confidence region $\CR_\vec{y}$ under $A$ gives rise to a confidence region for $\varrho_0$.
    These may be unbounded if the measurements are not tomographically complete -- a drawback that can be cured by taking into account the physical constraints on quantum states, i.e.\ positivity.
  }
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


A particularly simple approach to QSE is the method of \emph{linear inversion}, which we are going to review now:
First, assume that the \emph{true} but unknown quantum state is represented by a $d\times d$ density matrix $\varrho_0 \in \States \subset \HermTrace$.
The data is obtained from measurements of $m\geq d^{2} - 1$ tomographically-complete measurement projectors $E_{1},\ldots,E_{m}$.
By $y_{k}=\tr\left(E_{k}\varrho_0\right)$, $k=1,\ldots,m$ we denote the (quantum) expectation values of $E_{k}$ for the true state $\varrho_0$.
Since these relations are linear, we can rewrite them as $\vec{y} = A \varrho_0$, where $A$ is the measurement (or design) matrix independent of $\varrho_0$.
The (pseudo)-inverse of the above relation is given by
\[
  \label{eq:ortho.linear_inversion}
  \varrho_0 ={\left(A^{T}A\right)}^{-1}A^{T}\vec{y}
\]
and simplifies to $\varrho_0=A^{-1}\vec{y}$ if $m=d^{2} - 1$.

Of course, in an experiment, the expectation values $\vec{y}$ are unknown and can only be approximated by some estimate $\estim{\vec y}$ based on the observed data.
The linear inversion estimate for the quantum state $\estim{\varrho}$ is then given by Eq.~\eqref{eq:ortho.linear_inversion} with the probabilities $\vec y$ replaced by the empirical frequencies $\estim{\vec y}$.
However, due to statistical fluctuations the estimated state $\estim{\varrho}$ is not necessarily positive semidefinite~\cite{Knips_2015_How}, which led to the development of estimators enforcing the physical constraints such as the maximum likelihood estimator~\cite{Hradil_2004_3}.
Although the linear inversion and maximum likelihood estimator solve two distinct problems -- namely the unconstrained and constrained one, respectively -- in certain cases the two are related.
 More precisely, if the outcomes approximately follow a Gaussian distribution, a fast projection algorithm computes the maximum likelihood estimate from the linear inversion estimate directly~\cite{Smolin_2012_Maximum}.

Here, we take a similar approach.
First, the simple geometric interpretation of the linear inversion estimator (see Fig.~\ref{fig:ortho.geometry}) allows us to map confidence regions for the expectation values to confidence regions for the state without taking into account the positivity constraint:
If $\CR_\vec{y}$ is a confidence region for $\vec y$ with confidence level $1 - \alpha$, then so is its pre-image under the measurement map
\[
  \label{eq:ortho.linear_confidence}
  \CR_{\varrho_0} := A^{-1}(\CR_{\vec y})
\]
for $\varrho_0$.
Second, the truncation $\CR^\cap_{\varrho_0} := \CR_{\varrho_0} \cap \States$ yields an improved confidence region for the problem with quantum constraints taken into account.
As shown in Lemma~\ref{lem:ortho.admissible_truncation}, this approach yields admissible confidence region provided the original region was admissible.

The same construction can also be carried out for tomographically incomplete measurements, i.e.\ for $m<d^{2}$:
Since the measurement matrix $A$ is non-invertible in this case, the estimate for the state $\estim{\varrho}$ satisfying $A\estim{\varrho} = \estim{\vec y}$ is not uniquely defined.
However, under additional structural assumptions, one can single out a unique estimate~\cite{Gross_2010_Quantum,Flammia_2012_Quantum}.
The singularity of the measurement map $A$ also reflects in the confidence region defined by Eq.~\eqref{eq:ortho.linear_confidence}.
Even if $\CR_\vec{y}$ is a bounded region, the confidence region for the state $\CR_{\varrho_0}$ extends to infinity in the directions \quotes{unobserved by $A$}.
In both cases, the tomographically complete and incomplete, we can use the intersection with the psd states $\States$ to reduce the the region's size while not sacrificing coverage.
This improvement is especially far-reaching in the latter case, where it turns an unbounded region to a bounded one just by taking into account the physical constraints.


Of course, the question is whether we can somehow characterize the truncated confidence region $\CR_{\varrho_0}^\cap := A^{-1}(\CR_\vec{y}) \cap \States$ computationally efficiently.
As already mentioned in \cref{sec:error.intro}, we are going to make the simplifying assumption that the measured frequencies are approximately Gaussian distributed.
Furthermore, we are going to focus on a class of confidence regions that are efficiently characterizable in the unconstrained setting, namely Gaussian confidence ellipsoids or, more precisely, ellipsoidal balls of the form
\[
  \CR_\vec{y} = \left\{ \vec y \in \Reals^m\colon  {\left(\vec{y}-\estim{\vec{y}}\right)}^T B\left(\vec{y}-\estim{\vec{y}}\right) \leq 1  \right\}
  \label{eq:ortho.confelip}
\]
centered at the the empirical frequencies  $\estim{\vec y}$.
The $m\times m$, symmetric, positive semi-definite matrix $B$ completely specifies the ellipsoidal shape of this confidence region.
These are the natural generalizations of the well-known $2\sigma$ confidence intervals to multivariate Gaussian distributions.

However, even in the unconstrained setting, the ellipsoidal construction~\eqref{eq:ortho.confelip} is known to be admissible only for $m=\left\{ 1,2\right\}$~\cite{Joshi_1969_Admissibility}, while it is not admissible for $m\geq3$~\cite{Joshi_1967_Inadmissibility} due to Stein's phenomenon discussed in \cref{sub:stat.frequentist}:
By shifting the center of the ellipsoid from the empirical mean $\hat{\vec y}$ to the Stein estimator~\eqref{eq:stat.frequentist.stein_estimator}, one can improve the coverage while keeping the volume constant~\cite{Joshi_1967_Inadmissibility}.
Smaller confidence ellipsoids with the same coverage can be obtained by shifting the center slightly~\cite{Tseng_1997_Good,Hwang_1982_Minimax} or even by using more complicated shapes~\cite{Shinozaki_????_Improved,Brown_1995_Optimal}.
Nevertheless, none of these constructions is known to be optimal and, to the best of the author's knowledge, no optimal confidence region for multivariate Gaussians in dimensions $m \ge 3$ is known.
But since our discussion is focused on the question how the physical psd constraints can be used to improve confidence regions, we are still going to use the ellipsoids~\eqref{eq:ortho.confelip} as a tractable example:
As we will prove later, it is impossible to characterize the truncated ellipsoids efficiently although they are fully described by only few parameters, namely $\estim{\vec y}$ and $B$.\\



In the remainder of this section, we are going to discuss a useful parametrization of the ellipsoids $\CR_{\varrho_0} = A^{-1}(\CR_\vec{y})$ with $\CR_\vec{y}$ given by \cref{eq:ortho.confelip}.
To this end we use the fact that any $d\times d$ Hermitian matrix can be expanded in a basis formed by the identity $\1$ and $d^{2}-1$ traceless Hermitian matrices $\sigma_{i}$, $i=1,\ldots,d^{2}-1$, normalized according to $\textrm{Tr}(\sigma_{i}\sigma_{j})=2\delta_{ij}$.
With the symbols $\sigma_{i}$ we associate here the most common choice of the basis elements~\cite{Kimura_2003_Bloch}  --  explicitly provided in \cref{sec:error.parametrisation}.
Any other basis $\sigma_{i}'=\sum_{j}O_{ji}\sigma_{j}$, which can be obtained from $\sigma_i$ by a $d^{2}-1$ dimensional, orthogonal matrix $O$, is of course equally valid.
For $d=2$ the choice stated in~\ref{sec:error.parametrisation} is simply the Bloch basis of Pauli matrices: $\sigma_{1} = \sigma_{x}$, $\sigma_{2} = \sigma_{y}$ and $\sigma_{3} = \sigma_{z}$.
In higher dimensions the matrices $\sigma_{i}$ maintain the Bloch basis structure:s
Let
\[
  \label{eq:ortho.x_yz_index}
  i_{d} = \frac{d(d-1)}{2},
\]
then the definition of $\sigma_i$ mimics $\sigma_{x}$ for $1\leq i\leq i_{d}$, $\sigma_{y}$ for $i_{d}+1\leq i\leq2i_{d}$ and $\sigma_{z}$ for $2i_{d}+1\leq i\leq d^{2}-1$.
Therefore, we are going to refer to the $\sigma_{i}$ as \emph{(generalized) Bloch matrices} and the corresponding parametrization of Hermitian matrices as the \emph{(generalized) Bloch representation}.
The following theorem provides a useful parameterization of pre-images under the design matrix $A$ of ellipsoids.
\begin{theorem}\label{thm:ortho.ellipsoids}
  For the tomographically complete case $m \geq d^2 - 1$, the pre-image under the design matrix of any ellipsoid of the form~\eqref{eq:ortho.confelip} can be written as
  \[
    \label{eq:ortho.ellipsoid}
    \CR_{\varrho_0} = A^{-1}\left( \CR_\vec{y} \right) = \left\{ \estim{\varrho} + \sum_{i}R_{i}u_{i}\sigma_{i}'\colon \vec{u}^{T}\vec{u}\leq1 \right\}.
  \]
  Here, $\estim{\varrho}$ is the linear inversion estimator corresponding to \cref{eq:ortho.linear_inversion}, that is a Hermitian matrix with $\tr \estim{\varrho} = 1$.
  The $R_{i}>0$ ($i=1,\ldots,d^{2}-1$) are the ellipsoid's radii in the directions given by $\sigma_{i}'=\sum_{j}O_{ji}\sigma_{j}$ and the orthogonal matrix $O\in\mathcal{SO}\left(d^{2}-1\right)$ furnishes any orientation of the semi-major axes of the ellipsoid.
\end{theorem}
\begin{proof}
  Note that whenever the sum has no limits specified (like in Eq.~\eqref{eq:ortho.ellipsoid}), by default it extends from $1$ to $d^{2}-1$.
  Let us parameterize both $\varrho \in \CR_{\varrho_0}$ and $\estim{\varrho}$ in the Bloch representation with coordinates $w_{i}$ and $\estim{w}_i$, respectively:
  \[
    \varrho=\frac{1}{d}\1 + \sum_{i}w_{i}\sigma_{i},
    \qquad\estim{\varrho}=\frac{1}{d} \1 + \sum_{i}\estim{w}_{i}\sigma_{i}.
  \]
  Since $\vec{y}=\textrm{Tr}\left(\vec{E}\varrho\right)$ and $\estim{\vec{y}}=\textrm{Tr}\left(\vec{E}\estim{\varrho}\right)$ we find
  \[
    \vec{y}-\estim{\vec{y}}=Q\left(\vec{w}-\estim{\vec{w}}\right),
  \]
  where $Q$ is a $m\times(d^{2}-1)$ matrix with elements $Q_{ki}=\textrm{Tr}\left(E_{k}\sigma_{i}\right)$.
  In other words, the Bloch coordinates satisfy the same ellipsoid equation (\ref{eq:ortho.confelip}) as the measurement outcomes with $B$ substituted by the $d^{2}-1$ dimensional square matrix $B'=Q^{T}BQ$.
  Since $B$ is symmetric and positive definite, the same holds for $B'$.
  Hence, $B'$ can be diagonalized to the form $B'=ODO^{T}$, where $O \in \mathcal{SO}(d^2 - 1)$ and $D=\textrm{diag}(R_{1}^{-2},\ldots,R_{d^{2}-1}^{-2})$ is a diagonal matrix with positive entries.
  If we rescale $\vec{w}-\estim{\vec{w}}=OD^{-1/2}\vec{u}$, then $\vec{u}^{T}\vec{u}\leq1$ and
  \[
    \varrho-\estim{\varrho}=\sum_{j}\left(\sum_{i}O_{ji}R_{i}u_{i}\right)\sigma_{j}.
  \]
  In the last step of the proof we simply change the orientation of the basis to $\sigma_{i}'=\sum_{j}O_{ji}\sigma_{j}$.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Computational Intractability of Truncated Ellipsoids}
\label{sub:ortho.hard}

Guided by the discussion from the previous section we now study the confidence region for the linear inversion QST defined as
\[
  \label{eq:ortho.truncated_ellipsoid}
  \CR_{\varrho_0}^\cap := \CR_{\varrho_0} \cap \States = A^{-1}(\CR_{\vec y}) \cap \States,
\]
where $\CR_{\varrho_0}$ is given by the ellipsoid~\eqref{eq:ortho.ellipsoid} for the tomographically complete case $m=d^2 - 1$.
In this section, we are going to show that in contrast to the full ellipsoid $\CR_{\varrho_0}$, the truncated ellipsoid $\CR_{\varrho_0}^\cap$ cannot be characterized computationally efficiently.
This shows, for example, in the fact that there is no efficient algorithm to answer the following question:
How much does taking into account the physical constraints reduce the size of the confidence region on a particular set of observed data?
Note that we will not be concerned with properties of the region estimator but with a single instance corresponding to a fixed set of data.
By abuse of notation, we are going to refer to these instances as $\CR_{\varrho_0}$ and $\CR_{\varrho_0}^\cap$ as well.

More precisely, we are concerned with the question if a fixed ellipsoid $\CR_{\varrho_0}$ changes due to the constraints in Eq.~\eqref{eq:ortho.truncated_ellipsoid} or if $\CR_{\varrho_0}$ is fully contained in the set of psd states.
For the precise formulation, we use the representation of ellipsoids from Thm.~\ref{thm:ortho.ellipsoids}.
\begin{problem}\label{prob:ortho.ellpos}
  Given the center $\estim\varrho$, radii $R_i$, and a basis $\sigma'_i$ for $\HermTrace$.
  Is there a $\vec u \in \Reals^{d^2 - 1}$ with $\vec{u}^{T}\vec{u} \leq 1$ such that
  \[
    \label{eq:ortho.ellpos_condition}
    \estim{\varrho} + \sum_{i}R_{i}u_{i}\sigma_{i}' \in \HermTrace \setminus \States?
  \]
\end{problem}
The main result of this section is the following statement on the computational complexity of the aforementioned problem.
\begin{theorem}\label{thm:ortho.hard}
  Problem~\ref{prob:ortho.ellpos} is $\NP$-complete.
\end{theorem}
As a consequence of \cref{thm:ortho.hard}, the problem of \quotes{characterizing} the truncated confidence ellipsoids $\CR_{\varrho_0}^\cap := A^{-1}(\CR_\vec{y}) \cap \States$ defined in Sec.~\ref{sub:ortho.linear_inversion} computationally is hard in general.
By characterizing we mean computing any property of $\CR_{\varrho_0}^\cap$ that is sensitive to whether the truncation influences the original ellipsoid or not, e.g.\ computing the volume of the truncated ellipsoid or its distance to boundary of the quantum state space with high enough precision.
Note, however, that there are also properties such as the diameter that might be unaffected by the truncation in certain special cases and, hence, their computational complexity cannot be classified using \cref{thm:ortho.hard}.
Furthermore, the more general problem of characterizing truncated confidence regions in general (without the Gaussian approximation) is hard as well since it subsumes \cref{prob:ortho.ellpos}.

Another consequence of the theorem concerns confidence regions for the constrained problem, which output \quotes{good regions} for the unconstrained problem when the constraints are not active:
More precisely, it is extremely natural to use likelihood ratio-based ellipsoidal confidence regions for unconstrained Gaussian data although they cannot be optimal due to Stein's phenomenon.
So it is natural to require any quantum region estimator to behave this way in the particular case that the likelihood function is concentrated well away from the boundary of state space.
What \cref{thm:ortho.hard} shows is that any region estimator subject to this criterion must necessarily solve NP-hard problems.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Theorem~\ref{thm:ortho.hard}}
\label{sub:error.ellpos}

The remainder of this section is dedicated to the proof of \cref{thm:ortho.hard} and to discuss a tractable special case.
First, note \cref{prob:ortho.ellpos} is in $\NP$ as any $\vec u$ fulfilling \cref{eq:ortho.ellpos_condition} serves as a certificate.
The rest of the proof of \cref{thm:ortho.hard} is inspired by a similar result due to Ben-Tal and Nemirovski~\cite{Tal_1998_Robust} in robust optimization theory, who showed that the following problem is $\NP$-complete.
\begin{problem}
  \label{prob:ortho.bental}
  Given $k \in \Naturals$ and $k$ $d\times d$ symmetric matrices $A_{1},\ldots,A_{k}$, check whether there is a $\vec u \in \Reals^k$ with $\vec{u}^{T}\vec{u} \leq 1$ such that $\sum_{i=1}^{k}u_{i}A_{i} > \1_{d}$.
\end{problem}
Although the two problems are strongly related, the intractability result of \cref{prob:ortho.bental} cannot be applied directly to \cref{prob:ortho.ellpos} due to the following crucial difference:
The proof of NP-hardness of \cref{prob:ortho.bental} constructs a reduction of the number partition problem~\ref{prob:complexity.partition} to the special case of \cref{prob:ortho.bental} with $k=\frac{d(d-1)}{2} + 1$ and real symmetric matrices $A_i$, which are not necessarily pairwise orthogonal to each other~\cite[Sec.~3.4.1]{Tal_1998_Robust}.
However, in Prob.~\ref{prob:ortho.ellpos}, the $\sigma'_i$ ($i=1,\ldots,d^2 - 1)$ form an orthogonal basis of the space of complex Hermitian, traceless matrices.
Hence, we need to adapt the original proof strategy to deal with the restrictions imposed by our QSE related problem.

For the proof of \cref{thm:ortho.hard}, we show that it is already hard to decide \cref{prob:ortho.ellpos} for the special case of ellipsoids that have their semi-major axes aligned with the generalized Bloch basis
In other words, we assume $\sigma'_i = \sigma_i$.
We consider the same radius $R_{1}$ for all directions generalizing the $x$-direction to higher dimensions and the distinct radius $R_{2}$ for the remaining directions:
\[
  \label{eq:ortho.subclass}
  \begin{split}
    R_{i}=R_{1} &\quad i=1,\ldots,i_{d}\\
    R_{i}=R_{2} &\quad i=i_{d}+1,\ldots,d^{2}-1.
  \end{split}
\]
Recall the definition of $i_d$ \cref{eq:ortho.x_yz_index}.
To prove the hardness of \cref{prob:ortho.ellpos}, we use a reduction from the number partition problem \ref{prob:complexity.partition}.
The main technical difficulty is to identify the values of $R_1$ and $R_2$ as well as $\estim{\rho}$ depending on an instance of the balanced sum problem $\vec a$ such that the corresponding ellipsoid $\CR$ given by \cref{thm:ortho.ellipsoids} contains an element with negative eigenvalues if and only if $\vec a$ has a balanced sum partition.\\




For this purpose, we introduce an explicit representation of pure states in terms of the orthonormal basis $\{\ket{i}\}_i$ from~\cref{sec:error.parametrisation}.
Let $\ket{\Psi}$ denote denote any element from the $d$ dimensional Hilbert space of pure states and define the corresponding  complex vector $\vec{\psi}$ in terms of its coordinates
\[
  \psi_{k} = \braket{k | \Psi} ,\qquad k=1,\ldots,d,\label{coordinates}
\]
Consequently, $\sqrt{\braket{\Psi | \Psi}}$ is the norm of $\ket{\Psi}$, while $\norm{\vec\psi}$ denotes the norm of $\vec\psi$.
Obviously both norms are equal.

In a first step of the proof we write down the positivity condition for the ellipsoid under investigation:
The confidence ellipsoid $\CR$ is fully contained in the set of psd states if and only if for all $\varrho \in \CR$ and all $\ket{\Psi}$,
\(
  \bra{\Psi} \varrho \ket{\Psi} \ge 0.
\)
holds.
In the parametrization from \cref{thm:ortho.ellipsoids}, this condition can be rewritten as
\[
  \bra{\Psi}\estim{\varrho} \ket{\Psi} +R_{1}\sum_{i=1}^{i_{d}}u_{i}v_{i}\left(\vec\psi\right)+R_{2}\sum_{i=i_{d}+1}^{d^{2}-1}u_{i}v_{i}\left(\vec\psi\right)\geq0,
  \label{eq:ellpos.positivity}
\]
where we have already restricted our attention to the special case from Eq.~\eqref{eq:ortho.subclass}.
Furthermore, we have used the shorthand $v_{i}\left(\vec\psi\right)=\bra{\Psi}\sigma_{i} \ket{\Psi} $, which are the rescaled Bloch coordinates of the density matrix $\ket{\Psi} \bra{\Psi}$.
Condition~\eqref{eq:ellpos.positivity} is independent of the norm of $\ket{\Psi}$.
Thus, we can fix $\left\langle \Psi \ket{\Psi} \right. = d$.
Recall that Eq.~\eqref{eq:ellpos.positivity} has to hold for all values of $\vec u$ with $\vec u^T \vec u \le 1$.
Since the left hand side assumes its minimal value for
\[
  u_{i} = -\frac{v_{i}\left(\vec\psi\right)}{\sqrt{\sum_{j}v_{i}^{2}\left(\vec\psi\right)}},
\]
we find that Eq.~\eqref{eq:ellpos.positivity} is equivalent to
\[
 \bra{\Psi}\estim{\varrho} \ket{\Psi} -\sqrt{R_{1}^{2}\sum_{i=1}^{i_{d}}v_{i}^{2}\left(\vec\psi\right)+R_{2}^{2}\sum_{i=i_{d}+1}^{d^{2}-1}v_{i}^{2}\left(\vec\psi\right)}\geq0.
  \label{eq:ellpos.worst_case}
\]
Using the unusual normalization of $\ket\Psi$, we find
\[
  \sum_{i}v_{i}^{2}\left(\vec\psi\right)=2 d\left(d-1\right) =: \mathcal{P},
\]
which can be utilized to simplify~\eqref{eq:ellpos.worst_case}
\[
 g(\vec\psi) := \bra{\Psi}\estim{\varrho} \ket{\Psi} -\sqrt{\mathcal{P}R_{2}^{2}+\left(R_{1}^{2}-R_{2}^{2}\right)\sum_{i=1}^{i_{d}}v_{i}^{2}\left(\vec\psi\right)}\geq0.
  \label{eq:ellpos.major}
\]
In the following, we restrict our attention to  $R_{1}>R_{2}$, so that both term inside the square root are manifestly positive.\\
In the second step of the proof we show and utilize the following lemma:
\begin{lemma}\label{lem:ellpos.real_min}
  If $\estim{\varrho}$ is a real, symmetric matrix w.r.t.\ $\ket{i}$, then the minimum of $g(\vec\psi)$ is attained by a vector $\vec{\psi}$ with real coordinates.
\end{lemma}
\begin{proof}
  Note that we can decompose any vector $\ket\Psi$ into its real and imaginary part
  \[
    \ket\Psi = \ket{\Psi_1} + \ii\Ket{\Psi_2},
  \]
  where the $\ket{\Psi_i}$ are defined in terms of real vectors $\vec\psi_i$.
  Therefore, for $\estim{\varrho}$ being real and symmetric, we find
  \[
    \bra{\Psi} \estim\varrho \ket{\Psi} = \bra{\Psi_1} \estim\varrho \ket{\Psi_1} + \bra{\Psi_2} \estim\varrho \ket{\Psi_2}.
  \]
  A similar equality holds with $\estim\varrho$ replaced by $\1$ or $\sigma_i$ for $i=1,\ldots,i_d$, since the latter matrices are symmetric and real as well.
  To shorten the notation, we now define two $i_d+1$ dimensional vectors $\vec x^1$ and $\vec x^2$ with components ($\alpha=1,2$)
  \[
    \begin{split}
      x^\alpha_0 &= \frac{\sqrt{\mathcal{P}}}{d} R_2 \norm{\vec\psi_\alpha}^2 \\
      x^\alpha_i &= \sqrt{R_1^2 - R_2^2}\;v_i\left(\vec\psi_\alpha\right) \qquad (i=1,\ldots,i_d).
    \end{split}
  \]
  Since $d = \norm{\vec\psi}^2 = \norm{\vec\psi_1}^2 + \norm{\vec\psi_2}^2$, we find
  \[
  \sqrt{ \mathcal{P} R_2^2 + (R_1^2 - R_2^2) \sum_{i=1}^{i_d} v_{i}^{2}\left(\vec\psi\right)} = \norm{\vec x^1 + \vec x^2} \le \norm{\vec x^1} + \norm{\vec x^2},
  \]
  where we used triangle inequality in the last step.
  Therefore we have
  \[
    \label{eq:ellpos.g_decomp}
    g(\vec\psi) \geq  g(\vec\psi_1) + g(\vec\psi_2)
  \]
  \Cref{eq:ellpos.g_decomp} implies that if $g(\vec\psi)$ is non-negative for all real vectors then it is also non-negative for any vector $\vec\psi$.
  More intuitively, the above result is true because the construction of $g(\vec\psi)$ utilizes only the generalized $\sigma_\mathrm{X}$ Pauli matrices and the expectation values of such generalized Pauli matrices are depend on the real parts of $\vec\psi^*\otimes\vec\psi$ w.r.t.\ the fixed basis.
  The imaginary part of such projectors only show up in expectation values of the generalized $\sigma_\mathrm{Y}$ matrices.
\end{proof}

The next step of the proof, which is crucial for encoding an instance number partition problem, is the choice of the ellipsoid's center $\estim\varrho$.
We choose
\[
  \estim{\varrho}=\frac{q}{d}\1+\frac{1-q}{a^{2}} \ket{\vec a} \bra{\vec a},\qquad0\leq q\leq1,\qquad a=\norm{\vec a},
  \label{eq:ellpos.rho0}
\]
with $q \in \Reals$ to be specified below and $\ket{\vec a} =\sum_{k}a_{k}\ket{k} $ denoting a state represented by a real, \emph{integral} vector $\vec{a}$.
The latter are exactly the instances of the number partition problem~\ref{prob:complexity.partition}.
Since $\estim\varrho$ given by Eq.~\eqref{eq:ellpos.rho0} is manifestly real and symmetric, we can restrict our attention to $\vec\psi \in \Reals^d$ due to Lemma~\ref{lem:ellpos.real_min}. We find
\[
  \bra{\Psi}\estim{\varrho} \ket{\Psi} =q+\frac{1-q}{a^{2}}{\left(\vec{a}\cdot\vec{\psi}\right)}^{2},
\]
and
\[
  \sum_{i=1}^{i_{d}}v_{i}^{2}\left(\vec\psi\right)=4\sum_{1\leq j<k\leq d}\psi_{j}^{2}\psi_{k}^{2}\equiv2d^{2}-2\sum_{k=1}^{d}\psi_{k}^{4}.
\]
%with $\vec{\psi}\in\mathbb{R}^{d}$.

Before we will be ready to take an advantage of the above encoding we need to perform a sequence of tedious algebraic manipulations.
In short, the function we work with has an algebraic form $g(\vec\psi)=\kappa-\sqrt{\Delta}$, with both $\kappa$ and $\Delta$ being non-negative.
Testing if this function is non-negative is thus equivalent to checking the inequality $\kappa^2- \Delta\geq 0$. If we divide this inequality by $2(R_1^2-R_2^2)$ and fix $q=q_+$ or $q=q_-$ with
\[
  q_{\pm}=\frac{1}{2}\left(1\pm\sqrt{1-8d\left(R_{1}^{2}-R_{2}^{2}\right)\frac{a^{2}}{1+a^{2}}}\right).\label{eq:ellpos.q}
\]
we can rearrange it to the convenient form
\[
  f\left(\vec{\psi}\right)-C_{2}{\left(\vec{a}\cdot\vec{\psi}\right)}^{4}\leq C_{1},
  \label{eq:ellpos.condition}
\]
where:
\begin{align}
%  f\left(\vec{\psi}\right)=d^{2}-\sum_{k=1}^{d}\psi_{k}^{4}+\left(d-\frac{\left(\vec{a}\cdot\vec{\psi}\right)^{2}}{1+a^{2}}\right)^{2},
  f\left(\vec{\psi}\right) &= 2d^{2}-\sum_{k=1}^{d}\psi_{k}^{4}-2d\frac{{\left(\vec{a}\cdot\vec{\psi}\right)}^{2}}{1+a^{2}}, \\
  \label{eq:ellpos.def_c1}
  C_{1}&=d^{2}+\frac{1}{R_{1}^{2}-R_{2}^{2}}\left[\frac{q_{\pm}^{2}}{2}-d\left(d-1\right)R_{2}^{2}\right], \\
  C_{2}&=\frac{q_{\mp}^{2}}{2a^{4}\left(R_{1}^{2}-R_{2}^{2}\right)}>0
%+\frac{1}{\left(1+a^{2}\right)^{2}}>0.
\end{align}
Both solutions~\eqref{eq:ellpos.q} assure that~\eqref{eq:ellpos.condition} is free from additional terms proportional to ${\left(\vec{a}\cdot\vec{\psi}\right)}^{2}$, except those already included in $f$.

Hence, the original problem of deciding whether the ellipsoid $\Eps$ centered at $\estim\varrho$ and with radii~\eqref{eq:ortho.subclass} is contained in the psd states can be rephrased as deciding whether the maximum of the left hand side of Eq.~\eqref{eq:ellpos.condition} is smaller or equal to some constant:
\[
  \Eps\subset\States
  \iff
  \max_{\vec\psi\in\mathbb{S}^{d-1}_{d}}\left[f\left(\vec\psi\right)-C_{2}{\left(\vec a \cdot \vec\psi\right)}^{4}\right]\leq C_{1}.
  \label{eq:ellpos.max_condition}
\]
Here, $\mathbb{S}^{d-1}_{\zeta}$ denotes a $(d-1)$-dimensional sphere in $\Reals^d$ with radius $\sqrt\zeta$, i.e.
\[
 \vec\psi\in\mathbb{S}^{d-1}_{d} \iff    \vec\psi\in\mathbb{R}^{d}\;\wedge \; \left\Vert \vec\psi\right\Vert^{2}=d.
\]
The relation of \cref{prob:ortho.ellpos} to the number partition problem is derived in the following Lemma.
\begin{lemma}\label{lem:ellpos.gap_or_no_gap}
  If the instance $\vec a$ of \cref{prob:complexity.partition} allows for a partition, then
  \[
    \max_{\vec\psi\in\mathbb{S}^{d-1}_{d}}\left[f\left(\vec\psi\right)-C_{2}{\left( \vec a \cdot \vec\psi\right)}^{4}\right]
    = 2d^{2}-d \label{eq:ellpos.def_pi0}.
  \]
  On the other hand, if there is no such partition, we have
 \begin{align}
    \max_{\vec\psi\in\mathbb{S}^{d-1}_{d}}\left[f\left(\vec\psi\right)-C_{2}{\left( \vec a \cdot \vec\psi\right)}^{4}\right]
    &<   \max_{\vec\psi\in\mathbb{S}^{d-1}_{d}}f\left(\vec\psi\right)\label{eq:ellpos.def_pi}\\
    &\leq 2d^{2}-d - \frac{2}{p(a d)}
    \label{eq:ellpos.def_pi2}.
  \end{align}
  where $p(x)=2 x^4$ is a non-negative polynomial.
\end{lemma}
For the sake of clarity we relegate the proof of the above lemma to the end of this section and discuss its implications now.
As a consequence of Lemma~\ref{lem:ellpos.gap_or_no_gap} the choice,
\[
  C_{1}=2d^{2}-d-p{(a d)}^{-1},
  \label{eq:ellpos.choice}
\]
implies that an efficient algorithm deciding whether the inequality~\eqref{eq:ellpos.condition} is satisfied or not is also capable of deciding the number partition problem~\ref{prob:complexity.partition} efficiently.
This proves the claim of \cref{thm:ortho.hard} that \cref{prob:ortho.ellpos} is $\NP$-hard.\\


The last step we need to make is to find the parameters $R_{1}$ and $R_{2}$ leading to the choice~\eqref{eq:ellpos.choice}.
To this end, we set $R_{2}=\epsilon R_{1}$ with $0<\epsilon<1$ and introduce two positive parameters
\[
  B_{1}=p{(a d)}^{-1},\qquad B_{2}=\frac{d a^2}{1+a^2}.
\]
Note that if $1\leq j\leq d$ is such that $|a_j|=\min_k |a_k|$, then for $\vec\psi^j$ given by $\psi^j_k=\sqrt{d} \delta_{jk}$ the function $f(\vec\psi^j)$ is equal to
\[
f\left(\boldsymbol{\psi}^{j}\right)=\frac{d^{2}}{1+a^{2}}\left(1+a^{2}-2a_{j}^{2}\right).
\]
Since $a^2-2a_j^2\geq(d-2)a_j^2$ the quantity $f(\boldsymbol{\psi}^{j})$ is non-negative, so is the right hand side of Eq.~\eqref{eq:ellpos.def_pi}.
From~\eqref{eq:ellpos.def_pi2} we can find the bound
\[
  \label{eq:ellpos.b1_bound}
  B_{1} \le d^{2}-d/2.
\]
Furthermore, $B_{2} \le d$.

Rearranging Eq.~\eqref{eq:ellpos.def_c1}, taking the square root and substituting~\eqref{eq:ellpos.choice} we can see that $R_1$ is implicitly defined by the relation
\[
  \sqrt{2}\sqrt{\left(d^{2}-d-B_1\right)\left(1-\epsilon^{2}\right)+d\left(d-1\right)\epsilon^{2}}R_{1}=q_\pm.
  \label{eq:ellpos.equiv}
\]
If the left hand side of~\eqref{eq:ellpos.equiv} happens to be bigger than $1/2$, we need to take the $q_+$ solution on the right hand side (and $q_-$ in the opposite case). In order for the square roots in Eq.~\eqref{eq:ellpos.equiv} to be real-valued, we need to assume
\[
  \left(d^{2}-d-B_1\right)\left(1-\epsilon^{2}\right)+d\left(d-1\right)\epsilon^{2}\geq0.
\]
and
\[
  1-8R_{1}^{2}\left(1-\epsilon^{2}\right)B_{2}\geq0,\label{eq:ellpos.extra_cond}
\]
The latter condition assures that $q_\pm$ are real while the former condition, as it
does not depend on $R_{1}$, can be immediately solved for $\epsilon$:
\[
  \epsilon^{2}\geq1-\frac{d\left(d-1\right)}{B_{1}}.
  \label{eq:ellpos.condition_epsilon1}
\]
However, Eq.~\eqref{eq:ellpos.condition_epsilon1} does not yield a universal bound for acceptable values of $\epsilon$ since $B_1$ depends on the particular instance $\vec a$.
To obtain a lower bound independent of $\vec a$, we use Eq.~\eqref{eq:ellpos.b1_bound}, obtaining:
\[
\epsilon^2 \geq \frac{1}{2d - 1}.
\]
Since both sides of~\eqref{eq:ellpos.equiv} are non-negative, we can take the square of this relation and turn it it into a quadratic equation for $R_1$. Surprisingly, this equation has a trivial solution $R_1=0$ (only relevant while dealing with $q_-$) and a single  non-trivial solution which can be simplified to the form:
\[
  R_{1}=\frac{1}{\sqrt{2}}\frac{\sqrt{d\left(d-1\right)-B_{1}\left(1-\epsilon^{2}\right)}}{d\left(d-1\right)-\left(B_{1}-B_{2}\right)\left(1-\epsilon^{2}\right)},
  \label{eq:ellpos_r1}
\]
The condition~\eqref{eq:ellpos.extra_cond} becomes trivially satisfied, while the left hand side of Eq. (\ref{eq:ellpos.equiv}) is greater than $1/2$ (relevant for $q_+$) for
\[
  \epsilon^{2}\geq1-\frac{d\left(d-1\right)}{\left(B_{1}+B_{2}\right)}.
  \label{eq:ellpos.condtion_epsilon2}
\]
In the opposite case the inequality is reversed.
When~\eqref{eq:ellpos.condtion_epsilon2} occurs, we find that
\begin{align}
  q_{+} &= \frac{d\left(d-1\right)-B_{1}\left(1-\epsilon^{2}\right)}{d\left(d-1\right)-\left(B_{1}-B_{2}\right)\left(1-\epsilon^{2}\right)}\label{qp},\\
  q_{-} &=\frac{B_{2}\left(1-\epsilon^{2}\right)}{d\left(d-1\right)-\left(B_{1}-B_{2}\right)\left(1-\epsilon^{2}\right)},
\end{align}
while in the opposite case the parameters $q_{+}$ and $q_{-}$ swap.
These interrelations between the parameters imply that regardless of the validity of~\eqref{eq:ellpos.condtion_epsilon2}, the solution~\eqref{eq:ellpos_r1} uniquely determines  $q$ initially introduced in~\eqref{eq:ellpos.rho0} as given by the formula~\eqref{qp}. This parameter is manifestly smaller than $1$ and due to~\eqref{eq:ellpos.condition_epsilon1} it is also non-negative.
With the given choice of parameters \eqref{eq:ellpos_r1} and \eqref{eq:ellpos.condtion_epsilon2} as well as $q$ specified above, we complete the reduction from the number partition problem.
To finalize the proof of Theorem~\ref{thm:ortho.hard}, we now state the proof of Lemma~\ref{lem:ellpos.gap_or_no_gap}.
\begin{proof}[Proof of Lemma~\ref{lem:ellpos.gap_or_no_gap}]
  The first part of the proof -- Eq.~\eqref{eq:ellpos.def_pi0} --  follows from a simple calculation utilizing the partition vector $\vec\psi$ defined in~\eqref{eq:ellpos.partition_vector}. Note that as $\vec a \cdot\vec \psi=0$, we immediately obtain the first equality in~\eqref{eq:ellpos.def_pi0}, which since $C_2$ is non-negative turns into inequality in~\eqref{eq:ellpos.def_pi}.

  To prove~\eqref{eq:ellpos.def_pi2}, we define the set of all possible ($2^d$ in total) partition vectors
  \[
    \mathcal{Z} := \left\{ \vec z \in \Reals^d\colon \forall i \, z_i = \pm 1 \right\}
  \]
  and (for an arbitrary $0<\lambda<1$) the set of vectors that are ``close'' to some element from $\mathcal{Z}$
  \[
    \mathcal{B} := \left\{ \vec\psi\in\Reals^d\colon \min_{\vec z\in \mathcal{Z}} \norm{\vec\psi - \vec z} \leq \frac{\lambda}{a} \right\}.
  \]
  Because $a \geq 1$, the set $\mathcal{B}$ can be thought of as a disjoint union of $2^d$ balls centered around the elements of $\mathcal{Z}$.
  For further convenience we denote $\tilde{\vec{z}} = \mathrm{argmin}_{\vec z\in\mathcal{Z}} \, \norm{\vec\psi - \vec z}$, and $\vec\delta := \vec\psi - \tilde{\vec{z}}$.
  By construction $\tilde{z}_k=\mathrm{sign}\,\psi_k$ so that for all $k=1,\ldots,d$
  \[
    \tilde{z}_k\delta_k=\tilde{z}_k\psi_k-\tilde{z}_k^2=|\psi_k|-1\geq-1.
  \]
  Since $\norm{\vec\psi}^2=d$ we find that
  \[
    2\tilde{\vec{z}} \cdot\vec\delta = - \norm{\vec\delta}^2.
  \]
  Using all the above, the fact that $\tilde{z}_k^2=1$ and  $\tilde{z}_k^3=\tilde{z}_k$, and the Jensen inequality we can further estimate
  \[
    -\sum_{k=1}^d \psi_k^4\leq-d-\sum_{k=1}^d \delta_k^4\leq-d-\frac{\norm{\vec\delta}^4}{d}.
  \]

  As $\vec a$ does not allow for partition and both, $\tilde{\vec z}$ and $\vec a$ are integral, we must necessarily have $\vert \vec a \cdot \tilde{\vec z} \vert \geq 1$. Thus
  \[
    1\leq\left|\boldsymbol{a}\cdot\tilde{\vec z}\right|=\left|\boldsymbol{a}\cdot\left(\boldsymbol{\psi}-\boldsymbol{\delta}\right)\right|\leq\left|\boldsymbol{a}\cdot\boldsymbol{\psi}\right|+\left|\boldsymbol{a}\cdot\boldsymbol{\delta}\right|\leq\left|\boldsymbol{a}\cdot\boldsymbol{\psi}\right|+a\norm{\boldsymbol{\delta}},
  \]
  so that
  \[
    -\left|\boldsymbol{a}\cdot\boldsymbol{\psi}\right|\leq\min\left\{0,a\norm{\boldsymbol{\delta}}-1\right\},
  \]
  Taking all the above results together with $\left|\boldsymbol{a}\cdot\boldsymbol{\psi}\right|\leq a\norm{\boldsymbol{\psi}}=a\sqrt{d}$ we obtain
  \[
    f(\vec \psi)\leq 2d^{2}-d-\frac{\norm{\vec\delta}^4}{d}+2 d^{3/2}a\frac{\min\left\{0,a\norm{\boldsymbol{\delta}}-1\right\}}{1+a^{2}}.
  \]

  We will now study two cases. For $\psi \in \mathcal{B}$, we have $0\leq\norm{\vec \delta}\leq\lambda/a$, so that
  \[
    f(\vec \psi)\leq 2d^{2}-d-2 d^{3/2}a\frac{1-\lambda}{1+a^{2}},
  \]
  while for the opposite case ($\psi \notin \mathcal{B}$), when  $\norm{\vec \delta}>\lambda/a$, one finds
  \[
    f(\vec \psi)\leq 2d^{2}-d-\frac{\lambda^4}{d a^4}.
  \]
  Therefore, we have for any $\vec\psi \in \Reals^d$ with $\norm{\vec\psi}^2 = d$
  \[
    f(\vec \psi)\leq 2d^{2}-d- \min\left\{2 d^{3/2}a\frac{1-\lambda}{1+a^{2}},\frac{\lambda^4}{d a^4}\right\},
  \]
  so that by setting $\lambda=d^{-3/4}$ we obtain the desired result with $p(ad)=2{(ad)}^4$.
\end{proof}


We conclude this section by investigating a tractable special case of \cref{prob:ortho.ellpos}.
Consider $R_{i}=R$ for all $i=1,\ldots,d^{2}-1$ that is all radii are equal and the ellipsoid is a ball.
With no loss of generality, we can assume $\sigma'_i = \sigma_i$.
The following Lemma provides an easily checkable, necessary, and sufficient condition to decide \cref{prob:ortho.ellpos} for this special case.
\begin{lemma}\label{lem:ortho.spheres}
  Let $\CR$ denote a ball parameterized according to \cref{thm:ortho.ellipsoids} with with radii $R_i=R$ and midpoint $\estim{\varrho}$.
  $\CR$ is fully contained in the set of psd density matrices if and only if
  \[
    R\leq\sqrt{\frac{d}{2\left(d-1\right)}} \, \operatorname{mineig} \estim{\varrho},
  \]
  where $\operatorname{mineig}\estim{\varrho}$ denotes the smallest eigenvalue of $\estim{\varrho}$.
\end{lemma}
\begin{proof}
  To check whether a sphere with radius $R$ centered at $\estim\varrho$ is contained in the set of psd states, specialize \cref{eq:ellpos.worst_case} to the special case $R_1 = R_2$:
  \[
    \bra{\Psi}\estim{\varrho} \ket{\Psi} -R\sqrt{\sum_{i}v_{i}^2\left(\vec\psi\right)}\geq0.
   \label{eq:spheres.worst_case}
  \]
  Since for any pure state $ \ket{\Psi} $ the identity
  \[
    \sum_{i}v_{i}^{2}\left(\vec\psi\right)=\frac{2\left(d-1\right)}{d},\label{puresum-1}
  \]
  holds (Bloch vectors of pure states live on the hypersphere), the inequality in question becomes
  \[
    \bra{\Psi}\estim{\varrho} \ket{\Psi} -R\sqrt{\frac{2\left(d-1\right)}{d}}\geq0.
  \]
  Simple minimization with respect to $ \ket{\Psi} $ concludes the proof.
\end{proof}

The statement is a straightforward but interesting extension of the known result that the largest ball centered at the completely mixed state and fully contained in the set of psd density matrices has radius $R_{\mathrm{max}}=\sqrt{\frac{1}{2d\left(d-1\right)}}$.
Intuitively, when the center of the ball is moved away from the completely mixed state, the allowed radii become smaller.
This correction happens to be quantified by the smallest eigenvalue of the new center.
In conclusions, spherical ellipsoids do not constitute hard instances of \cref{prob:ortho.ellpos} provided that the minimal eigenvalue of $\estim\varrho$ can be computed efficiently with high enough accuracy.
However, the slight modification with two distinct radii considered above



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Hardness results for credible regions}
\label{sec:error.bayesian}

In this section we are going to present the second major result of~\cite{Suess_2016_Error} concerned with Bayesian credible regions in QSE.
Bayesian techniques in the context of QSE have been introduced by different authors~\cite{Jones_1991_Principles,Slater_1995_Quantum,Derka_1997_From,Schack_2001_Quantum,Buzek_1998_Reconstruction}.
The main advantage the Bayesian approach to QSE has over the more established frequentist's methods is the ability to include prior knowlege naturally into the inference procedure.
Furthermore, the Bayesian framework is conceptually simpler and it provides notion of error region with a more natural interpretation as discussed in \cref{sec:error.bayesian}.
However, analytical solutions for Bayesian inference problems only exist in special cases when we consider conjugate priors for the likelihood functions.
In~\cite{Audenaert_2009_Quantum}, the authors use the self-conjugate Gaussian priors to provide an approximative estimate of the state and the corresponding credible region.
However, in order to deal with the psd constraints, the authors had to resort to an uncontrolled approximation.
In this work we use the same model and show that the \emph{exact} inference problem under quantum shape constraints cannot be solved efficiently.
A different approach, which was pioneered in~\cite{Huszar_2012_Adaptive,Ferrie_2014_Quantum}, is to use Monte Carlo algorithms to perform approximate inference.
Such sampling algorithms also allow the computation of highest posterior density ellipsoids, which are near-optimal credible region for the posteriors relevant to QSE, i.e.\ Pauli measurements on multiple qubits~\cite{Ferrie_2014_High}.
For more details on the practical application of Bayesian inference to QSE, we refer the reader to~\cite{Granade_2016_Practicala}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{MVCR for Gaussian distributions}
\label{sub:bayesian.gaussian}

As a first step towards obtaining good credible regions for the QSE model we ignore the positivity constraints in this section.
The Gaussian approximation of the measurement statistics in \cref{eq:intro.gauss_approx} suggests that we use a Gaussian prior for $\varrho$:
On the one hand, this yields a Gaussian posterior as well and the parameters can be computed analytically by means of \emph{linear Kalman filter update equations} similar to \cref{eq:stat.bayesian.kalman} \cite[Sec.\ 2.4]{Audenaert_2009_Quantum}.
On the other hand, the MVCR for Gaussian distributions are simply ellipsoids, and therefore, can be characterized by a few parameters.
Below, we show how those parameters can be computed efficiently.

Since credible regions are purely defined in terms of the posterior, we can ignore the details how the posterior arose.
Hence, we assume that the posterior distribution of $\varrho$ under consideration is a Gaussian with mean $\theta$ and covariance matrix $\Sigma$.
In other words, we assume the posterior for $\varrho$ has probability density
\[
  \label{eq:bayesian.gaussian_density}
  \Pdf_{\theta,\Sigma}(\varrho) = {(2\pi)}^{-\Nhalf} \abs{\Sigma}^{-\frac{1}{2}} \exp\left( -\frac{1}{2} \norm{\varrho - \theta}_\Sigma^2 \right).
\]
where
\[
  \norm{\varrho - \theta}_\Sigma := \sqrt{{(\varrho - \theta)}^T \Sigma^{-1} (\varrho - \theta)}
\]
is the Mahalanobis distance and  $\abs{\Sigma}$ denotes the determinant of $\Sigma$.
As elaborated in \cref{sub:stat.bayesian}, the MVCRs are exactly the highest posterior density sets as defined in \cref{eq:stat.bayesian.mvcr}.
Therefore, the MVCR with credibility $\alpha$ for the Gaussian posterior~\eqref{eq:bayesian.gaussian_density} is given by
\[
  \label{eq:bayesian.gaussian_cr}
  \CR = \{ \vec x \in \Reals^N\colon \norm{\vec x - \vec\theta}_\Sigma \le r_{\alpha} \} =: \Eps(r_{\alpha}).
\]
This is an ellipsoid centered at $\vec\theta$ with radius $r_{\alpha}$ determined by the saturated credibility condition~\eqref{eq:stat.bayesian.credibility}:
\[
  \label{eq:bayesian.radius}
  \begin{split}
    \alpha
    &= {(2\pi)}^{-\Nhalf} \abs{\Sigma}^{-\frac{1}{2}} \int_{\Eps(r_{\alpha})} \exp\left( -\frac{1}{2} \norm{\vec x - \vec\theta}_\Sigma^2 \right) \dd^N x \\
    &= \frac{\gamma\left( \Nhalf, \frac{r^2_{\alpha}}{2} \right)}{\Gamma\left( \Nhalf \right)}
    \equiv P\left( \Nhalf, \tfrac{r^2_{\alpha}}{2} \right).
    \end{split}
\]
By $\gamma(\cdot,\cdot)$ we denote the incomplete $\Gamma$-function and $P(\cdot,\cdot)$ is its normalized version.
The above condition fixes $r_{\alpha}$ uniquely since $x \mapsto P(\Nhalf, x)$ is strictly monotonic for any $N > 0$.
Hence, determining the MVCR for a multivariate Gaussian posterior with known mean and covariances reduces to computing the radius $r_\alpha$, which is formalized in the following problem.
\begin{problem}\label{prob:bayesian.cr}
  For given mean $\vec\theta \in \Reals^N$, covariance matrix $\Sigma \in \Reals^{N \times N}$ with $\Sigma \geq 0$, credibility $\alpha \in [0,1]$, and accuracy $\delta$ with $\delta^{-1} \in \mathbb{N}$, determine the radius of the MVCR $r_{\alpha}$ defined in Eq.~\eqref{eq:bayesian.radius} with given accuracy.
\end{problem}
Note that this does not constitute a decision problem, but an algebraic computation:
We are asking to compute a number $y$ such that
\[
  \label{eq:bayesian.precission}
  y \in (r_\alpha - \delta, r_\alpha + \delta).
\]
Although there are elaborate algebraic computational models~\cite[Sec.\ 16]{Arora_2009_Computational}, we are going to use the following simpler strategy here:
We are going to consider algorithms for problems such as \cref{prob:bayesian.cr} that compute a rational number $y$ with binary encoding of its numerator and denominator.
Such an algorithm exists for solving \cref{prob:bayesian.cr} that runs in polynomial time in the sense of \cref{def:complexity.runtime}.
With the shorthand notation $x = r^2_{\alpha}/2$, the algorithm is outlined below:
\begin{enumerate}
  \item W.l.o.g.\ we can assume that $\alpha \le 0.9$ (or some other arbitrary constant).
  Otherwise, the problem can be restated in terms of $Q(\Nhalf, x) = 1 - P(\Nhalf, x)$, which allows for a similar analysis.
  The condition $\alpha \le 0.9$ restricts the search space for $x$ to some finite interval $[0, t_\mathrm{max}]$.
  Note that the upper bound $t_\mathrm{max}$ grows at worst polynomially in $\Nhalf$.
  \item The above restriction, the finite precision, and the fact that $x \mapsto P(\Nhalf, x)$ is strictly monotonic allow for interpreting the problem of finding $x$ given $\alpha$ as a search in an ordered, finite list of size $M \sim \tfrac{t_\mathrm{max}}{\delta}$.
  \item Each entry of this list can be evaluated with exponential precision in polynomial time using a power series expansion of $P(\Nhalf, x)$ (for more details see Lemma~\ref{lem:bayesian.normalization_constant} in~\ref{sub:error.proof_bayesina}).
  \item Since finding $x$ in this list only requires $\log M$ evaluations using binary search, the whole problem can be solved in polynomial time.
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bayesian QSE}
\label{sub:bayesian.tomography}

In order to incorporate the positivity constraints on $\varrho$ imposed by quantum mechanics, we choose a prior distribution that is concentrated on $\States$ and vanishes on its complement.
One possible choice are truncated Gaussian priors.
These are defined in terms of their density $\Pdf_{\theta,\Sigma}^+(\varrho)$ with respect to the flat Hilbert-Schmidt measure $\dd\varrho$ on $\HermTrace$
\[
  \label{eq:bayesian.density_plus}
  \Pdf^+_{\theta,\Sigma}(\varrho) = C_{\theta,\Sigma}\ \chi(\varrho)\ \Pdf_{\theta,\Sigma}(\varrho).
\]
Here, $\Pdf_{\theta,\Sigma}$ is the multivariate Gaussian from Eq.~\eqref{eq:bayesian.gaussian_density} with $\theta \in \HermTrace$.
The other factors in Eq.~\eqref{eq:bayesian.density_plus} ensure that $\Pdf^+_{\theta,\Sigma}$ is a proper probability distribution supported on $\States$:
$\chi(\varrho)$ is the indicator function of $\States$ and $C_{\theta,\Sigma}$ is the normalization constant defined by
\[
  \label{eq:bayes.normalization_factor}
  C_{\theta,\Sigma}^{-1} = \int_{\States} \Pdf_{\theta,\Sigma}(\varrho) \,\dd\varrho.
\]
Since the numerator in Bayes rule~\eqref{eq:stat.bayesian.bayes_rule} is linear in the prior, the posterior distribution updated with Gaussian likelihood function is also of the form~\eqref{eq:bayesian.density_plus}.
Furthermore, we can use the same linear Kalman filter update equations for both the standard and the truncated Gaussian distributions.
The only additional complication of computing the posterior corresponding to the truncated prior~\eqref{eq:bayesian.density_plus} is that the normalization factor $C_{\theta,\Sigma}$ needs to be reevaluated after each update step in order to obtain a properly normalized probability distribution.
From now on we only consider a fixed posterior distribution and drop the subscripts indicating the mean $\theta$ and the covariance matrix $\Sigma$ if no confusion arises.
It is then important to remember that the constant in question is denoted by $C$, while the credibility region is $\CR$.

The problem we try to solve is the following:
Given the mean $\theta$, covariance matrix $\Sigma$, and credibility $\alpha$, can we find the MVCR for the truncated Gaussian distribution supported on $\States$?
Since the truncated density~\eqref{eq:bayesian.density_plus} is supported on the psd states and MVCRs are highest-density sets due to~\eqref{eq:stat.bayesian.mvcr}, the MVCR is of the form
\[
  \Eps(r^+_{\alpha}) \cap \States = \{ \varrho \in \States \colon \norm{\varrho - \theta}_\Sigma \le r^+_{\alpha} \}.
  \label{eq:bayesian.mvcr}
\]
Similar to Eq.~\eqref{eq:bayesian.radius}, the radius is determined by the credibility condition
\[
  \label{eq:bayesian.radius_plus}
  \alpha = C \int_{\Eps(r^+_{\alpha}) \cap \States} \Pdf_{\theta,\Sigma}(\varrho) \dd \varrho.
\]
However, this case involves the normalization constant $C$ from~\eqref{eq:bayesian.density_plus} and the integral is restricted to the psd states.
Also, there is no closed-form analogue to Eq.~\eqref{eq:bayesian.radius} due to the psd constraint.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Computational Intractability}
\label{sub:bayesian.hard}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
  \centering
  \input{tikz/error_ellipsoids.tex}
  \caption{\label{fig:bayesian.ellipsoids}
    The two possible cases for the credible regions.
    \emph{Left:} The original ellipsoid $\Eps(r_\frac{\alpha}{C})$ with credibility $\frac{\alpha}{C}$ (yellow) lies completely inside the psd states and is, therefore, equal to the ellipsoid taking into account positivity $\Eps(r^+_{\alpha})$ with credibility $\alpha$ (blue hatched).
    \emph{Right:}  Parts of the original ellipsoid $\Eps(r_\frac{\alpha}{C})$ lie outside the psd states (blue).
    Hence, the ellipsoid that takes into account positivity $\Eps(r^+_{\alpha})$ has to have a larger radius in order to achieve the sought for credibility.
  }
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Our main result from this section concerns MVCR for Gaussian posteriors that are fully supported on the psd states.
We will show that the following problem is computational hard.
\begin{problem}\label{prob:bayesian.trucated_cr}
  For given mean $\theta \in \HermTrace$, covariance matrix $\Sigma$, credibility $\alpha \in [0,1]$, and accuracy $\delta$ with $\delta^{-1} \in \mathbb{N}$, determine the radius of the MVCR $r^+_{\alpha}$ defined in Eq.~\eqref{eq:bayesian.radius_plus} with given accuracy.
\end{problem}
In other words, there is no efficient algorithm that outputs smallest volume credibility regions for every Gaussian distribution on $\HermTrace$ restricted to the positive semidefinite states and every credibility $\alpha$.
Consequently, there cannot be an efficient algorithm to solve the problem of MVCR for QSE, since the latter more general problem contains the instances of \cref{prob:bayesian.trucated_cr}.
To prove \cref{prob:bayesian.trucated_cr}, we use a reduction from \cref{prob:ortho.ellpos}, which has already been shown to be $\NP$-complete.
This reduction runs along the following lines:
\begin{enumerate}
  \item Assume that Prob.~\ref{prob:bayesian.trucated_cr} can be solved efficiently.

  \item As we will prove later, every ellipsoid $\Eps^*$ in $\HermTrace$ can be encoded as a minimum volume credible ellipsoid for some Gaussian distribution $\Pdf$ with a suitable choice of $\theta$, $\Sigma$, and $R$:
  \[
    \Eps^* = \Eps_{\theta,\Sigma}(R).
  \]
  Note that only $\theta$ is uniquely defined.
  $\Sigma$ is defined only up to a multiplicative, positive constant, since every rescaling of $\Sigma$ can be compensated by an appropriate rescaling of $R$.

  \item  Using the assumed efficient algorithm for Prob.~\ref{prob:bayesian.trucated_cr}, we can compute the normalization constant $C$ of the truncated distribution~\eqref{eq:bayesian.density_plus} for given $\theta$ and $\Sigma$ with sufficient precision in polynomial time.

  \item Based on this, we can compute a credibility $\alpha$ such that $R = r_\frac{\alpha}{C}$ and, therefore,
  \[
    \Eps^* = \Eps_{\theta,\Sigma}(r_\frac{\alpha}{C}).
  \]

  \item The crucial observation is that this ellipsoid is contained in the psd states if and only if the corresponding MVCR for the truncated distribution $\Pdf^+$ fulfills
  \[
    \label{eq:bayesian.criterion}
    r^+_{\alpha} = r_\frac{\alpha}{C}.
  \]
  See Fig.~\ref{fig:bayesian.ellipsoids} for an illustration.
  Since we can compute $r^+_{\alpha}$ efficiently by assumption, checking Eq.~\eqref{eq:bayesian.criterion} allows us to decide Prob.~\ref{prob:ortho.ellpos}.
\end{enumerate}

In conclusion, the main result from this section is the following lower bound on the computational complexity of Problem~\ref{prob:bayesian.cr}.
\begin{theorem}\label{thm:bayesian.hardness}
  If Problem~\ref{prob:bayesian.trucated_cr} has a polynomial time algorithm, then we can also decide Problem~\ref{prob:ortho.ellpos} in polynomial time.
  Therefore, there is no efficient algorithm for Problem~\ref{prob:bayesian.trucated_cr} unless $\mathrm{P} = \mathrm{NP}$.
\end{theorem}

The proof runs along the lines outlined above and can be found in the next section.
The main technical problem is that we are dealing with finite-precision arithmetic.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Theorem~\ref{thm:bayesian.hardness}}%
\label{sub:error.proof_bayesina}

Let us now construct the polynomial time reduction of \cref{prob:ortho.ellpos} to \cref{prob:bayesian.cr}.
We will begin with the main observation of this proof, namely Eq.~\eqref{eq:bayesian.criterion}.
\begin{lemma}\label{lem:bayesian.criterion}
  Let $\Pdf(\varrho)$ denote a Gaussian distribution on $\HermTrace$ and $\Pdf^+(\varrho) = C \Pdf(\varrho) \chi(\varrho)$ the corresponding restricted Gaussian with the same mean and covariance matrix, as defined in Eq.~\eqref{eq:bayesian.density_plus}.
  For any $\alpha \in [0,1]$, the credible ellipsoid $\Eps(r_\frac{\alpha}{C})$ with credibility $\frac{\alpha}{C}$ is contained in the psd if and only if the credible ellipsoid for $\Pdf^+$, $\Eps(r^+_{\alpha})$, with credibility $\alpha$ has the same radius, that is Eq.~\eqref{eq:bayesian.criterion} holds.
\end{lemma}
\begin{proof}
  The two cases of  $\Eps(r_\frac{\alpha}{C})$ being contained and not being contained in the psd states are illustrated in Fig.~\ref{fig:bayesian.ellipsoids}.
  First, assume that $\Eps(r_\frac{\alpha}{C}) \subset \States$, then
  \[
    \frac{\alpha}{C} = \int_{\Eps(r_\frac{\alpha}{C})} \Pdf(\varrho) \, \dd \varrho.
    \implies
    \alpha = \int_{\Eps(r_\frac{\alpha}{C}) \cap \States } C \Pdf(\varrho) \, \dd \varrho.
  \]
  Note that the right equation is exactly the defining Eq.~\eqref{eq:bayesian.radius_plus} for the positive radius $r^+_{\alpha}$ if $r^+_\alpha = r_\frac{\alpha}{C}$.

  Now, assume that a part of the ellipsoid $O = \Eps(r_\frac{\alpha}{C})  \setminus \States \neq \emptyset$ lies outside the psd states.
  Then, as can be seen on the right side of Fig.~\ref{fig:bayesian.ellipsoids}, we need to enlarge $r^+_{\alpha}$ to compensate for the lost probability weight of $O$.
  The latter cannot be vanishing, since the Gaussian density $\Pdf(\varrho)$ is strictly positive.
  Therefore, $r^+_\alpha > r_\frac{\alpha}{C}$ in this case.
\end{proof}
Of course, the difference between $r_\frac{\alpha}{C}$ and $r^+_{\alpha}$ may in general become too small to be efficiently detectable.
However, we will show that for the instances of the number partition problem encoded in Problem~\ref{prob:ortho.ellpos}, this is not the case.
A first step toward this is the following Lemma.

\begin{lemma}\label{lem:bayesian.positivity_violation}
  Let $\vec a \in \mathbb{N}^d$ be an instance of the number partition problem~\ref{prob:complexity.partition} and
  \[
    \label{eq:bayesian.positivity_violation.ellipsoid}
    \Eps_{\vec{a}} = \left\{ \varrho_0 + R_1 \sum_{i=1}^{i_d} u_i \sigma_i^ + R_2 \sum_{i=i_d + 1}^{d^2 - 1} u_i \sigma_i \colon \vec u^T \vec u \le 1 \right\}
  \]
  the corresponding encoding ellipsoid for Problem~\ref{prob:ortho.ellpos} defined in \cref{sub:error.ellpos}.
  Then, there exists a polynomial $\tilde p$ such that if $\Eps_{\vec{a}}$ is not a subset of $\States$, there is an element $\varrho \in \Eps_{\vec{a}}$ with
  \[
    \mathrm{mineig}(\varrho) \le -\tilde p{(\norm{\vec a})}^{-1} < 0.
  \]
\end{lemma}
\begin{proof}
  The main proof idea is to trace back the proof for polynomial gap in Lemma~\ref{lem:ellpos.gap_or_no_gap}.
  Recall that Eqs.~\eqref{eq:ellpos.def_pi0} and~\eqref{eq:ellpos.choice} ensure that if $\vec a$ has a partition, there is a $\vec\Psi \in {\{\pm 1\}}^d$ such that $\vec a \cdot \vec\Psi = 0$ and
  \[
    d^2 - \sum_k \psi_k^4 + {\left( d - \frac{{(\vec a \cdot \vec \psi)}^2}{1 + \norm{\vec a}^2} \right)}^2 - C_2 {(\vec a \cdot \vec \psi)}^4 = C_1 + p{(\norm{\vec a})}^{-1}.
  \]
  By tracing back the steps which lead to this equation, we find for $\ket{\Psi} := \sum_{k=1}^d \psi_k / \sqrt{d} \ket{k}$
  \begin{align}
    \label{eq:bayesian.posviol_1}
    \frac{2 (R_1^2 - R_2^2)}{d} \, p{(\norm{\vec a})}^{-1} + \bra{\Psi} \varrho_0 \ket{\Psi}^2 \\
    = R_1^2 \sum_i {\left( \bra\Psi \sigma_i^{(x)} \ket\Psi \right)}^2 + R_2^2 \sum_i {\left( \bra\Psi \sigma_i^{(y,z)} \ket\Psi \right)}^2 \\
    =: \sum_i R_i^2 {\left( \bra\Psi \sigma_i \ket\Psi \right)}^2
  \end{align}
  Due to the special choice for $\varrho_0$ in~\eqref{eq:ellpos.rho0} and $\vec a \cdot \vec \psi = 0$, we have
  \[
    \bra\Psi\varrho_0\ket\Psi = \frac{q}{d}
  \]
  with $q$ defined in~\eqref{eq:ellpos.q}.
  Therefore, we can rewrite Eq.~\eqref{eq:bayesian.posviol_1} as
  \begin{align}
    \bra\Psi \varrho_0 \ket\Psi - \sqrt{\sum_i R_i^2 \bra\Psi \sigma_i \ket\Psi^2}
    &&= \frac{q}{d} \left( 1 - \sqrt{1 + \frac{2d (R_1^2 - R_2^2)}{q^2 \, p(\norm{\vec a})}} \right) \nonumber\\
    &&\le - \min \left( \frac{R_1^2 - R_2^2}{2 q \, p(\norm{\vec a})},\, \frac{2q}{d} \right)
    \label{eq:bayesian.posviol_2}
  \end{align}
  where we have used
  \[
    1 - \sqrt{1 + x^2} \le
    \left\{
      \begin{array}{ll}
        -x^2 / 4 \quad &  x \le 2 \sqrt{2} \\
       -2 & x > 2 \sqrt{2} \\
      \end{array}
    \right.
  \]
  Since all the constants on the right hand side of Eq.~\eqref{eq:bayesian.posviol_2} can be expressed as polynomials in the input, it defines the polynomial $\tilde p(\norm{\vec a})$ of the lemma.
  The left hand side of \cref{eq:bayesian.posviol_2} is equal to $\bra\Psi \varrho \ket\Psi$, where
  \[
    \varrho = \varrho_0 + \sum_i R_i u_i \sigma_i \in \Eps_{\vec{a}}
  \]
  for the special choice of $u$ from~\eqref{eq:ellpos.positivity}.
  The claim of the lemma follows for this $\varrho$ using Eq.~\eqref{eq:bayesian.posviol_2}.
\end{proof}

We will now show how the explicitly parameterized ellipsoid~\eqref{eq:bayesian.positivity_violation.ellipsoid} can be encoded as a MVCR-ellipsoid of a Gaussian distribution.

\begin{lemma}\label{lem:bayesian.encoded_ellipsoid}
  Denote by
  \[
   \Eps^* = \left\{ \varrho_0 + \sum_{i=1}^{d^2 - 1} u_i R_i \sigma_i \colon \norm{\vec u}_2 = 1 \right\}
  \]
  an ellipsoid $\Eps^* \subset \HermTrace$, which is axis-aligned with the coordinate axes defined by the generalized Pauli operators.
  Then, $\Eps^*$ can be encoded as a $\frac{\alpha}{C}$ MVCR-ellipsoid for a Gaussian distribution with mean $\varrho_0 \in \States$ and covariance matrix $\Sigma$.
  The latter is diagonal in the generalized Bloch basis $\sigma_i$ with entries $\Sigma_{ij} = R_i^2 \delta_{ij}$ and for the corresponding radius we have $r_\frac{\alpha}{C} = \sqrt{2}$.
  Hence, the credibility is given by
  \[
    \label{eq:bayesian.encoded_ellipsoid.credibility}
    \alpha = C\, P\left(\Nhalf, 1\right),
  \]
  which can be calculated efficiently with exponential precision for given $C$ and $N$.
 \end{lemma}
 \begin{proof}
   Since the generalized Pauli operators form an orthogonal system with $\tr (\sigma_i\sigma_j) = 2 \delta_{ij}$, we find for $\varrho \in \Eps^*$
   \[
      \norm{\varrho}^2_2 = \sum_{i,j} u_i u_j \, R_i R_j \, {(\Sigma^{-1})}_{ij} \, 2 \delta_{ij} = 2 \norm{\vec u}_2^2.
   \]
   Therefore, $\Eps^* = \Eps(\sqrt{2})$ with mean $\varrho_0$ and the stated covariance matrix.
   The efficient computation of the credibility~\eqref{eq:bayesian.encoded_ellipsoid.credibility} is given later in the proof of Lemma~\ref{lem:bayesian.always_contained}.
 \end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
  \centering
  \input{tikz/error_volume_truncation.tex}
  \caption{\label{fig:bayesian.r_separation}
    Same as Fig.~\ref{fig:bayesian.ellipsoids} (right).
    Note that the solid blue and hatched blue regions need to have the same volume.
  }
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Based on the gap proven in Lemma~\ref{lem:bayesian.positivity_violation}, we will now turn to the following question:
In case Eq.~\eqref{eq:bayesian.criterion} does not hold -- that is the corresponding ellipsoid is not fully contained in the psd states -- is the corresponding gap always large enough to be efficiently detectable?
\begin{lemma}\label{lem:bayesian.r_separation}
   Let $\vec a \in \mathbb{N}^d$ be an instance of the number partition problem and denote by $\Eps_{\vec{a}}$ the corresponding encoding ellipsoid as given by Eq.~\eqref{eq:bayesian.positivity_violation.ellipsoid}.
  Furthermore, denote by $\Pdf_{\varrho_0,\Sigma}$ the Gaussian density, which encodes $\Eps_{\vec{a}} = \Eps(r_\frac{\alpha}{C})$ as an $\frac{\alpha}{C}$ credible region as given by Lemma~\ref{lem:bayesian.encoded_ellipsoid}.
  Assume that $\vec a$ has a balanced sum partition and, therefore, $\Eps_{\vec{a}}$ is not a subset of $\States$.

  Then, there exists a polynomial $p$ such that
  \[
    {r^+_{\alpha}}^2 - {r_\frac{\alpha}{C}}^2 \ge 2^{-p(\log \norm{\vec a}_1)}.
  \]
  Here, $\norm{\vec a}_1 = \sum_k \abs{a_k}$.
  In words, the gap of violation of Eq.~\eqref{eq:bayesian.criterion} can only become polynomially small in the logarithm of the size of the problem specification.
\end{lemma}
\begin{proof}
  First, let us lower bound the volume of $\Eps(r_\frac{\alpha}{C})$ that lies outside the psd states (the solid blue region in Fig.~\ref{fig:bayesian.r_separation}).
  From Lemma~\ref{lem:bayesian.positivity_violation} we know, that there exists a $\varrho\in \Eps(r_\frac{\alpha}{C})$ with smallest eigenvalue smaller than $- \tilde p{(\norm{\vec a})}^{-1}$ for some polynomial $\tilde p$.
  This also gives us a lower bound on
  \[
    \mathrm{dist}(\varrho,\States) = \inf_{\varrho' \in \States} \norm{\varrho - \varrho'}_2.
  \]
  From~\cite[Theorem~III.2.8]{Bhatia_1997_Matrix} we know that for every $\varrho_+ \in \States$ the following bound holds:
  \[
    \begin{split}
      \norm{\varrho - \varrho_+}_2
      &\ge \norm{\varrho - \varrho_-}_\infty
      \ge \norm{\vec\lambda^\uparrow(\varrho) - \vec\lambda^\uparrow(\varrho_+)}_2 \\
      &\ge \abs{\mineig(\varrho) - \mineig(\varrho_+)}
      \ge \tilde p{(\norm{\vec a})}^{-1}.
    \end{split}
  \]
  Here, $\vec\lambda^\uparrow(\rho)$ denotes the vector of eigenvalues of $\rho$ in ascending order.
  Therefore,
  \[
    \label{eq:bayesian.r_separation.dist}
    \mathrm{dist}(\varrho,\States) \ge \tilde p{(\norm{\vec a})}^{-1}.
  \]
  This allows us to lower bound the volume of $\Eps(r_\frac{\alpha}{C})$ that lies outside the psd states by an ellipsoid with the same covariance, but radius ${(2\, \tilde p(\norm{\vec a}) \, \maxeig(\Sigma))}^{-1}$
  \begin{align}
    \label{eq:bayesian.r_separation.volume}
    \mathrm{Vol}\left( \Eps(r_\frac{\alpha}{C}) \setminus \States \right)
    &\ge \frac{\pi^{\Nhalf} \abs{\Sigma}}{\Gamma(\Nhalf + 1)} \, \frac{1}{{\left( 2 \tilde p(\norm{\vec a}) \, \maxeig(\Sigma) \right)}^{N}} \\
  \end{align}
  Furthermore, we have
  \[
    \label{eq:bayesian.r_separation.volume2}
    \mathrm{Vol}\left(\Eps(r^+_{1-\alpha}) \setminus \Eps(r_\frac{1-\alpha}{C}) \right)
    = \mathrm{Vol}\left( \Eps(r_\frac{1-\alpha}{C}) \setminus \States \right)
  \]
  since the solid blue and hatched blue regions in Fig.~\ref{fig:bayesian.r_separation} must be of same size.
  We now relate the volume inequality~\eqref{eq:bayesian.r_separation.volume} to a lower bound for the mass of the ellipsoid outside the psd states w.r.t.\ the Gaussian density:
  Due to the set of states $\States$ having finite radius $\sqrt{\tfrac{2(d-1)}{d}}$~\cite[Eq.~(18)]{Kimura_2003_Bloch}, we must have $r^+_{\alpha} \le 2 \sqrt{2}$.
  Therefore,
  \begin{align}
    P\left( \Nhalf, \tfrac{{r^+_{\alpha}}^2}{2} \right) - P\left( \Nhalf, \tfrac{{r_\frac{\alpha}{C}}^2}{2} \right)
    &= \frac{1}{ {(2\pi)}^{\frac{N}{2}} \, \abs{\Sigma}^{\frac{1}{2}} } \, \int_{\Eps(r^+_{\alpha}) \setminus \Eps(r_\frac{\alpha}{C})} \mathrm{e}^{-\frac{1}{2} \norm{\varrho - \varrho_0}^2} \dd^N \varrho \\
    &\ge \frac{\mathrm{e}^{-4}}{{(2\pi)}^{\frac{N}{2}} \, \abs{\Sigma}^{\frac{1}{2}}} \, \mathrm{Vol}\left(  \Eps(r^+_{\alpha}) \setminus \Eps(r_\frac{\alpha}{C})  \right) \\
    &\ge \frac{\mathrm{e}^{-4} \pi^{\Nhalf} \, \abs{\Sigma}^{\frac{1}{2}}}{2^\frac{N}{2} \Gamma(\Nhalf + 1)} \, \frac{1}{{\left( 2 \tilde p(\norm{\vec a}) \, \maxeig(\Sigma) \right)}^{N}} \\
    &=: 2^{-p(\log \norm{\vec a}_1) - 1}
    \label{eq:bayesian.r_separation.p_diff}
  \end{align}
  Finally, note that the following crude inequality
  \[
     P\left( \Nhalf, \tfrac{{r^+_{\alpha}}^2}{2} \right) - P\left( \Nhalf, \tfrac{{r_\frac{\alpha}{C}}^2}{2} \right)
     = \int_y^x \frac{t^{\Nhalf - 1} \ee^{-t}}{\Gamma(\Nhalf + 1)} \,\dd t \le x - y
  \]
  holds for $x \ge y$, since the integrand is less than 1.
  Therefore, with Eq.~\eqref{eq:bayesian.r_separation.p_diff}
  \[
    {r^+_{\alpha}}^2 - {r_\frac{\alpha}{C}}^2 \ge 2^{-p(\log \norm{\vec a}_1)},
  \]
  which proofs the claim.
\end{proof}


We now turn to the problem of computing the normalization constant $C$ for the restricted Gaussian distribution~\eqref{eq:bayesian.density_plus}.
First, we efficiently compute a credibility $\alpha' \in [0,1]$ such that the corresponding credible ellipsoid $\Eps(r_\frac{\alpha'}{C})$ is guaranteed to be contained in the psd states without knowing the value of $C$.
This allows us to leverage Eq.~\eqref{eq:bayesian.criterion} to compute $C$.

\begin{lemma}\label{lem:bayesian.always_contained}
  Let $\vec a \in \mathbb{N}^d$ be an instance of the number partition problem and denote by $\Eps_{\vec{a}}$ the corresponding encoding ellipsoid as defined by Eq.~\eqref{eq:bayesian.positivity_violation.ellipsoid}.
  Denote by $\Pdf_{\varrho_0,\Sigma}$ the Gaussian density, which encodes $\Eps_{\vec{a}}$ as an $\alpha$ credible region according to Lemma~\ref{lem:bayesian.encoded_ellipsoid}.
  Then, the ellipsoid $\Eps(r)$ is fully contained in the psd states provided
  \[
    \label{eq:bayesian.always_contained}
    r \le \sqrt{\frac{d}{2(d-1)}} \, \frac{\mineig\varrho_0}{\sqrt{\maxeig\Sigma}}
  \]
\end{lemma}
\begin{proof}
  We know that for any $\varrho \in \Eps(r)$ with $r$ fulfilling~\eqref{eq:bayesian.always_contained} the following inequalities hold
  \begin{align*}
    \norm{\varrho - \varrho_0}
    &\le \frac{1}{\sqrt{\mineig\Sigma^{-1}}}\, \norm{\varrho - \varrho_0}_\Sigma \\
    &\le \frac{1}{\sqrt{\mineig\Sigma^{-1}}}\, r \\
    &\le \sqrt{\frac{d}{2(d-1)}}\, \mineig \varrho_0.
  \end{align*}
  Here, we have used $\mineig\Sigma^{-1} = {(\maxeig \Sigma)}^{-1}$, which holds for any positive definite matrix $\Sigma$.
  Therefore, $\Eps(r) \subset \States$ due to Lemma~\ref{lem:ortho.spheres}.
\end{proof}


\begin{lemma}\label{lem:bayesian.normalization_constant}
  Using the same notation as Lem.~\ref{lem:bayesian.always_contained} and assuming Prob.~\ref{prob:bayesian.trucated_cr} can be solved efficiently.
  Then, for every instance $\vec a$ of the number partition problem consider the corresponding ellipsoid encoding distribution according to \cref{lem:bayesian.encoded_ellipsoid} with parameters $\theta, \Sigma$.
  Then, we can efficiently approximate the normalization constant $C$ of $\Pdf^+_{\theta,\Sigma}$ with exponentially small multiplicative error.
  More precisely, we have
  \[
    C = \tilde C (1 + \epsilon),
  \]
  where $\tilde C$ can be computed in polynomial time making the correction term $\epsilon$ exponentially small.
\end{lemma}
\begin{proof}
  Due to Lemma~\ref{lem:bayesian.always_contained} and $\mineig\theta > 0$, we can always find an $r > 0$ such that $\Eps(r)$ is fully contained in the psd.
  Indeed, the eigenvalues of $\theta$ and $\Sigma$ are readily calculated because of their particular simple form in Eq.~\eqref{eq:ellpos.rho0} and Lemma~\ref{lem:bayesian.encoded_ellipsoid}:
  \[
    \sqrt{\frac{d}{2(d-1)}} \, \frac{\mineig\theta}{\sqrt{\maxeig\Sigma}}
    =  \frac{q}{R_1 \sqrt{2d(d-1)}}
  \]
  Set\footnote{%
    Note that $\alpha$ does not denote the credibility used for encoding the ellipsoid in question, but an auxiliary ellipsoid used for computing $C$ here.
  }
  \[
    \alpha := P\left( \Nhalf, \tfrac{r^2}{2} \right).
  \]
  Since we can choose $r$ as small as we want, we may assume that $x = \frac{r^2}{2} \ll 1 < \Nhalf$.
  In this regime, we can expand the normalized incomplete $\Gamma$-function $P$ in a power series~\cite{Gil_2012_Efficient}
  \[
    \label{eq:bayesian.normalization_constant.incomplete_gamma}
    P\left( \Nhalf, x \right) = \frac{x^{\Nhalf} \ee^{-x}}{\Gamma\left( \Nhalf + 1 \right)} \sum_{k=0}^\infty \frac{x^k}{{\left( \Nhalf + 1 \right)}_k},
  \]
  where
  \[
    {\left( \Nhalf + 1 \right)}_k = \frac{\Gamma\left( \Nhalf + k + 1 \right)}{\Gamma\left( \Nhalf + 1 \right)}.
  \]
  Truncating the series in Eq.~\eqref{eq:bayesian.normalization_constant.incomplete_gamma} for $k \ge k_0$
  \[
    \label{eq:bayesian.normalization_constant.series}
    P\left( \Nhalf,x \right) = P_{k_0}\left( \Nhalf,x \right) + R_{k_0}\left( \Nhalf,x \right),
  \]
  with
  \[
    P_{k_0}\left( \Nhalf,x \right)
    = \frac{x^{\Nhalf} \ee^{-x}}{\Gamma\left( \Nhalf + 1 \right)} \sum_{k=0}^{k_0} \frac{x^k}{{\left( \Nhalf + 1 \right)}_k}
  \]
  we can derive a bound on the truncation error $R_{k_0}(\Nhalf,x)$~\cite[Eq.~(2.18)]{Gil_2012_Efficient}
  \[
    R_{k_0}(\Nhalf,x) \le \frac{x^{\Nhalf + k_0} \ee^{-x}}{\Gamma(\Nhalf + k_0 + 1)}\, \frac{\Nhalf + k_0}{\Nhalf + k_0 - x - 1}.
  \]
  Since $x \ll 1$, the term $x^{k_0}$ ensures that we can make the error in computing $\alpha$ exponentially small using only polynomial time in evaluating $P_{k_0}(\Nhalf, x)$.


  % Trace the error \delta -> error bound for r^+_{1-\alpha}
  Now, assume that we have computed $\tilde\alpha = \alpha-\epsilon$ for some truncation error $\epsilon = R_{k_0}(\Nhalf,x) > 0$.
  We may now use the postulated efficient algorithm for Prob.~\ref{prob:bayesian.trucated_cr} to compute the radius of the manifestly positive MVCR $r^+_{\tilde\alpha}$ and, hence, using Eq.~\eqref{eq:bayesian.criterion} the normalization constant:
  Since $C>1$, we have with $r_{\alpha} = r$
  \[
    r_\frac{\tilde\alpha}{C} = r_\frac{\alpha-\epsilon}{C} < r_{\alpha} \implies \Eps(r_\frac{\tilde\alpha}{C}) \subset \States \implies r_\frac{\tilde\alpha}{C} = r^+_{\tilde\alpha} \le r_{\alpha}.
  \]
  Therefore, the ellipsoid with radius $r^+_{\tilde\alpha}$ is also contained in the psd states.
  The same holds true if we replace $r^+_{\tilde\alpha}$ by the actual output $r^+_{\tilde\alpha} \pm \delta$ of the postulated efficient algorithm for Prob.~\ref{prob:bayesian.cr}
  Here, $\delta$ denotes the accuracy that is part of the input to the problem of computing $r^+_{\tilde\alpha}$.
  By choosing $\delta$ small enough and possibly replacing the original radius $r$ by $r - \delta$, we can ensure that
  \[
    \label{eq:bayesian.normalization_constant.small_enough_r}
    \Eps(r^+_{\tilde\alpha} \pm \delta) \subset \States,
  \]
  as well.
  Therefore, Eq.~\eqref{eq:bayesian.criterion} holds and we find
  \begin{align}
    \label{eq:bayesian.normalization_constant.almost_c}
    \frac{\tilde\alpha}{C}
    &= P\left( \Nhalf, \tfrac{{r^+_{\tilde\alpha}}^2}{2} \right) \\
    &= P\left( \Nhalf, \tfrac{{(r^+_{\tilde\alpha} \pm \delta)}^2}{2} \right) - \frac{1}{\Gamma(\Nhalf)} \int_{\tfrac{{r^+_{\tilde\alpha}}^2}{2}}^{\tfrac{{(r^+_{\tilde\alpha} \pm \delta)}^2}{2}}\, t^{\Nhalf - 1} \ee^{-t} \dd t.
  \end{align}
  The first addend on the right hand side can be evaluated using the same series expansion as in Eq.~\eqref{eq:bayesian.normalization_constant.series}, since we are in the same regime $\tfrac{{r^+_{\tilde\alpha}}^2}{2} \ll \Nhalf$.
  The second addend can be bounded by
  \[
    \label{eq:bayesian.normalization_constant.upper_bound}
    \Abs{\frac{1}{\Gamma(\Nhalf)} \int_{\tfrac{{r^+_{\tilde\alpha}}^2}{2}}^{\tfrac{{(r^+_{\tilde\alpha} \pm \delta)}^2}{2}}\, t^{\Nhalf - 1} \ee^{-t} \dd t}
    < \frac{\left( 2 {r^+_{\tilde\alpha}} \delta + \delta^2 \right)}{2}
  \]
  since
  \[
    \frac{t^{\Nhalf - 1} \ee^{-t}}{\Gamma(\Nhalf)} < 1.
  \]
  Let us assume w.l.o.g.\ $r^+_{\tilde\alpha} \le 1$.
  This bound, as well as the error bound $\epsilon' > 0$ for the finite series-evaluation of $P$ in~\eqref{eq:bayesian.normalization_constant.almost_c} leads to
  \[
    \frac{\tilde\alpha}{C} = P_{k_0}\left( \Nhalf, \tfrac{{(r^+_{\tilde\alpha} \pm \delta)}^2}{2} \right) + \epsilon' \pm D \delta
  \]
  for some appropriate constant $D$.
  A little arithmetic gives
  \[
    \label{eq:bayesian.normalization_constant.formula_c}
    C = \frac{\tilde\alpha}{P_{k_0}(\ldots)} \, \left( 1 - \frac{\epsilon' \pm D\delta}{P_{k_0}(\ldots) + \epsilon' \pm D\delta} \right).
  \]
  By assumption we can make both $\epsilon'$ and $\delta$ exponentially small using only polynomial time
  Furthermore, $P_{k_0}(\Nhalf, x) \uparrow P(\Nhalf,x)$ for $k_0 \to \infty$ and the correction to
  \[
    \tilde C = \frac{\tilde\alpha}{P_{k_0}\left( \Nhalf, \tfrac{{(r^+_{\tilde\alpha} \pm \delta)}^2}{2} \right)}
  \]
  in Eq.~\eqref{eq:bayesian.normalization_constant.formula_c} can be made exponentially small using polynomial time.
  On the other hand, $\tilde C$ can be computed in polynomial time as well.
\end{proof}

We now have all the necessary parts for the proof of the main theorem~\ref{thm:bayesian.hardness}, which concludes this section.

\begin{proof}[Proof of Thm.~\ref{thm:bayesian.hardness}]
  The proof follows the outline stated in the main text:
  First, we encode the ellipsoid of Problem~\ref{prob:ortho.ellpos} to be checked as a MVCR of a Gaussian with mean $\varrho_0$ and covariance matrix $\Sigma$ according to Lemma~\ref{lem:bayesian.encoded_ellipsoid}.
  Using Lemma~\ref{lem:bayesian.normalization_constant}, we compute an estimate $\tilde C$ to the normalization constant $C$.
  Using the techniques from the proof of the aforementioned Lemma, we may compute an estimate
  \[
    \alpha = C \, P\left( \Nhalf, 1 \right) = \tilde C (1 + \epsilon) \left( P_{k_0}\left( \Nhalf, 1 \right) + \epsilon' \right) = \tilde\alpha + \epsilon''.
  \]
  This can be done for exponential small errors $\epsilon, \epsilon'$ in polynomial time.
  Here, the computable value is given by
  \[
    \tilde\alpha = \tilde C \, P_{k_0}\left(\Nhalf, 1 \right).
  \]
  An exponential small difference of $\alpha$ and $\tilde\alpha$ also implies an exponential small difference of $r^+_{1-\alpha}$ and $r^+_{\tilde\alpha}$:
  Set $x := r^+_{\alpha}$ and $\tilde x := r^+_{\tilde\alpha}$ and assume $x > \tilde x$ -- the opposite case can be treated along the same lines by choosing a larger constant as a bound for $\tilde x$.
  Following Eq.~\eqref{eq:bayesian.r_separation.p_diff}, we have
  \begin{align*}
    P\left( \Nhalf, \tfrac{x^2}{2} \right) - P\left( \Nhalf, \tfrac{{\tilde x}^2}{2} \right)
    &\ge \frac{\mathrm{e}^{-4}}{{(2\pi)}^{\frac{N}{2}} \, \abs{\Sigma}^{\frac{1}{2}}} \, \mathrm{Vol}\left(  \Eps(x) \setminus \Eps(\tilde x)  \right) \\
    &= \frac{\mathrm{e}^{-4}}{2^{\Nhalf} \Gamma(\Nhalf + 1)} \left( x^N - {\tilde x}^N \right).
  \end{align*}
  Since for fixed $N$, the left hand side can be made exponentially small in polynomial time by improving $\tilde\alpha$, so can the right hand side.
  Therefore, the difference $\abs{x - \tilde x}$ can be made exponentially small as well.


  Now, choose the errors $\epsilon$ and $\epsilon'$ in such a way that
  \[
    \abs{r^+_{\alpha} - r^+_{\tilde\alpha}} \le \frac{\Delta}{4}.
  \]
  Here, $\Delta = 2^{-p(\log \norm{\vec a}_1)}$ is the (at worst exponentially small) gap from Lemma~\ref{lem:bayesian.r_separation}.
  Furthermore, we run the algorithm for computing $r^+_{\tilde\alpha}$ with precision $\delta = \frac{\Delta}{4}$ and denote the result by $\tilde r$.
  If $\abs{\tilde r - \sqrt{2}} \le \frac{\Delta}{2}$, we know that $r^+_{\alpha} = r_\frac{\alpha}{C}$ and the ellipsoid is fully contained in the psd states.
  Otherwise we know that it is not.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion \& Outlook}
\label{sec:error.outlook}

The goal of this work is to provide an absolute \quotes{upper bound} on what we can expect from algorithms computing error regions for QSE and to demonstrate that there is a trade-off between optimality and efficiency.
This work should not be understood as providing a no-go theorem for efficient algorithms in practice.
As discussed in the end of \cref{sec:error.complexity}, the negative result of this work does not rule out efficient algorithms for practically acceptable approximations to optimal regions.
Also, there is no indication that the various approaches used in practice give rise to regions that are far from optimal or do not have the advertised coverage.
The reason our result leaves room for feasible approaches in practice are twofold:
First, like any result showing $\NP$-hardness, we prove that there is no efficient algorithm solving the exact problem deterministically for any instance.
Hence, our result neither precludes the existence of efficient approximate or probabilistic algorithms, nor cannot make any statement about average case hardness.
Second, although the experimental effort necessary for full-fledged QSE scales polynomially in the dimension of the system -- and is, therefore, efficient in the sense of computational complexity -- in practice other characterization techniques such as randomized benchmarking or direct fidelity estimation become more important for larger dimensions.
It should now be the goal of future work to further close down the gap between existing positive results and the proven no-go theorems from either side.

More specifically, due to the simplifying assumptions made, we investigate computational intractability that is solely caused by the quantum constraints and not by the general complications in high-dimensional statistics.
In the Bayesian settings we show that minimal volume (w.r.t.\ the Hilbert-Schmidt measure) credible regions for truncated Gaussian posterior distributions are hard to compute.
Therefore, the problem of determining MVCR for QSE cannot be solved efficiently as well, since any algorithm solving the latter must also be able to solve instances with the specific prior used in Prob.~\ref{prob:bayesian.trucated_cr}.

The result for frequentist confidence regions is somewhat weaker since optimal confidence regions for high-dimensional Gaussian distributions are not known for most natural notions of optimality.
Nevertheless, Gaussian confidence ellipsoids constitute a viable choice due to their simplicity and tractability.
However, our results show that the constraints imposed by quantum mechanics render the task of characterizing the confidence regions for the constrained problem computationally intractable -- even under the simplifying assumptions made.
Of course, any more general setting encompassing the Gaussian approximation will be at least as hard to treat as the one used in this work.
Furthermore, it also shows that computing any confidence region estimator yielding ellipsoids when the constraints are not active (and anything possibly better when they are) involves solving $\NP$-hard problems.\\


Recently, the mathematical statistics community has started to analyze the trade-offs between computational complexity and optimality in inference problems -- see e.g.~\cite{Berthet_2013_Complexity,Berthet_2013_Computational,Zhang_2014_Lower}.
Early papers concentrated on the problem of \emph{sparse principal component analysis}, which roughly asks whether the covariance matrix of a random vector possess a sparse eigenvector with large eigenvalue~\cite{Berthet_2013_Complexity,Berthet_2013_Computational,Zhang_2014_Lower}.
Later works have addressed the much better-studied problem of sparse inference~\cite{Zhang_2014_Lower}.
The main difference between these papers and the present one is that we always condition on a data set and show that certain operations for quantifying uncertainty given the data are hard.
This approach is canonical for a Bayesian analysis, but merely \quotes{natural} for frequentist confidence regions (c.f.\ \cref{sub:stat.frequentist}).
In contrast, Refs.~\cite{Berthet_2013_Complexity,Berthet_2013_Computational,Zhang_2014_Lower} analyze the \quotes{global} performance of orthodox estimators -- i.e.\ they do not require looking at worst-case scenarios over the data.
References~\cite{Berthet_2013_Complexity,Berthet_2013_Computational,Zhang_2014_Lower} achieve this by reducing a certain problem (\quotes{hidden clique}) -- that is conjectured to be hard in the average case -- to the sparse PCA problem; while~\cite{Zhang_2014_Lower} employs a more subtle argument involving the non-uniform complexity class $\P/\mathrm{poly}$.
It would be very interesting to adapt such arguments to the problem of quantum uncertainty quantification.


Of course, from the practical point of view, \quotes{positive} results -- i.e.\ new algorithms to solve the problem -- would be more beneficial.
Here, recent work on sampling distributions restricted to convex bodies~\cite{Cousins_2013_Cubic,Cousins_2015_Bypassing} could be a starting point for further investigations.

Beside quantum state tomography, our results might also be relevant to problems involving psd constraints such as the estimation of covariance matrices.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
