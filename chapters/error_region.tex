 % -*- root: ../thesis.tex -*-
\chapter{Uncertainty Quantification for quantum state estimation}
\label{chap:error}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction to Statistics}
\label{sec:error.intro}

% two main flavours of statistics...
% task of parameter estimation, model
% parametric model: state space \Omega
However, even if our model describes the data perfectly we cannot exactly recover this value from a finite amount of data due to statistical fluctuations.
The concept of error bars, or more generally error regions, allows for quantifying the uncertainty of a given estimate. 

% uncertatiny quantification, problem with point estimators

\subsection{Frequentist Statistics}
\label{sub:intro.frequentist}

In the frequentist (or orthodox) framework, the probability of an outcome of a random experiment is defined in terms of its relative frequency of occurrence when the number of repetitions goes to infinity~\cite{Keynes_2007_Treatise,Kiefer_2012_Introduction}.
More precisely, denote the number of repetitions of an experiment by $T$ and the number of times the event under consideration $x$ occurred by $n_T$. 
Then, a Frequentist interprets the probability $\Prob(x)$ as the statement that if $T \to \infty$, $\frac{n_T}{T} \to \Prob(x)$.

% probabliities = hypothetical frequencies, not repeatable
For the task of parameter estimation, we assume that the observed data are generated from the parametric model with \quotes{true} parameter $\Theta \in \Omega$, which is unknown.
From a finite number of observations $X_1, \ldots X_N$, we must construct an estimate for $\Theta$ that is close to the true value in some sense.
The function $\hat\Theta$ that maps observations to such an estimate is called a point estimator.
The quality of an estimator is measured by its risk function.
A risk function commonly used for continuous parameter spaces is the mean square error
\[
  \label{eq:frequentist.mean_square}
  \mathcal{L}_{\hat\Theta}(\Theta) := \Exp_\Theta \left( \Norm{\Theta - \hat\Theta(X_1, \ldots, X_N)}^2 \right).
\]
\todo{Make loss without expectation}
Note that \cref{eq:frequentist.mean_square} -- and therefore the performance of a given estimator -- still depends on the unknown true value $\Theta$.
Strategies to make statements independent of the true value include 
\begin{definition}
  \label{def:frequentist.optimality_conditions}
  \begin{itemize}
    \item $\hat\Theta$ is called a \emph{uniformly best} estimator, if for all other estimators $\hat\Theta'$ and all values of the true parameter $\Theta \in \Omega$
    \[
      \mathcal{L}_{\hat\Theta}(\Theta) \le \mathcal{L}_{\hat\Theta'}(\Theta).
    \]

    \item $\hat\Theta$ is called \emph{minimax}, if for all other estimators $\hat\Theta'$
    \[
      \sup_{\Theta\in\Omega} \mathcal{L}_{\hat\Theta}(\Theta) \le \sup_{\Theta\in\Omega} \mathcal{L}_{\hat\Theta'}(\Theta).
    \]

    \item $\hat\Theta$ is called \emph{best on average} w.r.t.\ a distribution of the true value $\Theta$ if for all other estimators $\hat\Theta'$
    \[
      \Exp_{\Theta} \mathcal{L}_{\hat\Theta}(\Theta) \le  \Exp_{\Theta} \mathcal{L}_{\hat\Theta'}(\Theta).
    \]

    \item $\hat\Theta$ is called \emph{admissible} if there is no other estimator $\hat\Theta'$ such that
    \[
      \forall\Theta \in \Omega\colon \mathcal{L}_{\hat\Theta'}(\Theta) \le \mathcal{L}_{\hat\Theta}(\Theta) 
      \quad\mbox{ and }\quad
      \exists\Theta \in \Omega\colon \mathcal{L}_{\hat\Theta'}(\Theta) < \mathcal{L}_{\hat\Theta}(\Theta) 
    \]
  \end{itemize}
\end{definition}
\todo{Citation}
\todo{Discussion of different properties, why no expectation value -> no "singular" regions?}
\todo{Choice is arbitrary!}
\todo{How does this connect with definition of probability?}
\todo{Typical estimators? Max likelihood?}
\todo{No statement about single runs!}


% examples (also depends on notion of volume), admissability
However, as already mentioned in the introduction, point estimators cannot convey uncertainty in the estimate. 
For this purpose we need to introduce a precise notion of \quotes{error bars}, namely \emph{confidence regions}.
A confidence region $\CR \subset \Omega$ with coverage $\alpha \in [0,1]$ is a region estimator -- that is a function that maps observed data to a subset of the parameter space -- such that the true parameter is contained in $\CR$ with probability greater than $\alpha$
\[
  \label{eq:frequentist.coverage}
  \forall \Theta\in\Omega \colon \Prob_\Theta\left( \CR(X_1, \ldots, X_N) \ni \Theta \right) \ge \alpha.
\]
 
Similar to point estimators, \cref{eq:frequentist.coverage} does not uniquely determine a confidence region construction.
Furthermore, additional constraints are necessary to exclude trivial constructions such as the following:
Take the region estimator, which is always equal to the full parameter space independent of the data $\CR(X_1, \ldots X_N) = \Omega$, then 
\[
  \Prob_\Theta\left(  \CR(X_1, \ldots, X_N) \ni \Theta  \right) = 1 \ge \alpha
\] 
for all confidence levels $\alpha$.
Although, this construction trivially fulfils the coverage condition~\eqref{eq:frequentist.coverage}, it does not provide useful information on the uncertainty as it does not restrict the parameter space at all.
Therefore, we have to impose a notion of what constitutes a good confidence region.

Clearly, if we have two confidence regions $\CR_1$ and $\CR_2$ with the same confidence level $\alpha$ and $\CR_1 \subset \CR_2$, then $\CR_1$ is more informative.
More generally, smaller regions should be preferred since they convey more confidence in the estimate and exclude more alternatives.
Therefore, measures of size such as (expected) volume or diameter are commonly used as risk functions for region estimators.
This leads to similar definitions as in \cref{def:frequentist.optimality_conditions} for confidence regions with the additional constraint that \cref{eq:frequentist.coverage} has to be fulfilled.

\todo{Today: Asymptotic optimal, adaptive, ...}


\subsection{Bayesian Statistics}
\label{sub:intro.bayesian}

% frequentist: parameters fixed, data random; Bayesian: other way
% two different camps of interpretation: subjective and objective -> only differetnt in interpretation, technically the same
% prior -> Bayes' rule -> posterior 

Let us now introduce the Bayesian point of view on inference.
In contrast to the Frequentist's framework, where the existence of a single \quotes{true value} for the model parameter is assumed and the observed data is assumed to be random, Bayesian statistics treats the parameter itself as a random variable.
More precisely, in Bayesian statistics probabilities reflect subjective beliefs and allow for consistently updating one's belief according to empirical observations.
In a parameter estimation problem, the \emph{prior distribution} $\Prob(\Theta)$ expresses one's belief about the parameter $\Theta$ before taking any data into account.
Given an observation $X$, the distribution of $\Theta$ is updated according to Bayes' rule~\cite{}
\[
  \label{eq:bayesian.bayes_rule}
  \Prob(\Theta | X) = \frac{\Prob(X | \Theta) \Prob(\Theta)}{\Prob(X)}.
\]
Here, $\Prob(X|\Theta)$ is the \emph{likelihood function} and $\Prob(\Theta | X)$ is the \emph{posterior distribution} -- or short posterior -- of $\Theta$.

Computing the Bayesian update~\eqref{eq:bayesian.bayes_rule} analytically is possible only in a few rare cases:
If for likelihood function the prior and the posterior are in the same family of distributions, the prior is called a \emph{conjugate prior} for the likelihood function.
For example, consider a Gaussian random variable $X \sim \Normal(\theta, \tau^2)$ with known variance $\tau^2$.
If we assume a Gaussian prior $\theta \sim \Normal(\mu, \sigma^2)$, we have~\cite{}
\[
  \theta \vert X=x \sim \Normal (\mu', \sigma'^2)
\]
with 
\[
  \mu' = \frac{ \frac{\mu}{\sigma^2} + \frac{x}{\tau^2} }{ \frac{1}{\sigma^2} + \frac{1}{\tau^2} },
  \quad\mbox{and}\quad
  \sigma' = \left( \frac{1}{\sigma^2} + \frac{1}{\tau^2} \right)^{- \tfrac{1}{2}}.
\]
In other words, an update of a Gaussian prior with a Gaussian likelihood function yields a Gaussian posterior distribution.
Although there are other well-known conjugate priors with explicit formulas for the parameter update, in practice the Bayesian update~\eqref{eq:bayesian.bayes_rule} can only be approximated numerically.
Commonly used methods include sampling techniques such as Markov Chain Monte Carlo and Sequential Monte Carlo~\cite{} as well as variational Bayes~\cite{}.\\


Note that the posterior contains all the information on $\Theta$. 
To summarize important features of the posterior, we introduce point and region estimators similar to the Frequentist case:
One commonly used estimator is the Bayesian mean estimator (BME) given by
\[
  \hat\Theta_\mathrm{BME}(X) = \int \Theta \, \Prob(\Theta | X) \, \dd\Theta.
\]
A justification for the BME is that it minimizes (under normal circumstances~\cite{Lehmann_1998_Theory}) the expected loss 
\[
  \label{eq:bayesian.expected_loss}
  \Exp_\Theta \mathcal{L}(\Theta, \hat\Theta(X)),
\]
provided the loss function $\mathcal{L}$ is a \emph{Bregman divergence}~\cite{Banerjee_2005_On}.
Note that the expectation over $\Theta$ in \cref{eq:bayesian.expected_loss} is taken over the posterior with observation $X$.

Another example of a point estimator is the \emph{maximum a posteriori} (MAP) estimator.
As the name suggests, the MAP estimator is obtained by maximizing the posterior~\eqref{eq:bayesian.bayes_rule}.
Since this does not require the expensive computation of the denominator in \cref{eq:bayesian.bayes_rule} and (at least local minima) of the posterior can be found efficiently via gradient descent, the MAP estimator is often used in high dimensional problem~\cite{Murphy_2012_Machine}.\\

Let us now introduce the concept of error regions in the Bayesian framework.
A \emph{credible region}\footnote{%
  We use the same letter for confidence and credible regions when the meaning is clear from the context.
}
$\CR$ with credibility $\alpha$ is a subset of the parameter space $\Omega$ containing at least mass $\alpha$ of the posterior
\[
  \label{eq:bayesian.credibility}
  \Prob(\Theta \in \CR | X_1, \ldots, X_N) \ge \alpha.
\]
Notice the different notion of randomness compared to \cref{eq:frequentist.coverage}:
Confidence regions are random variables due to their dependence on the data and \cref{eq:frequentist.coverage} demands that the true value is contained in the confidence region with high probability.
Here, \quotes{probability} refers to (possibly hypothetical) repetitions of the experiment -- no statement can be made for a single run of an experiment with given outcomes.
In contrast, the definition of credible regions~\eqref{eq:bayesian.credibility} only refers to probability w.r.t.\ the posterior and, therefore, is well defined even for a single run of the experiment.

In order to single out \quotes{good} credible regions, we need to introduce a notion of optimality. 
As argued in \cref{sub:intro.frequentist}, smaller error regions are generally more informative. 
Therefore, good credible regions are minimal-volume credible regions (MVCRs) or credible regions with the smallest diameter.
Since the prior and data are  assumed to be fixed, no ambiguity similar to \cref{def:frequentist.optimality_conditions} arises in this setting.
Although in the following we deal with MVCRs w.r.t.\ a geometric notion of volume, some authors have proposed to measure the volume of a region by its prior probability~\cite{Evans_2006_Optimally,Shang_2013_Optimal}.


\todo{Objective vs subjective Bayesian}